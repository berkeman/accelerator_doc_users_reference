%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2021 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


This chapter presents an overview of the Accelerator's features in a
rather non-formal way.  It is based on an article published on the
eBay Tech Blog website.



\section{High Level View}
The Accelerator is a client-server based application, and from a high
level, it can be visualised like in figure~\ref{fig:overview}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/overview.pdftex_t}
    \caption{High level view of the Accelerator framework.  See text
      for details.}
    \label{fig:overview}
  \end{center}
\end{figure}

In the figure, the client side, with shell commands and web interface,
is on the left, and the server side, with job databases is to the
right.  The Accelerator executes \textsl{build scripts}, that issue
\textsl{jobs} on the server.  These jobs are stored in the server's
job databases for later retreival.  There are two databases, one for
storing everything related to a job's execution, called the
\textsl{workdir}, and one transaction log used for storing
meta-information about built jobs.  The workdir will contain all
inputs, source code, parameters, and outputs of all executed jobs,
whereas the job log database ensures reproducibility and transparency,
and it will be further discussed in chapter~\ref{chap:urd}.


\section{Interacting with the Accelerator}
There are two ways to interface the Accelerator, using
\begin{itemize}
\item[] the command line, and
\item[] using a web brower.
\end{itemize}
The command line interface is based on the \texttt{ax} shell command.
It can be used for a large variety of operations, including starting
execution of scripts, listing data in datasets, showing content of
databases, and more.

The web interface, called \textsl{Accelerator Board}, is used to
inspect jobs, datasets, and more.  Resulting files such as images,
text files, pdfs, video files and so on are rendered in the web
browser.


\section{Jobs}
The basic operation of the Accelerator is to execute small Python
programs called \textsl{methods}.  A method is nothing but a Python
program with one or a few special functions that are used to execute
code sequentially or in parallel and to pass parameters and results.
A method that has completed execution is called a \textsl{job}.

Jobs are stored in \textsl{job directories}.  A dedicated directory in
the workdir will be created for each new job, and this directory will
contain all information regarding the job, such as its input
parameters, stored files, return values, profiling information, Python
interpreter version and path, and more.

The Accelerator has a database that keeps track of all jobs that has
been run.  This avoids unnecessary re-computing in favour of re-using
previously computed results.  Re-using jobs does not only speed up
processing and encourage incremental design, but also makes it
transparent which code and which data that was used for any particular
result, minimising these kind of uncertainties to zero.


\subsection{A ``Hello, World'' Example}
Here's an example of a very simple method.  It does not take any input
parameters and does almost nothing, it will just return the string
``\texttt{hello world}'' and exit.
\begin{python}
def synthesis():
    return "hello world"
\end{python}
Everything inside the \texttt{synthesis()}-function is run once
without any parallelisation.  In order to get this method to execute,
it is called from a \textsl{build script} looking something like this
\begin{python}
def main(urd):
    job = urd.build('hello_world')
    print(job.load())
\end{python}
The \texttt{urd} object made available by the \texttt{main()}-function
contains functions for job building and organisation, and is described
in chapter~\ref{chap:urd} and section~\ref{sec:classes:urd}.  This
build script is then run by a shell command like this
\begin{shell}
ax run
\end{shell}
The \texttt{build()}-call will instruct the server to execute the
\texttt{hello\_world} method.  During its execution, a job directory
will be created that contains everything associated with the build
process.  When the \texttt{build()}-call is completed, a job object, of type
\texttt{Job}, is returned to the program.  This object provides a
convenient interface to the data in the corresponding job directory,
and contains member functions such as \texttt{.load()}, that is used
in the example to read back the returned value from the job.


\subsection{Jobs Can Only be Run Once}
If the build script is executed again, the \texttt{hello\_world} job
will not be re-built, simply because the Accelerator remembers that
the job has been built in the past, and its associated information is
stored in a job directory.  Instead, the Accelerator immediately
returns a job object representing the previous run.  This means that
from a user's perspective, there is no visible difference between
running a job for the first time or re-using results from an existing
run!  In order to have the method execute again, either the source
code or input parameters has to change.  If there are changes, the
method will be re-executed, and a new job will be created that
reflects these changes.


\subsection{Back to the ``Hello, World'' Example}
Figure~\ref{fig:execflow-hello-world} illustrates the dispatch of the
\texttt{hello\_world} method.  The created job gets the \textsl{jobid}
\texttt{test-0}, and parts of the corresponding job directory
information is shown in green.  (Jobids are job identifiers, that are
named by their corresponding \textsl{workdir} plus an integer counter
value.)  The job directory contains several files, of which the most
important are
\begin{itemize}
\item[] \texttt{setup.json}, containing job meta information;
\item[] \texttt{result.pickle}, containing the returned data; and
\item[] \texttt{method.tar.gz}, containing the method's source code.
\end{itemize}

\begin{figure}[b]
  \begin{center}
    \input{figures/job0.pdftex_t}
    \caption{A simple hello world program, represented as graph and
      work directory.}
    \label{fig:execflow-hello-world}
  \end{center}
\end{figure}

The \texttt{Job} object provides a convenient way to access files and
data stored in this directory.  For example, as we've already seen,
the job's return value can be loaded into a variable using the
\texttt{.load()} function.



\subsection{Workdirs and Sharing Jobs}

Workdirs are used to store jobs.  There can be more than one workdir,
and different workdirs can be used separate jobs into different
physical locations.  The Accelerator can be set up to have any number
of workdirs associated, but only one is used for writing.

If the same workdir is entered into two or more different user's
configuration files, the workdir and its contents will be shared
between the users.  Each Accelerator server will update its knowledge
about the contents of all workdirs before executing a build script, to
make sure that the latest jobs are taken into account.  In addition,
the job log database, as described in chapter~\ref{chap:urd}, is
designed for efficient re-use and sharing of particularly interesting
jobs.


\subsection{Linking Jobs}

Using jobs, complex tasks can be split into several smaller
operations.  Jobs can be connected so that the next job will depend on
the result of a previous job or set of jobs, and so on.

To continue the simple hello world example, assume for a second that
the \texttt{hello\_world}-job is computationally expensive, and that
it returns a result that is to be used as input to further processing.
To keep things simple, this further processing is represented by
printing the result to standard output.  A new method
\texttt{print\_result} is created, and it goes like this
\begin{python}
jobs = ('hello_world_job',)

def synthesis():
    data = jobs.hello_world_job.load() 
    print(data)
\end{python}
This method expects the \texttt{hello\_world\_job} input parameter to
be provided at execution time, and it is accomplished by the following
extended build script
\begin{python}
def main(urd):
    job1 = urd.build('hello_world')
    job2 = urd.build('print_result', hello_world_job=job1)
\end{python}
The \texttt{print\_result} method then loads the result from the
provided job and prints its contents to \texttt{stdout}.  Note that
this method does not return anything.

Figure~\ref{fig:execflow-print-result} illustrates the situation.
(Note the direction of the arrow: the second job, \texttt{test-1} has
\texttt{test-0} as input parameter, but \texttt{test-0} does not know
of any jobs run in the future.  Hence, arrows point to previous jobs.)

\begin{figure}[b]
  \begin{center}
    \input{figures/job0job1.pdftex_t}
    \caption{Job \texttt{test-0}, is used as input to the
      \texttt{print\_result} job.}
    \label{fig:execflow-print-result}
  \end{center}
\end{figure}

The example shows how a complex task may be split into several jobs,
each reading intermediate results from previous jobs.  The Accelerator
will keep track of all job dependencies, so there is no doubt which
jobs that are run when and on which data.  Furthermore, since the
Accelerator remembers if a job has been executed before, it will link
and re-use previous jobs when possible.  This may bring a significant
improvement in execution speed.  Furthermore, a re-used job is a proof
of that the code, input- and output data is unchanged and connected.


\section{Datasets: Storing Data}

The \texttt{dataset} is the Accelerator's default storage type for
small or large quantities of data, designed for parallel processing
and high performance.  Datasets are built on top of jobs, so
\emph{datasets are created by methods and stored in job directories,
  just like any job result.}

Internally, data in a dataset is stored in a row-column format, and is
typically \emph{sliced} into a fixed number of slices to allow
efficient parallel access, see figure~\ref{fig:dataset}. Columns are
accessed independently, so there is no overhead in reading a single or
a set of columns.  Only the data relevant for a task is read from
disk.


\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_files.pdftex_t}
    \caption{A dataset containing three columns, $A$, $B$, and $C$
      stored using two slices.  Each dotted box corresponds to a file,
      so there are two files for each column, allowing for parallel
      read of the data using two processes.}
    \label{fig:dataset}
  \end{center}
\end{figure}

Furthermore, datasets may be \textsl{hash partitioned}, so that
slice-membership is based on the hash value of a given column.
Partitioning base on, for example, a column containing some ID string
will put all rows corresponding to any particular ID in a single slice
only.  In many practical applications, hash partitioning makes
parallel processes independent, minimising the need for complicated
merging operations.  This is explained further in
section~\ref{sec:slicing_and_hashing}.



\subsection{Importing Data}

A project typically starts with \textsl{importing} some data from a
file on disk.  The bundled method \texttt{csvimport} is designed to
parse a plethora of ``comma separated values''-file formats and store
the data as a dataset.  See figure~\ref{fig:dataset_csvimport}.
\begin{figure}[b]
  \begin{center}
    \input{figures/import_file1.pdftex_t}
    \caption{Importing \texttt{file0.txt}.}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}
The method takes several input options in addition to the mandatory
filename to control the import process.  Here is an example
invocation
\begin{python}
def main(urd):
    jid = urd.build('csvimport', filename='file0.txt')
\end{python}
When executed, the created dataset will be stored in the resulting job
directory, and the name of the dataset will by default be the jobid
plus the string \texttt{default}.  For example, if the
\texttt{csvimport} jobid is \texttt{imp-0}, the dataset will be
referenced by \texttt{imp-0/default}.  In this case, and always when
there is no ambiguity, the jobid alone (\texttt{imp-0}) could be used
too, for simplicity.  In general, a job could contain any number of
datasets, but a single dataset is a common case.




\subsection{Linking Datasets, Chaining}

Just like jobs can be linked to each other, datasets can link to each
other too.  Since datasets are built on top of jobs, this is
straightforward.  Assume the file \texttt{file0.txt} is imported into
dataset \texttt{imp-0/default}, and that there is more data like it
stored in the file \texttt{file1.txt}.  The second file is imported
with a link to the first dataset, see
figure~\ref{fig:dataset_csvimport_chain}.
\begin{figure}[t]
  \begin{center}
    \input{figures/import_file0file1.pdftex_t}
    \caption{Chaining the import of \texttt{file1.txt} to the previous
      import of \texttt{file0.txt}.}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}
The \texttt{imp-1} (or \texttt{imp-1/default}) dataset reference can
now be used to access all data imported from \textsl{both} files!

Linking datasets containing related content is called \emph{chaining},
and this is particularly convenient when dealing with data that grows
over time.  A good example is any kind of \emph{log} data, such as
logs of transactions, user interactions, and similar.  Using chaining,
datasets can be with more rows just by linking, which is a lightweight
constant time operation.



\subsection{Adding New Columns to a Dataset}
In the previous section it was shown that datasets can be chained and
thereby grow in number of rows.  A dataset chain is created simply by
linking one dataset to the other, so the overhead is minimal.  In this
section it is shown that by the same principles, it is equally simple
to add new columns to existing datasets.  Adding columns is a common
operation and the Accelerator handles this situation efficiently using
links.

The idea is very simple.  Assume a ``source'' dataset to which one or
more new columns should be added.  A new dataset is created containing
\textsl{only} the new column(s), and while creating it, the
constructor is instructed to link all the source dataset's columns to
the new dataset such that the new dataset appears to contain all
columns from both datasets.  (Note that this linking is similar to but
different from chaining.)

Accessing the new dataset will transparently access all the columns in
both the new and the source dataset in parallel, making it
indistinguishable from a single dataset.  See
Figure~\ref{fig:dep_dataset_append_column}.

\begin{figure}[b]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{Adding one new column to the source dataset.}
    \label{fig:dep_dataset_append_column}
  \end{center}
\end{figure}

A common case is to compute new columns based on existing ones.  In
this case, values are written to the new columns in the new dataset
while reading from the iterator iterating over the existing columns in
the source dataset.  This will be discussed in detail in
section~\ref{sec:appending_new_columns}



\subsection{Multiple Datasets in a Job}

Typically, a method creates a single dataset in the job directory, but
there is no limit to how many datasets that could be created and
stored in a single job directory.  This leads to some interesting
applications.

One application for keeping multiple datasets in a job is when data is
split into subsets based on some condition.  This could, for example,
be when a dataset is split into a training set and a test set.  One
way to achieve this using the Accelerator is by creating a Boolean
column that tells if the current row is train or test data, followed
by a job that splits the dataset in two based on the value on that
column.  See Figure~\ref{fig:dep_dataset_csvimport_chain}.

\begin{figure}[h!]
  \hspace{1cm}
  \input{figures/filter_dataset.pdftex_t}
  \caption{\texttt{job-1} separates the dataset
    \texttt{job-0/default} into two new datasets, named
    \texttt{job-1/train} and \texttt{job-1/test}.}
  \label{fig:dep_dataset_csvimport_chain}
\end{figure}

In the setup of figure~\ref{fig:dep_dataset_csvimport_chain} we have
full tracking from either \texttt{train} or \texttt{test} datasets.
If we want to know the source of one of these sets, we just follow the
links back to the previous jobs until we reach the source job.  In the
figure, \texttt{job-0} may for example be a \texttt{csvimport} job,
and will therefore contain the name of the input file in its
parameters.  Thus, it is straightforward to link any data to its
source.

Splitting a dataset into parts creates ``physical'' isolation while
still keeping all the data at the same place.  No data is lost in the
process, and this is good for transparency reasons.  For example, a
following method may iterate over \textsl{both} datasets in
\texttt{job-1} and by that read the complete dataset.



\subsection{Parallel Dataset Access and Hash Partitioning}
As shown earlier in this chapter, data in datasets is stored in
multiple files for two reasons.  The first reason is that we can read
only the columns that we need, without overhead, and the second reason
is that it allows fast parallel reads.  The parameter \texttt{slices}
determines how many slices that the dataset should be partitioned
into, and it also sets the number of parallel processes that are used for
processing the dataset.  There is always one process for each slice of
the dataset, and each process operates on a unique part of the
dataset.

Datasets can be partitioned, or sliced, in different ways.  One
obvious way is to use round robin, where each consecutive data row is
written to the next slice, modulo the number of slices.  This leads to
``well balanced'' datasets with approximately equal number of rows per
slice.  Another alternative to slicing is to slice based on the hash
value of a particular column's values.  Using this method, all rows
with the same value in the hash column end up in the same slice.  This
is efficient for many parallel processing tasks, and will be described
in detail later on.




\subsection{Dataset Column Types}

There are large a number of useful types available for dataset
columns.  They include \textsl{floating} and \textsl{integer point
  numbers}, \textsl{Booleans}, \textsl{timestamps}, several
\textsl{string types} (handling all kinds of string encodings), and
\textsl{json} as well as \textsl{pickle} types for storing arbitrary
data collections.  Most of these types come with advanced parsers,
making importing data from text files straightforward with
deterministic handling of errors, overflows, and so on.



\subsection{Dataset Attributes}
The dataset has a number of attributes associated with it, such as
shape, number of rows, column names and types, and more.
An attribute is accessed like this
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.shape)
    print(datasets.source.columns)
\end{python}
and so on.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Iterators: Working with Data}

Data in a dataset is typically accessed using an \emph{iterator} that
reads and streams one dataset slice at a time to a CPU core.  The
parallel processing capabilities of the Accelerator makes it possible
to dispatch a set of parallel iterators, one for each slice, in order
to have efficient parallel processing of the whole dataset.

This section shows how iterators are used for reading data, how to
take advantage of slicing to have parallel processing, and how to
efficiently create new datasets.


\subsection{Iterator Basics}

Basic iterator functionality is introduced here using an example.
Assume a dataset that has a column containing movie titles named
\texttt{movie}, and the problem is to extract the ten most frequently
occuring movies.  Here's a complete method solving this problem
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(datasets.source.iterate(None, 'movie'))
    top10 = c.most_common(10)
    print(top10)
    return top10
\end{python}
This will print the ten most common movie titles and their
corresponding counts in the \texttt{source} dataset.  The code will
run on a single CPU core, because we use the single-process
\texttt{synthesis()} function, which is called and executed only once.
The \texttt{dataset.iterate()} (class-)method therefore has to read
through all slices, one at a time, in a serial fashion, and this is
reflected by the first argument to the iterator being \pyNone.  The
method also returns the variable \texttt{top10} so that it can be used
by other methods.



\subsection{Parallel Execution}
It is easy to write parallel programs using the Accelerator.  The fact
that data in a dataset is sliced into disjoint sets and files makes
parallel processing of data straightforward.  Here's a slightly
modified version of the program from the previous section that will
now execute in parallel
\begin{python}
def analysis(sliceno):
    return Counter(datasets.source.iterate(sliceno, 'movie'))

def synthesis(analysis_res):
    c = analysis_res.merge_auto()
    top10 = c.most_common(10)
    return top10
\end{python}
For larger datasets, this parallel version of the movie title counter
will run much faster.  Here, \texttt{.iterate()} is moved inside the
\texttt{analysis()} function.  This function is forked once for each
slice, and the argument \texttt{sliceno} will contain an integer
between zero and the number of slices minus one.  The returned value
from the analysis functions will be available as input to the
synthesis function in the \texttt{analysis\_res} Python iterable.  It
is possible to merge the results explicitly, but the iterable
comes with a rather magic method \texttt{merge\_auto()}, that merges
the results from all slices into one based on the data type.  It can
for example merge \texttt{Counter}s, \texttt{set}s, and composed types
like \texttt{set}s of \texttt{Counter}s, and so on.


\subsection{Iterating over Several Columns}
Since each column is stored independently in a dataset, there is no
overhead from reading a subset of a dataset's columns.  In the
previous section we've seen how to iterate over a single column using
\texttt{.iterate()}.  Iterating over more columns is straightforward
by feeding a list of column names to \texttt{.iterate()}, like in this
example
\begin{python}
from collections import defaultdict
datasets = ('source',)

def analysis(sliceno):
    user2movieset = defaultdict(set)
    for user, movie in datasets.source.iterate(sliceno, ('user', 'movie')):
        user2movieset[user].add(movie)
    return user2movieset
\end{python}
This example creates a lookup dictionary from users to sets of movies.
Note that in this case, we would like to have the dataset hashed on
the \texttt{user} column, so that each user appears in exactly one slice.
This will make later merging (if necessary) much easier.

It is also possible to iterate over all columns by specifying an empty
list of columns or by using the value \pyNone.
\begin{python}
...
def analysis(sliceno):
    for columns in datasets.source.iterate(sliceno, None):
        ...
\end{python}
Here, \texttt{columns} will be a list of values, one for each column
in the dataset, in sorted column name order.


\subsection{Iterating over Dataset Chains}

The \texttt{iterate} function is used to iterate over a single
dataset.  There is a corresponding function, \texttt{iterate\_chain},
that is used for iterating over chains of datasets.  This function
takes a number of arguments, such as
\begin{itemize}
\item[] \texttt{length}, i.e.\ the number of datasets to iterate over.
  By default, it will iterate over all datasets in the chain.
\item[] \texttt{callbacks}, functions that can be called before and/or
  after each dataset in a chain.  Very useful for aggregating data
  between datasets.
\item[] \texttt{stop\_id} which stops iterating at a certain dataset.
  This dataset could be from \textsl{another} job's parameters, so we
  can for example iterate exactly over all new datasets not covered by
  a previous job.
\item[] \texttt{range}, which allows for iterating over a range of
  data.
\end{itemize}
The \texttt{range} options is based on the max/min values stored for
each column in the dataset.  Assuming that the chain is sorted, one
can for example set
\begin{python}
range={'ts_column': ('2016-01-01', '2016-01-31')}
\end{python}
in order to get rows within the specified range only.  Using
\texttt{range=} is quite costly, since it requires each row in the
dataset chain with dates within the range to be checked against the
range criterion.  Therefore, there is a \texttt{sloppy} version that
iterates over complete datasets in the chain that contains at least
one row with a date within the range.  This runs at full speed, and is
useful, for example, to very quickly produce histograms or plots of
subsets of a huge dataset.



\subsection{Job Execution Flow and Result Passing}

Execution of code in a method is either parallel or serial depending
on which function is used to encapsulate it.  There are three
functions in a method that are called from the Accelerator when a
method is running, and they are \texttt{prepare()},
\texttt{analysis()}, and \texttt{synthesis()}.  All three may exist in
the same method, and at least one is required.  When the method
executes, they are called one after the other.
\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of, and actually forked from
  \texttt{prepare()}.  Common input parameters are \texttt{sliceno},
  holding the number of the current process instance, and
  \texttt{prepare\_res}.  The return value for each process becomes
  available in the \texttt{analysis\_res} variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  parameters.  The latter is an iterator of the results from the
  parallel processes.
\end{itemize}
Figure~\ref{fig:prepanasyn} shows the execution order from top to
bottom, and the data passed between functions in coloured branches.
\texttt{prepare()} is executed first, and its return value is
available to both the \texttt{analysis()} and \texttt{synthesis()}
functions.  There are \texttt{slices} (a configurable parameter)
number of parallel \texttt{analysis()} processes, and their output is
available to the \texttt{synthesis()} function, which is executed
last.

Return values from any of the three functions may be stored in the
job's directory making them available to other jobs.


\begin{figure}[t]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}


\begin{figure}[b]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobs},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}


\clearpage
\subsection{Job Parameters}
\label{sec:jobparams}
We've seen how completed jobs can be used as input to new
jobs.  Jobs are one of three kinds of input parameters that
a job can take.  Here the input parameters are summarised:
\begin{itemize}
\item[] \texttt{jobs}, a set, or list, of identifiers to previously executed jobs;
\item[] \texttt{options}, a dictionary of options; and
\item[] \texttt{datasets}, a set, or list, of input \textsl{datasets}.
\end{itemize}
See Figure~\ref{fig:execflow}.  Parameters are entered as global
variables early in the method's source.


\section{A Class Based Programming Model}
The Accelerator is based on an class based paradigm. Access to the
Accelerator's built in functions and parameters are typically done
through a few \textsl{objects} that are populated by the running
Accelerator.
