This chapter presents an overview of the Accelerator's features in a
rather non-formal way.  It is based on an article published on the
eBay Tech Blog website.



\section{High Level View}

The Accelerator is a client-server based application, and from a high
level, it can be visualised like in figure~\ref{fig:overview}.
\begin{figure}[h!]
  \begin{center}
    \input{figures/overview.pdftex_t}
    \caption{High level view of the Accelerator framework.  See text
      for details.}
    \label{fig:overview}
  \end{center}
\end{figure}
This manual will describe the setup in detail.  For now, the most
important features are as follows.  On the left side there is a
\texttt{runner} client.  To the right, there are two servers, called
\texttt{daemon} and \texttt{urd}.  The \texttt{runner} program runs
scripts, called \texttt{build scripts}, that execute jobs on the
\texttt{daemon} server.  This server will load and store information
and results for all jobs executed using the \textsl{workdirs} file
system based database.  In parallel, all jobs covered by a build
script will be stored by the \texttt{urd} server into the \textsl{job
  logs} file system database.  \texttt{urd} is also responsible for
finding collections, or lists, of related previously executed jobs.



\section{Jobs:  Executing Code}
The basic operation of the Accelerator is to execute small Python
programs called \textsl{methods}.  In a method, some reserved function
names are used to execute code sequentially or in parallel and to pass
parameters and results.  A method that has completed execution is
called a \textsl{job}.

The Accelerator keeps a record of all jobs that have been run.  It
turns out that this is very useful for avoiding unnecessary
re-computing and instead rely on reusing previosly computed results.
This does not only speed up processing and encourage incremental
design, but also makes it transparent which code and which data was
used for any particular result, thus reducing uncertainty.

In this section, we'll look at basic job running and result sharing.
%, and parameters.



\subsection{Basic Job Running:  ``Hello, World''}

Let's begin with a simple ``hello world'' program.  We create a method
that we call \texttt{hello\_world} with the following contents
\begin{python}
def synthesis():
    return "hello world"
\end{python}
This program does not take any input parameters.  It just returns a
string and exits.  Slightly simplified for clarity, we run the method
like this. (Running methods on the Accelerator is further explained in
section~\ref{chap:urd_basic}.)
\begin{python}
jobid = build('hello_world')
\end{python}
When execution of the method is completed, a single link, called a
\texttt{jobid} is the only thing that is returned to the user.  The
jobid points to a directory where the result from the execution is
stored, together with all information that was needed to run the job
plus some profiling information.

If we try to run the job again it will not execute, simply because the
Accelerator remembers that the job has been run in the past.  Instead
of running the job again, it immediately returns the jobid pointing to
the previous run.  This means that from a user's perspective, there is
no difference between job running and job result recalling!  In order
to have the job executing again, we have to change either the source
code or input parameters.

Figure~\ref{fig:execflow-hello-world} illustrates the dispatch of the
\texttt{hello\_world} method.  The created jobid is called
\texttt{test-0}, and corresponding directory information is shown in
green.  The job directory contains several files, of which the most
important to mention right now are
\begin{itemize}
\item[] \texttt{setup.json}, containing job information;
\item[] \texttt{result.pickle}, containing the returned data; and
\item[] \texttt{method.tar.gz}, containing the method's source code.
\end{itemize}

\begin{figure}[h!]
  \begin{center}
    \input{figures/job0.pdftex_t}
    \caption{A simple hello world program, represented as graph and
      work directory.}
    \label{fig:execflow-hello-world}
  \end{center}
\end{figure}



\subsection{Linking Jobs}
Assume that the job that we just run was computationally expensive,
and that it returned a result that we'd like to use as input to further
processing.

To keep things simple, we demonstrate the principle by creating a
method that just reads and prints the result from the previous job to
\texttt{stdout}.  We create a new method \texttt{print\_result}, and it
goes like this
\begin{python}
import blob

jobids = {'hello_world_job',}

def synthesis():
    x = blob.load(jobid=jobids.hello_world_job)
    print(x)
\end{python}
This method expects the \texttt{hello\_world\_job} input parameter to
be provided at execution time, and this is accomplished by the
following \textsl{simplified} build script
\begin{python}
jobid1 = build('hello_world')
jobid2 = build('print_result', hello_world_job=jobid1)
\end{python}
The \texttt{print\_result} method then reads the result from the
provided jobid and assigns it to the variable \texttt{x}, which is
then printed to \texttt{stdout}.  Note that this method does not
return anything.  Figure~\ref{fig:execflow-print-result} illustrates
the situation.  (Note the direction of the arrow: the second job,
\texttt{test-1} had \texttt{test-0} as input parameter, but
\texttt{test-0} does not know of any jobs run in the future.  Hence,
arrows point to previous jobs.)

\begin{figure}[h!]
  \begin{center}
    \input{figures/job0job1.pdftex_t}
    \caption{Jobid \texttt{test-0}, is used as input to the
      \texttt{print\_result} job.}
    \label{fig:execflow-print-result}
  \end{center}
\end{figure}



\section{Datasets: Storing Data}

The \texttt{dataset} is the Accelerator's default storage type for
small or large quantities of data, designed for parallel processing
and high performance.  Datasets are built on top of jobs, so
\emph{datasets are created by methods and stored in job directories,
  just like any job result.}

Internally, data in a dataset is stored in a row-column format, and is
typically \emph{sliced} into a fixed number of slices to allow
efficient parallell access.  Columns are accessed independently, so
there is no overhead in reading a set of or even a single column.

Furthermore, datasets may be \textsl{hashed}, so that slicing is based
on the hash value of a given column.  Slicing on, for example, a
column containing some ID string will partition all rows such that
rows corresponding to any particular ID is stored in a single slice
only.  In many practical applications, hashing makes parallel
processes independent, minimising the need for complicated merging
operations.  This is explained further in chapter~\ref{chap:datasets}.



\subsection{Importing Data}

Let's have a look at the common operation of \textsl{importing},
i.e.\ creating a dataset from a file.  See
figure~\ref{fig:dataset_csvimport}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/import_file1.pdftex_t}
    \caption{Importing \texttt{file0.txt}.}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}

\noindent The bundled standard method \texttt{csvimport} is designed
to parse a pletoria of ``comma separated values''-file formats and
store the data as a dataset.  The method takes several input options,
where the file name is mandatory.  When executed, the created dataset
will be stored in the resulting job, and the name of the dataset will
by default be the jobid plus the string \texttt{default}.  For
example, if the \texttt{csvimport} jobid is \texttt{imp-0}, the
dataset will be referenced by \texttt{imp-0/default}.  In this case,
and always when there is no ambiguity, the jobid alone
(\texttt{imp-0}) could be used too.  The reason for this is that a
single job can contain any number of datasets, as we will see shortly.



\subsection{Linking Datasets, Chaining}

Just like jobs can be linked to eachother, datasets can link to
eachother too.  Since datasets are build on top of jobs, this is
straightforward.  Assume that we've just imported \texttt{file0.txt}
into \texttt{imp-0/default} and that there is more data like it stored
in \texttt{file1.txt}.  We can import the latter file and supply a
link to the previous dataset, see
figure~\ref{fig:dataset_csvimport_chain}.
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_file0file1.pdftex_t}
    \caption{Chaining the import of \texttt{file1.txt} to the previous
      import of \texttt{file0.txt}.}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}
The \texttt{imp-1} (or \texttt{imp-1/default}) dataset reference can
now be used to access all data imported from \textsl{both} files!

Linking datasets containing related content is called \emph{chaining},
and this is particularly convenient when dealing with data that grows
over time.  Good example are all kind of \emph{log} data, such as logs
of transactions, user interactions, etc.  Using chaining, we can
extend datasets with more rows just by linking, which is a very
lightweight constant time operation.



\subsection{Adding New Columns to a Dataset}
We have seen how easy it is to add more lines to data to a dataset
using chaining.  A dataset chain is created simply by linking one
dataset to the other, so the overhead is minimal.  Now we'll see that
is it equally simple to add new columns to existing datasets.  Adding
columns is a common operation and the Accelerator handles this
situation efficiently using links.

The idea is very simple.  Assume that we have a ``source'' dataset to
which we want to add one or more new columns.  We create a new dataset
containing \textsl{only} the new column(s), and while creating it we
instruct the Accelerator to link all the source dataset's columns to
the new dataset such that the new dataset appears to contain all
columns from both datasets.  (Note that this linking is similar to but
different from chaining.)

Accessing the new dataset will transparently access all the columns in
both the new and the source dataset in parallel, making it
indistinguishable from a single dataset.  See
Figure~\ref{fig:dep_dataset_append_column}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{Adding one new column to the source dataset.}
    \label{fig:dep_dataset_append_column}
  \end{center}
\end{figure}

A common case is to compute new columns based on existing ones.  In
this case, we iterate over the rows in the source dataset, and for
each iteration we write a new row of values to the new columns.  This
will be shown later in section~\ref{}



\subsection{Multiple Datasets in a Job}

Typically, a method creates a single dataset in the job directory, but
there is no limit to how many datasets that could be created and
stored in a single job directory.  This leads to some interesting
applications.

One application for keeping multiple datasets in a job is when data is
split into subsets based on some condition.  This could, for example,
be when a dataset is split into a training set and a test set.  One
way to achieve this using the Accelerator is by creating a boolean
column that tells if the current row is train or test data, followed
by a job that splits the dataset in two based on the value on that
column.  See Figure~\ref{fig:dep_dataset_csvimport_chain}.

\begin{figure}[h!]
  \hspace{1cm}
  \input{figures/filter_dataset.pdftex_t}
  \caption{\texttt{job-1} separates the dataset
    \texttt{job-0/default} into two new datasets, named
    \texttt{job-1/train} and \texttt{job-1/test}.}
  \label{fig:dep_dataset_csvimport_chain}
\end{figure}

%% The figure shows how \texttt{job-1} has created two datasets,
%% \texttt{job-1/train} and \texttt{job-1/test}, based on the input
%% dataset \texttt{job-0/default}.  A third job, \texttt{job-2} is then
%% accessing the \texttt{job-1/train} dataset.  (Note that \texttt{job-1}
%% does not have a \texttt{default} dataset.)

In the setup of figure~\ref{fig:dep_dataset_csvimport_chain} we have
full tracking from either \texttt{train} or \texttt{test} datasets.
If we want to know the source of one of these sets, we just follow the
links back to the previous jobs until we reach the source job.  In the
figure, \texttt{job-0} may for example be a \texttt{csvimport} job,
and will therefore contain the name of the input file in its
parameters.  Thus, it is straightforward to link any data to its
source.

Splitting a dataset into parts creates ``physical'' isolation while
still keeping all the data at the same place.  No data is lost in the
process, and this is good for transparency reasons.  For example, a
following method may iterate over \textsl{both} datasets in
\texttt{job-1} and by that read the complete dataset.



\subsection{Parallel Dataset Access and Hashing}
As shown in detail in section~\ref{chap:datasets}, data in datasets are stored in
multiple files, allowing for fast parallel reads.  The parameter
\texttt{slices}  determines how many slices that the dataset
should be partitioned into.  This parameter also sets the number of
parallel \texttt{analysis}-processes, so that each \texttt{analysis}
process operates on a unique slice of the dataset.

\comment{REWRITE THIS AND PERHAPS ADD FIGURE}

Datasets can be partitioned, sliced, in different ways.  One obvious
way is to use round robin, where each consecutive data row is written
to the next slice, modulo the number of slices.  This leads to
datasets with approximately equal number of rows per slice.  Another
alternative to slicing is to slice based on the hash value of a
particular column's values.  Using this method, all rows with the same
value in the hash column end up in the same slice.  This is efficient
for some parallel processing tasks.




\subsection{Dataset Column Types}
\label{sec:dataset-typing}
There are a number of useful types available for dataset columns.
They include floating and integer point numbers, booleans, timestamps,
several string types, and json types.  Most of these types come with
advanced parsers, making importing data from text files
straightforward with deterministic handling of errors, overflows, and
so on.



\subsection{Dataset Attributes}
The dataset has a number of attributes associated with it, such as
shape, number of rows, column names and types, and more.
An attribute is accessed like this
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.shape)
    print(datasets.source.columns)
\end{python}
and so on.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage
\section{Iterators: Working with Data}

Data in a dataset is typically accessed using an \emph{iterator} that
reads and streams one dataset slice at a time to a CPU core.  In this
section, we'll have a look at iterators for reading data, how to take
advantage of slicing to have parallel processing, and how to
efficiently create datasets.

\subsection{Iterator Basics}

Assume that we have a dataset with a column containing movie titles
named \texttt{movie}, and we want to know the ten most frequent
movies.  Consider the following example of a complete method
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(datasets.source.iterate(None, 'movie'))
    print(c.most_common(10))
\end{python}
This will print the ten most common movie titles and their
corresponding counts in the \texttt{source} dataset.  The code will
run on a single CPU core, because of the \texttt{synthesis} function,
which is called only once.  The \texttt{iterate} (class-)method
therefore has to read through all slices, one at a time, in a serial
fashion, and this is reflected by the first argument to the iterator
being \texttt{None}.


\subsection{Parallel Execution}
The Accelerator is much about parallel processing, and since datasets
are sliced, we can modify the above program to execute in parallel by
doing the following modification
\begin{python}
def analysis(sliceno):
    return Counter(datasets.source.iterate(sliceno, 'movie'))

def synthesis(analysis_res)
    c = analysis_res.merge_auto()
    print(c.most_common(10))
\end{python}
Here, we run \texttt{iterate} inside the \texttt{analysis()} function.
This function is forked once for each slice, and the argument
\texttt{sliceno} will contain an integer between zero and the number
of slices minus one.  The returned value from the analysis functions
will be available as input to the synthesis function in the
\texttt{analysis\_res} Python iterable.  It is possible to merge the
results explicitly, but the iterator comes with a rather magic method
\texttt{merge\_auto()}, which merges the results from all slices into
one based on the data type.  It can for example merge
\texttt{Counter}s, \texttt{set}s, and composed types like
\texttt{set}s of \texttt{Counter}s, and so on.


\subsection{Iterating over Several Columns}
Since each column is stored independently in a dataset, there is no
overhead from reading a subset of a dataset's columns.  In the
previous section we've seen how to iterate over a single column using
\texttt{iterate}.  Iterating over more columns is straightforward by
feeding a list of column names to \texttt{iterate}, like in this
example
\begin{python}
from collections import defaultdict
datasets = {'source',}

def analysis(sliceno):
    user2movieset = defaultdict(set)
    for user, movie in datasets.source.iterate(sliceno, ('user', 'movie')):
    user2movieset[user].add(movie)
    return user2movieset
\end{python}
This example creates a lookup dictionary from users to sets of movies.
It is also possible to iterate over all columns by specifying an empty
list of columns or by using the value \texttt{None}.
\begin{python}
...
def analysis(sliceno):
    for columns in datasets.source.iterate(sliceno, None):
    print(columns)
    break
\end{python}
This example will print the first row for each slice of a dataset and
then exit.


\subsection{Iterating over Dataset Chains}

Previously, we've seen how to iterate over a single dataset using
\texttt{iterate}.  There is a corresponding function,
\texttt{iterate\_chain}, that is used for iterating over chains of
datasets.  This function takes a number of arguments, such as
\begin{itemize}
\item[] \texttt{length}, i.e.\ the number of datasets to iterate over.
  By default, it will iterate over all datasets in the chain.
\item[] \texttt{callbacks}, functions that can be called before and/or
  after each dataset in a chain.  Very useful for aggregating data
  between datasets.
\item[] \texttt{stop\_id} which stops iterating at a certain dataset.
  This dataset could be from \textsl{another} job's parameters, so we
  can for example iterate exactly over all new datasets not covered by
  a previous job.

\item[] \texttt{range}, which allows for iterating over a range of
  data.
\end{itemize}
The \texttt{range} options is based on the max/min values stored for
each column in the dataset.  Assuming that the chain is sorted, one
can for example set \mintinline{python}{range={timestamp, ('2016-01-01', '2016-01-31')}}
in order to get rows within the specified range only.  The %
\texttt{range} is quite costly, since it requires each row in the
dataset chain with dates within the range to be checked against the
range criterion.  Therefore, there is a \texttt{sloppy} version that
iterates over complete datasets in the chain that contains at least
one row with a date within the range.  This is useful, for example, to
very quickly produce histograms or plots of subsets of the data.


\subsection{Asserting the Hashlabel}
Depending on how the parallel processing is implemented in a method,
some methods will only work if the input datasets are hashed on a
certain column.  To make sure this is the case, there is an optional
\texttt{hashlabel} parameter to the iterators that will cause a
failure if the supplied column name does not correspond to the
dataset's hashlabel.



It is also possible to have the iterate re-hash on-the-fly.  In
general this is not recommended, since there is a
\texttt{dataset\_rehash} method that does the same and stores the
result for immediate re-use.  Using \texttt{dataset\_rehash} will be
much more efficient.

\subsection{Dataset Translators and Filters}

The iterator may perform data translation and filtering on-the-fly
using the \texttt{translators} and \texttt{filters} options.  Here is
an example of how a dictionary can be fed into the iterator to map a
column
\begin{python}
mapper = {2: "HUMANLIKE", 4: "LABRADOR", 5: "STARFISH",}
for animal in datasets.source.iterate_chain(sliceno, \
  "NUM_LEGS", translator={"NUM_LEGS": mapper,}):
    ...
\end{python}
This will iterate over the \texttt{NUM\_LEGS} column, and map numbers
to strings according to the \texttt{mapper} dict.

Filters work similarly.



\subsection{Job Execution Flow and Result Passing}

There are three functions in a method that are called from the
Accelerator when a method is running, and they are \texttt{prepare()},
\texttt{analysis()}, and \texttt{synthesis()}.  All three may exist in
the same method, and at least one is required.  When the method
executes, they are called one after the other.
\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of \texttt{prepare()}.  Common
  input paramerers are \texttt{sliceno}, holding the number of the
  current process instance, and \texttt{prepare\_res}.  The return
  value for each process becomes available in the
  \texttt{analysis\_res} variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  parameters.  The latter is an iterator of the results from the
  parallel processes.
\end{itemize}
Figure~\ref{fig:prepanasyn} shows the execution order from top to
bottom, and the data passed between functions in coloured branches.
\texttt{prepare()} is executed first, and its return value is
available to both the \texttt{analysis()} and \texttt{synthesis()}
functions.  There are \texttt{slices} (a configurable parameter)
number of parallel \texttt{analysis()} processes, and their output is
available to the \texttt{synthesis()} function, which is executed
last.

Return values from any of the three functions may be stored in the
job's directory making them available to other jobs.


\begin{figure}[h!]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}



\subsection{Job Parameters}
\label{sec:jobparams}
We've seen how jobids from completed jobs can be used as input to new
jobs.  Jobid parameters is one of three kinds of input parameters that
a job can take.  Here the input parameters are summarised:
\begin{itemize}
\item[] \texttt{jobids}, a set of identifiers to previously executed jobs;
\item[] \texttt{options}, a dictionary of options; and
\item[] \texttt{datasets}, a set of input \textsl{datasets}, explained later.
\end{itemize}
See Figure~\ref{fig:execflow}.  Parameters are entered as global
variables early in the method's source.



\subsubsection*{The Params Variable}
A job's parameters are always available in the \texttt{params}
variable.  The \texttt{params} variable is make available by adding it
as input to \texttt{prepare()}, \texttt{analysis()}, or
\texttt{synthesis()}, like this
\begin{python}
def synthesis(params):
    print(params)
\end{python}
Accessing the \texttt{params} variable for \textsl{another} job is
done like this
\begin{python}
jobids = {'thejob',}

def synthesis():
    print(jobids.thejob.params)
\end{python}
it turns out it can be very useful to know for example which datasets
another job has been processing, and so on.

Apart from the input parameters, the \texttt{params} variable also
gives access to more information about the job and the current
Accelerator setup, such as the total number of slices, the random
seed, the hash of the source code, and method execution start time.


\begin{figure}[h!]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobids},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}


\subsubsection*{Input parameter \texttt{jobids}}
The \texttt{jobids} parameter is a set containing any number of input
jobids.  For example
\begin{python}
jobid = {'sales_stats', 'overhead_stats',}
\end{python}
When a method is to be executed, actual links to jobs, i.e.\ jobids,
are passed using these parameters.


\subsubsection*{Input parameter \texttt{options}}
Options are supplied as a dictionary and is very flexible in typing.  For example
\begin{python}
options = dict(
    name = 'alice',
    defaultnames = set('alice', 'bob'),
    param1 = 1,
    param2 = float,
)
\end{python}
Here, values are defaults, so \texttt{param1} is default set to 1,
while a floating point value must be supplied to \texttt{param2}.

\subsubsection*{Input parameter \texttt{datasets}}
Datasets will be described in more details later, and the
\texttt{datasets} parameter is similar to the \texttt{jobids}
parameter.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

