
This chapter presents an overview of the Accelerator's features in a
rather non-formal way.  It is based on an article published on the
eBay Tech Blog website.



\section{High Level View}

The Accelerator is a client-server based application, and from a high
level, it can be visualised like in figure~\ref{fig:overview}.
\begin{figure}[h!]
  \begin{center}
    \input{figures/overview.pdftex_t}
    \caption{High level view of the Accelerator framework.  See text
      for details.}
    \label{fig:overview}
  \end{center}
\end{figure}

On the left side there is the \texttt{run} program.  To the right, there
are two servers, called \texttt{daemon} and \texttt{urd}.  The
\texttt{run} program runs what is called \texttt{build scripts}, that
execute jobs on the \texttt{daemon} server.  This server will load and
store information and results for all jobs executed using the
\textsl{workdirs} file system based database.

In parallel, all jobs covered by a build script may be stored by the
\texttt{urd} server into the \textsl{job logs} file system database.
\texttt{urd} is also responsible for finding collections, or lists, of
related previously executed jobs.  The \texttt{urd} server is very
capable and helps ensuring reproducibility and transparency, and it
will be further discussed in section~\ref{}.


\section{Jobs}
The basic operation of the Accelerator is to execute small Python
programs called \textsl{methods}.  In a method, some reserved function
names are used to execute code sequentially or in parallel and to pass
parameters and results.  A method that has completed execution is
called a \textsl{job}.

Jobs are stored in \textsl{job directories}.  A dedicated directory
will be created for each new job, and the directory will contain all
information regarding the job, such as its input parameters, stored
files, return values, profiling information, and more.

The Accelerator keeps a record of all jobs that have been run.  It
turns out that this is very useful for avoiding unnecessary
re-computing and instead rely on reusing previously computed results.
This does not only speed up processing and encourage incremental
design, but also makes it transparent which code and which data was
used for any particular result, thus reducing uncertainty.




\subsection{A Very Simple Job:  ``Hello, World''}

The following example method is very simple.  It does not take any
input parameters and does almost nothing, it will just return the
string ``\texttt{hello world}'' and exit.
\begin{python}
def synthesis():
    return "hello world"
\end{python}
In order to get the method to execute, it is called from a
\textsl{build script} looking something like this
\begin{python}
def main(urd)
    job = urd.build('hello_world')
\end{python}
The \texttt{urd} object contains functions for job building and
organisation, and is described in detail in section~\ref{}.  Remember
that during the job build process, a job directory is created that
will contain everything associated with the build.

When execution is completed, a job object is returned to the user.  In
the example above, the variable \texttt{job} will be of type
\texttt{Job}.  This object provides a convenient interface to the data
in the corresponding job directory.



\subsection{Trying to Run the Job Again}
If the build script is executed again, the job will not be re-built,
simply because the Accelerator remembers that the job has been built
in the past and its associated information is stored in a job
directory.  Instead, the Accelerator immediately returns a job object
representing the previous run.  This means that from a user's
perspective, there is no difference between job running and job result
recalling!  In order to have the job executing again, either the
source code or input parameters have to change.



\subsection{Back to the ``Hello, World'' example}
Figure~\ref{fig:execflow-hello-world} illustrates the dispatch of the
\texttt{hello\_world} method.  The created job gets the \textsl{jobid}
\texttt{test-0}, and parts of the corresponding job directory
information is shown in green.  The job directory contains several
files, of which the most important to here are
\begin{itemize}
\item[] \texttt{setup.json}, containing job information;
\item[] \texttt{result.pickle}, containing the returned data; and
\item[] \texttt{method.tar.gz}, containing the method's source code.
\end{itemize}

\begin{figure}[h!]
  \begin{center}
    \input{figures/job0.pdftex_t}
    \caption{A simple hello world program, represented as graph and
      work directory.}
    \label{fig:execflow-hello-world}
  \end{center}
\end{figure}

The \texttt{Job} class provides a convenient way to access important
files in this directory.  For example, the job's return value can be
loaded into a variable using the \texttt{.load()} function, like this
\begin{python}
def main(urd)
    job = urd.build('hello_world')
    print(job.load())
\end{python}
Running this build script will print the string to the \texttt{run}
program's standard output.


Jobids are job identifiers, that are named by their corresponding
\textsl{workdir} plus an integer counter value.  Workdirs are used to
separate jobs into different physical locations, and jobs may be
shared between users on the workdir level.  A running Accelerator
could have any number of workdirs associated, of which one is default for writing.



\subsection{Linking Jobs}

Using jobs, complex tasks can be split into several smaller
operations.  Jobs can be connected so that the next job will depend on
the result of a previous job or set of jobs, and so on.

To continue the simple example, assume for a second that the ``hello
world''-job is computationally expensive, and that it returns a result
that is to be used as input to further processing.  To keep things
simple, this further processing is represented by printing the result
to standard output.  A new method \texttt{print\_result} is created,
and it goes like this
\begin{python}
jobids = {'hello_world_job',}

def synthesis():
    print(jobids.hello_world_job.load())
\end{python}
This method expects the \texttt{hello\_world\_job} input parameter to
be provided at execution time, and this is accomplished by the
following build script
\begin{python}
def main(urd):
    job1 = urd.build('hello_world')
    job2 = urd.build('print_result', options=dict(hello_world_job=job1))
\end{python}
The \texttt{print\_result} method then loads the result from the
provided job and prints its contents to \texttt{stdout}.  Note that
this method does not return anything.

Figure~\ref{fig:execflow-print-result} illustrates the situation.
(Note the direction of the arrow: the second job, \texttt{test-1} has
\texttt{test-0} as input parameter, but \texttt{test-0} does not know
of any jobs run in the future.  Hence, arrows point to previous jobs.)

\begin{figure}[h!]
  \begin{center}
    \input{figures/job0job1.pdftex_t}
    \caption{Job \texttt{test-0}, is used as input to the
      \texttt{print\_result} job.}
    \label{fig:execflow-print-result}
  \end{center}
\end{figure}

The example shows how a complex task may be split into several jobs,
each reading intermediate results from previous jobs.  The Accelerator
will keep track of all job dependencies, so there is no doubt which
jobs that are run when and on which data.  Furthermore, since the
Accelerator remembers if a job has been executed before, it will link
and ``recycle'' previous jobs.  This may bring a significant
improvement in execution speed.  Furthermore, a recycled job is a
proof of that the code, input- and output data is connected.


\section{Datasets: Storing Data}

The \texttt{dataset} is the Accelerator's default storage type for
small or large quantities of data, designed for parallel processing
and high performance.  Datasets are built on top of jobs, so
\emph{datasets are created by methods and stored in job directories,
  just like any job result.}

Internally, data in a dataset is stored in a row-column format, and is
typically \emph{sliced} into a fixed number of slices to allow
efficient parallel access, see figure~\ref{fig:dataset}. Columns are
accessed independently, so there is no overhead in reading a single or
a set of columns.


\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_files.pdftex_t}
    \caption{A dataset containing three columns, $A$, $B$, and $C$
      stored using two slices.  Each dotted box corresponds to a file,
      so there are two files for each column, allowing for parallel
      read of the data using two processes.}
    \label{fig:dataset}
  \end{center}
\end{figure}

Furthermore, datasets may be \textsl{hash partitioned}, so that
slicing is based on the hash value of a given column.  Slicing on, for
example, a column containing some ID string will partition all rows
such that rows corresponding to any particular ID is stored in a
single slice only.  In many practical applications, hash partitioning
makes parallel processes independent, minimising the need for
complicated merging operations.  This is explained further in
chapter~\ref{sec:why_hashing}.



\subsection{Importing Data}

A project typically starts with \textsl{importing} some data from a
file on disk.  The bundled method \texttt{csvimport} is designed to
parse a plethora of ``comma separated values''-file formats and store
the data as a dataset.  See figure~\ref{fig:dataset_csvimport}.
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_file1.pdftex_t}
    \caption{Importing \texttt{file0.txt}.}
    \label{fig:dataset_csvimport}
  \end{center}
\end{figure}
The method takes several input options in addition to the mandatory
filename to control the import process.  Here is an example
(non-simplified) invocation
\begin{python}
def main(urd):
    jid = urd.build('csvimport', option=dict(filename='file0.txt'))  
\end{python}
When executed, the created dataset will be stored in the resulting job
directory, and the name of the dataset will by default be the jobid
plus the string \texttt{default}.  For example, if the
\texttt{csvimport} jobid is \texttt{imp-0}, the dataset will be
referenced by \texttt{imp-0/default}.  In this case, and always when
there is no ambiguity, the jobid alone (\texttt{imp-0}) could be used
too.  In general, a job could contain any number of datasets, but a
single dataset is a common case.




\subsection{Linking Datasets, Chaining}

Just like jobs can be linked to each other, datasets can link to each
other too.  Since datasets are build on top of jobs, this is
straightforward.  Assume the file \texttt{file0.txt} is imported into
dataset \texttt{imp-0/default}, and that there is more data like it
stored in the file \texttt{file1.txt}.  The second file is imported
with a link to the first dataset, see
figure~\ref{fig:dataset_csvimport_chain}.
\begin{figure}[h!]
  \begin{center}
    \input{figures/import_file0file1.pdftex_t}
    \caption{Chaining the import of \texttt{file1.txt} to the previous
      import of \texttt{file0.txt}.}
    \label{fig:dataset_csvimport_chain}
  \end{center}
\end{figure}
The \texttt{imp-1} (or \texttt{imp-1/default}) dataset reference can
now be used to access all data imported from \textsl{both} files!

Linking datasets containing related content is called \emph{chaining},
and this is particularly convenient when dealing with data that grows
over time.  A good example is any kind of \emph{log} data, such as
logs of transactions, user interactions, and similar.  Using chaining,
datasets can be with more rows just by linking, which is a lightweight
constant time operation.



\subsection{Adding New Columns to a Dataset}
In the previous section it was shown that datasets can be chained and
thereby grow in number of rows.  A dataset chain is created simply by
linking one dataset to the other, so the overhead is minimal.  In this
section it is shown that it is equally simple to add new columns to
existing datasets.  Adding columns is a common operation and the
Accelerator handles this situation efficiently using links.

The idea is very simple.  Assume a ``source'' dataset to which one or
more new columns should be added.  A new dataset is created containing
\textsl{only} the new column(s), and while creating it, the
constructor is instructed to link all the source dataset's columns to
the new dataset such that the new dataset appears to contain all
columns from both datasets.  (Note that this linking is similar to but
different from chaining.)

Accessing the new dataset will transparently access all the columns in
both the new and the source dataset in parallel, making it
indistinguishable from a single dataset.  See
Figure~\ref{fig:dep_dataset_append_column}.

\begin{figure}[h!]
  \begin{center}
    \input{figures/dataset_append_column.pdftex_t}
    \caption{Adding one new column to the source dataset.}
    \label{fig:dep_dataset_append_column}
  \end{center}
\end{figure}

A common case is to compute new columns based on existing ones.  In
this case, values are written to the new columns in the new dataset
while reading from the iterator iterating over the existing columns in
the source dataset.  This will be discussed in detail in
section~\ref{sec:appending_new_columns}



\subsection{Multiple Datasets in a Job}

Typically, a method creates a single dataset in the job directory, but
there is no limit to how many datasets that could be created and
stored in a single job directory.  This leads to some interesting
applications.

One application for keeping multiple datasets in a job is when data is
split into subsets based on some condition.  This could, for example,
be when a dataset is split into a training set and a test set.  One
way to achieve this using the Accelerator is by creating a Boolean
column that tells if the current row is train or test data, followed
by a job that splits the dataset in two based on the value on that
column.  See Figure~\ref{fig:dep_dataset_csvimport_chain}.

\begin{figure}[h!]
  \hspace{1cm}
  \input{figures/filter_dataset.pdftex_t}
  \caption{\texttt{job-1} separates the dataset
    \texttt{job-0/default} into two new datasets, named
    \texttt{job-1/train} and \texttt{job-1/test}.}
  \label{fig:dep_dataset_csvimport_chain}
\end{figure}

%% The figure shows how \texttt{job-1} has created two datasets,
%% \texttt{job-1/train} and \texttt{job-1/test}, based on the input
%% dataset \texttt{job-0/default}.  A third job, \texttt{job-2} is then
%% accessing the \texttt{job-1/train} dataset.  (Note that \texttt{job-1}
%% does not have a \texttt{default} dataset.)

In the setup of figure~\ref{fig:dep_dataset_csvimport_chain} we have
full tracking from either \texttt{train} or \texttt{test} datasets.
If we want to know the source of one of these sets, we just follow the
links back to the previous jobs until we reach the source job.  In the
figure, \texttt{job-0} may for example be a \texttt{csvimport} job,
and will therefore contain the name of the input file in its
parameters.  Thus, it is straightforward to link any data to its
source.

Splitting a dataset into parts creates ``physical'' isolation while
still keeping all the data at the same place.  No data is lost in the
process, and this is good for transparency reasons.  For example, a
following method may iterate over \textsl{both} datasets in
\texttt{job-1} and by that read the complete dataset.



\subsection{Parallel Dataset Access and Hashing}
As shown earlier in this chapter, data in datasets is stored in
multiple files for two reasons.  One reason is that we can read only
the columns that we need, without overhead, and the other is to allow
fast parallel reads.  The parameter \texttt{slices} determines how
many slices that the dataset should be partitioned into, and it also
sets the number of parallel process that may be used for processing
the dataset.  There is always one process for each slice of the
dataset, and each process operates on a unique part of the dataset.

Datasets can be partitioned, sliced, in different ways.  One obvious
way is to use round robin, where each consecutive data row is written
to the next slice, modulo the number of slices.  This leads to ``well
balanced'' datasets with approximately equal number of rows per slice.
Another alternative to slicing is to slice based on the hash value of
a particular column's values.  Using this method, all rows with the
same value in the hash column end up in the same slice.  This is
efficient for many parallel processing tasks, and we'll talk more
about it later on.

Methods may be designed simpler and more efficient using hash
partitioning, since the partitioning ensures some kind of data
independence between slices and processes.  If, however, the same
method is used on data that is not partitioned in the expected way, it
will not process the data correctly.  To ensure that an assumption
about hash partitioning is correct, there is an optional
\texttt{hashlabel} parameter to the iterators that will cause a
failure if the supplied column name does not correspond to the
dataset's hashlabel.

On the other hand it is also possible to have the iterator re-hash
on-the-fly.  In general this is not recommended, since there is a
\texttt{dataset\_rehash} method that does the same and stores the
result for immediate re-use.  Using \texttt{dataset\_rehash} will be
much more efficient.




\subsection{Dataset Column Types}

There are a number of useful types available for dataset columns.
They include \textsl{floating} and \textsl{integer point numbers},
\textsl{Booleans}, \textsl{timestamps}, several \textsl{string types}
(handling all kinds of encodings), and \textsl{json} types for storing
arbitrary data collections.  Most of these types come with advanced
parsers, making importing data from text files straightforward with
deterministic handling of errors, overflows, and so on.



\subsection{Dataset Attributes}
The dataset has a number of attributes associated with it, such as
shape, number of rows, column names and types, and more.
An attribute is accessed like this
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.shape)
    print(datasets.source.columns)
\end{python}
and so on.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Iterators: Working with Data}

Data in a dataset is typically accessed using an \emph{iterator} that
reads and streams one dataset slice at a time to a CPU core.  The
parallel processing capabilities of the Accelerator makes it possible
to dispatch a set of parallel iterators, one for each slice, in order
to have efficient parallel processing of the dataset.

This section shows how iterators are used for reading data, how to
take advantage of slicing to have parallel processing, and how to
efficiently create new datasets.

\subsection{Iterator Basics}

Assume a dataset that has a column containing movie titles named
\texttt{movie}, and the problem is to extract the ten most frequent
movies.  Consider the following complete example
\begin{python}
from collections import Counter
datasets = ('source',)

def synthesis():
    c = Counter(datasets.source.iterate(None, 'movie'))
    print(c.most_common(10))
\end{python}
This will print the ten most common movie titles and their
corresponding counts in the \texttt{source} dataset.  The code will
run on a single CPU core, because we use the single-process
\texttt{synthesis} function, which is called and executed only once.
The \texttt{.iterate} (class-)method therefore has to read through all
slices, one at a time, in a serial fashion, and this is reflected by
the first argument to the iterator being \texttt{None}.



\subsection{Parallel Execution}
The Accelerator is much about parallel processing, and since datasets
are sliced, the program can be  modified to execute in parallel by
doing the following modification
\begin{python}
def analysis(sliceno):
    return Counter(datasets.source.iterate(sliceno, 'movie'))

def synthesis(analysis_res)
    c = analysis_res.merge_auto()
    print(c.most_common(10))
\end{python}
Here, \texttt{.iterate} is run inside the \texttt{analysis()}
function.  This function is forked once for each slice, and the
argument \texttt{sliceno} will contain an integer between zero and the
number of slices minus one.  The returned value from the analysis
functions will be available as input to the synthesis function in the
\texttt{analysis\_res} Python iterable.  It is possible to merge the
results explicitly, but the iterator comes with a rather magic method
\texttt{merge\_auto()}, which merges the results from all slices into
one based on the data type.  It can for example merge
\texttt{Counter}s, \texttt{set}s, and composed types like
\texttt{set}s of \texttt{Counter}s, and so on.  For larger datasets,
this version will run much faster.



\subsection{Iterating over Several Columns}
Since each column is stored independently in a dataset, there is no
overhead from reading a subset of a dataset's columns.  In the
previous section we've seen how to iterate over a single column using
\texttt{iterate}.  Iterating over more columns is straightforward by
feeding a list of column names to \texttt{iterate}, like in this
example
\begin{python}
from collections import defaultdict
datasets = {'source',}

def analysis(sliceno):
    user2movieset = defaultdict(set)
    for user, movie in datasets.source.iterate(sliceno, ('user', 'movie')):
        user2movieset[user].add(movie)
    return user2movieset
\end{python}
This example creates a lookup dictionary from users to sets of movies.
Note that in this case, we would like to have the dataset hashed on
the \texttt{user} column, so that each user appears in exactly one slice.
This will make later merging (if necessary) much easier.

It is also possible to iterate over all columns by specifying an empty
list of columns or by using the value \texttt{None}.
\begin{python}
...
def analysis(sliceno):
    for columns in datasets.source.iterate(sliceno, None):
        ...
\end{python}
Here, \texttt{columns} will be a list of values, one for each column
in the dataset.


\subsection{Iterating over Dataset Chains}

The \texttt{iterate} function is used to iterate over a single
dataset.  There is a corresponding function, \texttt{iterate\_chain},
that is used for iterating over chains of datasets.  This function
takes a number of arguments, such as
\begin{itemize}
\item[] \texttt{length}, i.e.\ the number of datasets to iterate over.
  By default, it will iterate over all datasets in the chain.
\item[] \texttt{callbacks}, functions that can be called before and/or
  after each dataset in a chain.  Very useful for aggregating data
  between datasets.
\item[] \texttt{stop\_id} which stops iterating at a certain dataset.
  This dataset could be from \textsl{another} job's parameters, so we
  can for example iterate exactly over all new datasets not covered by
  a previous job.
\item[] \texttt{range}, which allows for iterating over a range of
  data.
\end{itemize}
The \texttt{range} options is based on the max/min values stored for
each column in the dataset.  Assuming that the chain is sorted, one
can for example set
\begin{python}
  range={timestamp, ('2016-01-01', '2016-01-31')}
\end{python}
in order to get rows within the specified range only.  Using
\texttt{range=} is quite costly, since it requires each row in the
dataset chain with dates within the range to be checked against the
range criterion.  Therefore, there is a \texttt{sloppy} version that
iterates over complete datasets in the chain that contains at least
one row with a date within the range.  This is useful, for example, to
very quickly produce histograms or plots of subsets of the data.



\subsection{Dataset Translators and Filters}

The iterator may perform data translation and filtering on-the-fly
using the \texttt{translators} and \texttt{filters} options.  Here is
an example of how a dictionary can be fed into the iterator to map a
column
\begin{python}
mapper = {2: "HUMANLIKE", 4: "LABRADOR", 5: "STARFISH",}
for animal in datasets.source.iterate_chain(sliceno, \
  "NUM_LEGS", translator={"NUM_LEGS": mapper,}):
    ...
\end{python}
This will iterate over the \texttt{NUM\_LEGS} column, and map numbers
to strings according to the \texttt{mapper} dict.

Filters work similarly.



\subsection{Job Execution Flow and Result Passing}

Execution of code in a method is either parallel or serial depending
on which function is used to encapsulate it.  There are three
functions in a method that are called from the Accelerator when a
method is running, and they are \texttt{prepare()},
\texttt{analysis()}, and \texttt{synthesis()}.  All three may exist in
the same method, and at least one is required.  When the method
executes, they are called one after the other.
\begin{itemize}
\item[] \texttt{prepare()} is executed first.  The returned value is
  available in the variable \texttt{prepare\_res}.
\item[] \texttt{analysis()} is run in parallel processes, one for each
  slice.  It is called after completion of \texttt{prepare()}.  Common
  input parameters are \texttt{sliceno}, holding the number of the
  current process instance, and \texttt{prepare\_res}.  The return
  value for each process becomes available in the
  \texttt{analysis\_res} variable.
\item[] \texttt{synthesis()} is called after the last
  \texttt{analysis()}-process is completed.  It is typically used to
  aggregate parallel results created by \texttt{analysis()} and takes
  both \texttt{prepare\_res} and \texttt{analysis\_res} as optional
  parameters.  The latter is an iterator of the results from the
  parallel processes.
\end{itemize}
Figure~\ref{fig:prepanasyn} shows the execution order from top to
bottom, and the data passed between functions in coloured branches.
\texttt{prepare()} is executed first, and its return value is
available to both the \texttt{analysis()} and \texttt{synthesis()}
functions.  There are \texttt{slices} (a configurable parameter)
number of parallel \texttt{analysis()} processes, and their output is
available to the \texttt{synthesis()} function, which is executed
last.

Return values from any of the three functions may be stored in the
job's directory making them available to other jobs.


\begin{figure}[h!]
  \begin{center}
    \input{figures/prepanasyn.pdftex_t}
    \caption{Execution flow and result propagation in a method.}
    \label{fig:prepanasyn}
  \end{center}
\end{figure}



\subsection{Job Parameters}
\label{sec:jobparams}
We've seen how jobids from completed jobs can be used as input to new
jobs.  Jobid parameters is one of three kinds of input parameters that
a job can take.  Here the input parameters are summarised:
\begin{itemize}
\item[] \texttt{jobids}, a set of identifiers to previously executed jobs;
\item[] \texttt{options}, a dictionary of options; and
\item[] \texttt{datasets}, a set of input \textsl{datasets}.
\end{itemize}
See Figure~\ref{fig:execflow}.  Parameters are entered as global
variables early in the method's source.


\begin{figure}[h!]
  \begin{center}
    \input{figures/execflow.pdftex_t}
    \caption{Execution flow of a method.  The method takes optionally
      three kinds of parameters: \texttt{options}, \texttt{jobids},
      and \texttt{datasets}.}
    \label{fig:execflow}
  \end{center}
\end{figure}





\section{A Class Based Programming Model}

\begin{figure}[h!]
  \begin{center}
    \input{figures/classes1.pdftex_t}
    \caption{xxx}
    \label{fig:classes}
  \end{center}
\end{figure}

\begin{figure}[h!]
  \begin{center}
    \input{figures/classes2.pdftex_t}
    \caption{xxx}
    \label{fig:classes}
  \end{center}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

