The Accelerator is a very efficient environment for running and
scheduling big data tasks.  It has been in continuous development
since 2012, and has been used in seven different projects, of which
three has been live to customers, and the other four was data analysis
and insight projects.

Due to its minimalistic implementation, the Accelerator is capable of
working at high speed with terabytes of data with billions of lines on
a single computer.  Two things stand out: first, all jobs are
bookkeeped in a novel way; and second, data is represented and
accessed with close to zero overhead.  The end result is a globally
optimised data processing machine with a wide spectrum of uses.

Some Accelerator features
\begin{itemize}
\item \textbf{Data integration} - The Accelerator has been used in
  several projects with different customers and data, and has thereby
  adopted to a number of formats and cases.
\item \textbf{Efficient Data Access} - Data is streamed through jobs
  using low level operating- and filesystem primitives.
\item \textbf{Simple job tracking} - Easy to find source data, easy to
  use results from other users.
\item \textbf{Transparency} - Total context for all jobs ever run is
  stored and straightforward to retreive.  Any job could be retrieved
  or replayed.
\end{itemize}
