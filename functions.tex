
This chapter is dedicated to the Accelerator's helper functions.
Some, like the \texttt{blob} module, are very useful, while others are
less likely to be included in a project.
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@22


\section{The \texttt{JobID} Class}
The \texttt{JobID} class is derived from the \texttt{str} class and
has some additional methods.

It appears everywhere a jobid appears, most commonly in
- returned from build / subjob.build
- seen by jobids=('dd',)
- urd

The \texttt{JobID} has the following methods
\starttabletwo
\texttt{method} & In a \texttt{JobList} this is set
to the name of the method.  Almost everywhere else it is set
to \pyNone.\\
\texttt{number} & The sequence number of the jobid\\
\texttt{workdir} & The name of the workdir, i.e.\ the part before the hyphen in the jobid string.\\
\texttt{path} & Absolute path to where the job is stored\\
\texttt{filename(<file>)} & Absolute path to a \texttt{file} stored in the job.\\
\texttt{params()} & The job's \texttt{params} dictionary.\\
\texttt{post()} & The job's post data, containing profiling information.\\
\stoptabletwo
It may seem strange at first that the \texttt{method} does not always
contain the name of the method corresponding to the job.  The reason
for this is that the jobid is commonly used as a job identifier.  In
order to know the method name, it has to be looked up inside the job,
and there is usually no point in doing that.





\section{The \texttt{JobList} Class}

\starttabletwo
\texttt{find(<method>)} & a new JobList with only jobs with that method in it.\\
\texttt{get(<method>, default=None)} & latest JobID with that method\\
\texttt{[<method>]} & Same as .get but error if no job with that method is in the list.\\
\texttt{as\_tuples} &  The same list but as (method, jid) tuple\\
\texttt{pretty} & a pretty-printed version (string)\\
\texttt{print\_profile} & \\
\stoptabletwo





\section{The \texttt{Dataset} Class}

    Represents a dataset. Is also a string 'jobid/name', or just 'jobid' if
        name is 'default' (for better backwards compatibility).

    You usually don't have to make these yourself, because datasets.foo is
        already a Dataset instance (or None).

    You can pass jobid=''jid/name'' or jobid=''jid'', name=''name'', or skip
        name completely for ``default''.

    You can also pass jobid={jid: dsname} to resolve dsname from the datasets
        passed to jid. This gives None if that option was unset.

    These decay to a (unicode) string when pickled.


\starttabletwo
\texttt{columns} & \\
\texttt{previous} & \\
\texttt{parent} & \\
\texttt{filename} & \\
\texttt{hashlabel} & \\
\texttt{caption} & \\
\texttt{lines} & \\
\texttt{shape} & \\
\texttt{link\_to\_here} &\\
\texttt{column\_filename} & \\
\texttt{chain} & \\
\texttt{iterate\_chain} & \\
\texttt{iterate} & \\
\texttt{iterate\_list} & \\
\stoptabletwo





\section{The \texttt{DatasetWriter} Class}

\begin{verbatim}
    Create in prepare, use in analysis. Or do the whole thing in
        synthesis.

    You can pass these through prepare_res, or get them by trying to
        create a new writer in analysis (don't specify any arguments except
            an optional name).

    There are three writing functions with different arguments:

    dw.write_dict({column: value})
        dw.write_list([value, value, ...])
            dw.write(value, value, ...)

    Values are in the same order as you add()ed the columns (which is in
        sorted order if you passed a dict). The dw.write() function names the
            arguments from the columns too.

    If you set hashlabel you can use dw.hashcheck(v) to check if v
        belongs in this slice. You can also call enable_hash_discard
            (in each slice, or after each set_slice), then the writer will
                discard anything that does not belong in this slice.

    If you are not in analysis and you wish to use the functions above
        you need to call dw.set_slice(sliceno) first.

    If you do not, you can instead get one of the splitting writer
        functions, that select which slice to use based on hashlabel, or
            round robin if there is no hashlabel.

    dw.get_split_write_dict()({column: value})
        dw.get_split_write_list()([value, value, ...])
            dw.get_split_write()(value, value, ...)

    These should of course be assigned to a local name for performance.

    It is permitted (but probably useless) to mix different write or
        split functions, but you can only use either write functions or
            split functions.

    You can also use dw.writers[colname] to get a typed_writer and use
        it as you please. The one belonging to the hashlabel will be
            filtering, and returns True if this is the right slice.

    If you need to handle everything yourself, set meta_only=True and
        use dw.column_filename(colname) to find the right files to write to.
            In this case you also need to call dw.set_lines(sliceno, count)
                before finishing. You should also call
                    dw.set_minmax(sliceno, {colname: (min, max)}) if you can.
\end{verbatim}

\starttabletwo
\texttt{add} & \\
\texttt{set\_slice} & \\
\texttt{column\_filename} & \\
\texttt{enable\_hash\_discard} & \\
\texttt{get\_split\_write} & \\
\texttt{get\_split\_write\_list} & \\
\texttt{get\_split\_write\_dict} & \\
\texttt{close} & \\
\texttt{discard} & \\
\texttt{set\_lines} & \\
\texttt{set\_minmax} & \\
\texttt{finish} & \\
\stoptabletwo




\section{The \texttt{DatasetChain} Class}

\starttabletwo
\texttt{min} & \\
\texttt{max} & \\
\texttt{lines} & \\
\texttt{column\_counts} & \\
\texttt{column\_count} & \\
\texttt{with\_column} & \\
\stoptabletwo





\section{The \texttt{Urd} Class}






\section{Generate Progress Messages:  the \texttt{status} Module}

The status module is used by the Accelerator to report processing
state.  It is also used by various functions to report iterator and
file access progress.  Status messages are presented in
the \texttt{run} shell by pressing \texttt{C-t}, i.e.\
the \texttt{Ctrl} and \texttt{t} keys simultaneously.

The status module can be used to write progress and status messages
for any function.  Here is an example of how to use the status module
\begin{python}
from accelerator.status import status
...
def analysis(sliceno):
    msg = "reached line %d already!"
    with status(msg % (0,) as update:
        for ix, data in enumerate(datasets.source.iterate(sliceno, 'data')):
            if ix % 1000000 == 0:
                update(msg % (ix,))
\end{python}
In the example above, the status message will be updated once every
million iteration.  By pressing \texttt{C-t} during its execution, the
user will get a message telling how many lines the iterator has
reached.





\section{Share Data Between Jobs:  the \texttt{blob} Module}

The simplest way to share reasonable amounts of data between jobs is
by using the \texttt{blob} module.  This module is a
convenience-wrapper around the Python \texttt{pickle} module.

Note that the Accelerator will set the ``current work directory'' to
the current job directory when building a method, so all files created
by a job will be stored in the current job directory, unless the
filename contains a path pointing elsewhere.

\subsection*{Storing/Loading a  Single File}
Data is saved in this way
\begin{python}
from accelerator import blob
def synthesis():
    data = ...  # some data created here
    blob.save(data, filename)
\end{python}
The data is loaded like this
\begin{python}
from accelerator import blob
def synthesis()
    data = blob.load(jobid, filename)
\end{python}
The \texttt{jobid} in \texttt{blob.load()} is not mandatory.  It
defaults to the current workdir unless specified.



\subsection{Storing/Loading a Sliced File}
It is also possible to use the \texttt{blob} module in \analysis.
From a user's perspective it will look like a single file is being
handled, but there is actually one file per slice.  This is how to do
it
\begin{python}
def analysis(sliceno):
    # save data in slices like this
    blob.save(data, filename, sliceno=sliceno)
    # load like this
    data = blob.load(filename, sliceno=sliceno)
\end{python}

Data can be passed ``in parallel'' between different jobs using this
feature.



\subsection{Default Value}

The value of the \texttt{default} parameter is returned if trying to
load a file that does not exist, for example
\begin{python}
x = blob.load('thisfiledoesnotexist', default=dict())
\end{python}
will set \texttt{x} to an empty dict if loading fails.



\subsection{Save Files for Debugging}
\label{sec:debugflag}
The \texttt{temp} argument controls persistence of the stored files.
By default it is being set to \pyFalse, which implies that the stored
file is \textsl{not} temporary.  But setting it to \pyTrue, like in
the following
\begin{python}
    blob.save(data, filename, temp=True)
\end{python}
will cause the stored file to be deleted upon job completion.  The
argument takes two additional values, \texttt{DEBUG} and
\texttt{DEBUGTEMP}, working like this
\vspace{3ex}

\begin{center}
\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}ll}
  \texttt{temp=}  & ``normal'' mode     & debug mode  \\\hline
  \pyFalse           & stored              & \\
  \pyTrue            & stored and removed  & stored and removed\\
  \texttt{DEBUG}     &                     & stored\\
  \texttt{DEBUGTEMP}\hspace{4ex} & stored and removed  & stored\\
\end{tabular*}
\end{center}
Debug mode is active if the Accelerator is started with the
\texttt{--debug} flag.

\noindent Example
\begin{python}
from accelerator.extras import Temp
def analysis(sliceno):
  # save only if --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUG)
  # save always, but remove unless --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUGTEMP)
\end{python}



\section{Find the Full Path to a File in Another Job}
Accessing a file stored in another job from within a method or build
script is simple, and the functionality is implemented
in \texttt{resolve\_jobid\_filename()}.  The function takes two
arguments, a \textsl{jobid} and a \textsl{filename}.  See the example
below
\begin{python}
from accelerator.extras import resolve_jobid_filename

jobids = ('oldjob',)

def synthesis():
    filename = resolve_jobid_filename(jobids.oldjob, 'nameoffile')
\end{python}
Note that this function works in a build script as well.



\section{Symlinking}
\label{sec:symlinking}
Creating a symlink, for example from the \texttt{result\_directory} to
current workdir, may be implemented in a simple and safe way like this
\begin{python}
from accelerator.extras import symlink

def synthesis(RESULT_DIRECTORY):

    # create file and write it to jobid
    ...
    with open(filename, 'wb') as fh:
        fh.write(...)

    # create a symlink to filename in RESULT_DIRECTORY
    symlink(filename, RESULT_DIRECTORY)
\end{python}
The \texttt{extras.symlink} function will write a soft link
to \texttt{filename} in \texttt{RESULT\_DIRECTORY}, overwriting it if
it already exists.



\section{job\_post}
The \texttt{job\_post} function returns a job's post execution
information as a Python \texttt{dict}
\begin{python}
from accelerator.extras import job_post
postinfo = job_post(jobid)
\end{python}
The post data contains mainly profiling information.



\section{json\_encode}
\begin{python}
from accelerator.extras import json_encode
json_encode(variable, sort_keys=True, as_str=False)
\end{python}

\starttabletwo
\RPtwo \texttt{variable} & variable to be serialised.  \texttt{set}s and
\texttt{tuple}s will be converted to \texttt{list}s.\\[1ex]

\RPtwo \texttt{sort\_keys} & Sort keys if \pyTrue.\\[1ex]

\RPtwo \texttt{as\_str} & return a \texttt{str} if \pyTrue, \texttt{bytes}
otherwise.
\stoptabletwo



\section{json\_decode}
\begin{python}
from accelerator.extras import json_decode
x = json_decode(s)
\end{python}
Return a datastructure defined by the string \texttt{s}.




\section{json\_save}
\begin{python}
from accelerator.extras import json_save
json_save(variable,
    filename='result',
    jobid=None,
    sliceno=None,
    sort_keys=True,
    _encoder=json_encode,
    temp=False
)
\end{python}




\section{json\_load}
\begin{python}
from accelerator.extras import json_load
x = json_load(
    filename='result',
    jobid='',
    sliceno=None,
    default=None,
    unicode_as_utf8bytes=PY2
)
\end{python}
The \texttt{unicode\_as\_utf8bytes} flag is active in Python2 in order
to let strings decode as bytes.  Override this for unicode.  In
Python3, strings decode as unicode.





\section{DotDict}
\begin{verbatim}
    """Like a dict, but with d.foo as well as d['foo'].
    d.foo returns '' for unset values by default, but you can specify
    _attr_default and _item_default constructors (or None to get errors).
    Normally you should specify _default to set them both to the same thing.
    The normal dict.f (get, items, ...) still return the functions.
\end{verbatim}



\section{gzutil}
\begin{python}
with gzutil.GzUnicodeLines(filename, strip_bom=True) as fh:
\end{python}



\section{profile\_jobs}
Returns a float corresponding to the total execution time (in seconds)
of an input list of jobids.
\begin{python}
from accelerator.automata_common import profile_jobs
...
print(profile_jobs(urd.joblist))
\end{python}
