
This chapter is dedicated to the Accelerator's helper functions.
Some, like the \texttt{blob} module, are very useful, while others are
less likely to be included in a project.
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@22

\section{The \texttt{Job} and \texttt{CurrentJob} Classes}
The \texttt{Job} class is used to represent and operate on existing
jobs.  An object of this class is returned from job \texttt{build()}
calls as well as when retreiving jobs from Urd or a \texttt{JobList}
object.  The \texttt{CurrentJob} class is an extension that provides
mechanisms for operations performed while a job is executing, such as
saving files to the job's jobdir.  The classes are derived from
the \texttt{str} class, and objects of these classes decay to
(unicode) strings when pickled.

The following attributes are available on both the \texttt{Job}
and \texttt{CurrentJob} classes:
\starttabletwo
\texttt{method} & The job method.  This can be overriden by \texttt{name=} if job instance is output from Urd or a \texttt{build()} call.\\
\texttt{path} & The filesystem directory where the job is stored.\\
\texttt{workdir} & The workdir name (the part before \texttt{-number} in the jobid).\\
\texttt{number} & The job number as an \texttt{int}.\\
\texttt{filename()} & Return absolute path to a file in a job.\\
\texttt{withfile()} & A \texttt{JobWithFile} with this job.\\
\texttt{params} & Return a dict corresponding to the file \texttt{setup.json} for this job.\\
\texttt{post} & Return a dict corresponding to \texttt{post.json} for this job.\\
\texttt{open()} & Similar to standard \texttt{open}, use to open files\\
\texttt{load()} & Load a pickle file from the job's directory.\\
\texttt{json\_load()} & Load a json file from the job's directory.\\
\texttt{dataset()} & Return a named dataset from the job.\\
\texttt{datasets} & List of datasets in this job.\\
\texttt{output()} & Return what the job printed to \texttt{stdout} and \texttt{stderr}.\\
\stoptabletwo
\noindent In addition, the \texttt{CurrentJob} class has these
attributes too:
\starttable
\texttt{datasetwriter()} && to get a DatasetWriter object.  See documentation for \texttt{Dataset.DatasetWriter()}, section~\ref{}.\\
\texttt{open()}&& With extra temp argument.\\
\texttt{save()} && to store a pickle file\\
\texttt{json\_save()} && to store a json file\\
\stoptable
\noindent Detailed description for the functions follows below:


\subsection{\texttt{Job.filename()}}
Return the absolute (full path) filename to a file stored in the job.
If the file is sliced, a particular slice file can be retrieved using
the \texttt{sliceno} parameter.
\starttable
\texttt{filename} & \textsl{Mandatory} & Name of file in job directory.\\
\texttt{sliceno}  & \pyNone & Set to current slice number if sliced, otherwise \pyNone.\\
\stoptable


\subsection{\texttt{Job.open()}}
This is a wrapper around the standard \texttt{open} function with some
extra features.  Note that
\begin{itemize}
\item[--]  \texttt{Job.open()} can only read files, not write
them, and therefore ``\texttt{r}'' flag must be set.
\item[--]  \texttt{CurrentJob.open()} can both read and write.
\item[--]  \texttt{CurrentJob.open()} must be used as a context manager,
like this
\begin{python}
with job.open(...) as fh:
    ....
\end{python}
\item[--]  \texttt{CurrentJob.open()} can use the \texttt{temp} flag to
modify the persistence of written files.
\end{itemize}
\starttable
\texttt{filename} & \textsl{Mandatory} & Name of file.\\
\texttt{mode} & \texttt{r} & Open file in this mode, see Python's \texttt{open()}\\
\texttt{sliceno} & \pyNone & Read or write sliced files.\\
\texttt{encoding} & \pyNone & @@@@@@@@@@@@@@\\
\texttt{errors} & \pyNone & @@@@@@@@@@@@\\
\texttt{temp} & \pyNone & @@@@@@@@@22\\
\stoptable


\subsection{\texttt{Job.withfile()}}
The \texttt{.withfile()} is used to highlight a specific file in a job
and feed it to another job \texttt{build()}.  The file could be
sliced.
\starttable
\texttt{filename} & \textsl{Mandatory} & Name of file.\\
\texttt{sliced} & \pyFalse & Boolean indicating if the file is sliced or not.\\
\texttt{extra} & \pyNone & Any additional information to the job to be built.\\
\stoptable


\subsection{\texttt{Job.load()}}
Load a file from a job in Python's pickle format.
\starttable
\texttt{filename} & \texttt{result.pickle} & \hspace{2ex}Name of file.\\
\texttt{sliceno} & \pyNone & \\
\texttt{encoding} & \texttt{bytes} & \\
\stoptable


\subsection{\texttt{Job.json\_load()}}
Load a file from a job, in JSON format.
\starttable
\texttt{filename} & \texttt{result.json} & Name of file.\\
\texttt{sliceno} & \pyNone & \\
\texttt{unicode\_as\_utf8bytes} & \texttt{PY2} & \\
\stoptable


\subsection{\texttt{Job.dataset()}}
Get a dataset instance from a job.
\starttable
\texttt{name} & \texttt{default} & \\
\stoptable


\subsection{\texttt{Job.output()}}
Get everything a job has printed to \texttt{stdout}
and \texttt{stderr} in a string variable.
\starttable
\texttt{what} & \pyNone & \\
\stoptable


\subsection{\texttt{Job.save()}}
For \texttt{CurrentJob} instances only.  Save data into the current
job's directory in Python's pickle format.
\starttable
\texttt{obj} & \textsl{Mandatory} & \\
\texttt{filename} & \texttt{result.pickle} & \\
\texttt{sliceno} & \pyNone & \\
\texttt{temp} & \pyNone & \\
\stoptable


\subsection{\texttt{Job.json\_save()}}
For \texttt{CurrentJob} instances only.  Save data into the current
job's directory in JSON format.
\starttable
\texttt{obj} & \textsl{Mandatory} & \\
\texttt{filename} & \texttt{result.json} & \\
\texttt{sliceno} & \pyNone & \\
\texttt{sort\_keys} & \pyTrue & \\
\texttt{temp} & \pyNone & \\
\stoptable





\section{The \texttt{JobWithFile} Class}
\texttt{resolve}
\texttt{load}
\texttt{json\_load}



\section{The \texttt{JobList} Class}

Objects of the \texttt{JobList} class are returned by functions @@@@@@@@@@ \texttt{Urd} class..

\starttabletwo
\texttt{find(<method>)} & a new \texttt{JobList} with only jobs with that method in it.\\
\texttt{get(<method>, default=None)} & latest \texttt{Job} with that method\\
\texttt{[<method>]} & Same as \texttt{.get} but error if no job with that method is in the list.\\
\texttt{as\_tuples} &  The same list but as \texttt{(method, jid)} \texttt{tuple}\\
\texttt{pretty} & Return a prettified string version of the \texttt{JobList}.\\
\texttt{profile} & Total exectime and exectime per method.\\
\texttt{print\_profile(verbose=True)} & Print profiling information to \texttt{stdout}.\\
\stoptabletwo





\section{The \texttt{Dataset} Class}

    Represents a dataset. Is also a string 'jobid/name', or just 'jobid' if
        name is 'default' (for better backwards compatibility).

    You usually don't have to make these yourself, because datasets.foo is
        already a Dataset instance (or None).

    You can pass jobid=''jid/name'' or jobid=''jid'', name=''name'', or skip
        name completely for ``default''.

    You can also pass jobid={jid: dsname} to resolve dsname from the datasets
        passed to jid. This gives None if that option was unset.

    These decay to a (unicode) string when pickled.


\starttable
\texttt{columns} & \\
\texttt{previous} & \\
\texttt{parent} & \\
\texttt{filename} & \\
\texttt{hashlabel} & \\
\texttt{caption} & \\
\texttt{lines} & \\
\texttt{shape} & \\
\texttt{link\_to\_here} & \\
\texttt{merge} & \\
\texttt{column\_filename} & \\
\texttt{chain} & \\
\texttt{iterate\_chain} & \\
\texttt{iterate} & \\
\texttt{iterate\_list} & \\
\texttt{new} & \\
\texttt{append} & \\
\stoptable






\section{The \texttt{DatasetWriter} Class}


\starttable
\texttt{add} & \\
\texttt{set\_slice} & \\
\texttt{column\_filename} & \\
\texttt{enable\_hash\_discard} & \\
\texttt{get\_split\_write} & \\
\texttt{get\_split\_write\_list} & \\
\texttt{get\_split\_write\_dict} & \\
\texttt{close} & \\
\texttt{discard} & \\
\texttt{set\_lines} & \\
\texttt{set\_minmax} & \\
\texttt{finish} & \\
\stoptable


\begin{verbatim}
    Create in prepare, use in analysis. Or do the whole thing in
        synthesis.

    You can pass these through prepare_res, or get them by trying to
        create a new writer in analysis (don't specify any arguments except
            an optional name).

    There are three writing functions with different arguments:

    dw.write_dict({column: value})
        dw.write_list([value, value, ...])
            dw.write(value, value, ...)

    Values are in the same order as you add()ed the columns (which is in
        sorted order if you passed a dict). The dw.write() function names the
            arguments from the columns too.

    If you set hashlabel you can use dw.hashcheck(v) to check if v
        belongs in this slice. You can also call enable_hash_discard
            (in each slice, or after each set_slice), then the writer will
                discard anything that does not belong in this slice.

    If you are not in analysis and you wish to use the functions above
        you need to call dw.set_slice(sliceno) first.

    If you do not, you can instead get one of the splitting writer
        functions, that select which slice to use based on hashlabel, or
            round robin if there is no hashlabel.

    dw.get_split_write_dict()({column: value})
        dw.get_split_write_list()([value, value, ...])
            dw.get_split_write()(value, value, ...)

    These should of course be assigned to a local name for performance.

    It is permitted (but probably useless) to mix different write or
        split functions, but you can only use either write functions or
            split functions.

    You can also use dw.writers[colname] to get a typed_writer and use
        it as you please. The one belonging to the hashlabel will be
            filtering, and returns True if this is the right slice.

    If you need to handle everything yourself, set meta_only=True and
        use dw.column_filename(colname) to find the right files to write to.
            In this case you also need to call dw.set_lines(sliceno, count)
                before finishing. You should also call
                    dw.set_minmax(sliceno, {colname: (min, max)}) if you can.
\end{verbatim}





\section{The \texttt{DatasetChain} Class}


\starttabletwo
\texttt{min} & \\
\texttt{max} & \\
\texttt{lines} & \\
\texttt{column\_counts} & \\
\texttt{column\_count} & \\
\texttt{with\_column} & \\
\stoptabletwo





\section{The \texttt{Urd} Class}

\starttable
\texttt{get} & \\
\texttt{latest} & \\
\texttt{first} & \\
\texttt{peek} & \\
\texttt{peek\_latest} & \\
\texttt{peek\_first} & \\
\texttt{since} & \\
\texttt{list} & \\
\texttt{begin} & \\
\texttt{abort} & \\
\texttt{finish} & \\
\texttt{truncate} & \\
\texttt{set\_workdir} & \\
\texttt{build} & \\
\texttt{build\_chained} & \\
\texttt{warn} & \\
\stoptable




\section{Generate Progress Messages:  the \texttt{status} Module}

The status module is used by the Accelerator to report processing
state.  It is also used by various functions to report iterator and
file access progress.  Status messages are presented in
the \texttt{run} shell by pressing \texttt{C-t}, i.e.\
the \texttt{Ctrl} and \texttt{t} keys simultaneously.

The status module can be used to write progress and status messages
for any function.  Here is an example of how to use the status module
\begin{python}
from accelerator.status import status
...
def analysis(sliceno):
    msg = "reached line %d already!"
    with status(msg % (0,) as update:
        for ix, data in enumerate(datasets.source.iterate(sliceno, 'data')):
            if ix % 1000000 == 0:
                update(msg % (ix,))
\end{python}
In the example above, the status message will be updated once every
million iteration.  By pressing \texttt{C-t} during its execution, the
user will get a message telling how many lines the iterator has
reached.





\section{Share Data Between Jobs:  the \texttt{blob} Module}
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@22222

The simplest way to share reasonable amounts of data between jobs is
by using the \texttt{blob} module.  This module is a
convenience-wrapper around the Python \texttt{pickle} module.

Note that the Accelerator will set the ``current work directory'' to
the current job directory when building a method, so all files created
by a job will be stored in the current job directory, unless the
filename contains a path pointing elsewhere.

\subsection*{Storing/Loading a  Single File}
Data is saved in this way
\begin{python}
from accelerator import blob
def synthesis():
    data = ...  # some data created here
    blob.save(data, filename)
\end{python}
The data is loaded like this
\begin{python}
from accelerator import blob
def synthesis()
    data = blob.load(jobid, filename)
\end{python}
The \texttt{jobid} in \texttt{blob.load()} is not mandatory.  It
defaults to the current workdir unless specified.



\subsection{Storing/Loading a Sliced File}
It is also possible to use the \texttt{blob} module in \analysis.
From a user's perspective it will look like a single file is being
handled, but there is actually one file per slice.  This is how to do
it
\begin{python}
def analysis(sliceno):
    # save data in slices like this
    blob.save(data, filename, sliceno=sliceno)
    # load like this
    data = blob.load(filename, sliceno=sliceno)
\end{python}

Data can be passed ``in parallel'' between different jobs using this
feature.



\subsection{Default Value}

The value of the \texttt{default} parameter is returned if trying to
load a file that does not exist, for example
\begin{python}
x = blob.load('thisfiledoesnotexist', default=dict())
\end{python}
will set \texttt{x} to an empty dict if loading fails.



\subsection{Save Files for Debugging}
\label{sec:debugflag}
The \texttt{temp} argument controls persistence of the stored files.
By default it is being set to \pyFalse, which implies that the stored
file is \textsl{not} temporary.  But setting it to \pyTrue, like in
the following
\begin{python}
    blob.save(data, filename, temp=True)
\end{python}
will cause the stored file to be deleted upon job completion.  The
argument takes two additional values, \texttt{DEBUG} and
\texttt{DEBUGTEMP}, working like this
\vspace{3ex}

\begin{center}
\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}ll}
  \texttt{temp=}  & ``normal'' mode     & debug mode  \\\hline
  \pyFalse           & stored              & \\
  \pyTrue            & stored and removed  & stored and removed\\
  \texttt{DEBUG}     &                     & stored\\
  \texttt{DEBUGTEMP}\hspace{4ex} & stored and removed  & stored\\
\end{tabular*}
\end{center}
Debug mode is active if the Accelerator is started with the
\texttt{--debug} flag.

\noindent Example
\begin{python}
from accelerator.extras import Temp
def analysis(sliceno):
  # save only if --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUG)
  # save always, but remove unless --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUGTEMP)
\end{python}



%% \section{Find the Full Path to a File in Another Job}
%% Accessing a file stored in another job from within a method or build
%% script is simple, and the functionality is implemented
%% in \texttt{resolve\_jobid\_filename()}.  The function takes two
%% arguments, a \textsl{jobid} and a \textsl{filename}.  See the example
%% below
%% \begin{python}
%% from accelerator.extras import resolve_jobid_filename

%% jobids = ('oldjob',)

%% def synthesis():
%%     filename = resolve_jobid_filename(jobids.oldjob, 'nameoffile')
%% \end{python}
%% Note that this function works in a build script as well.



\section{Symlinking}
\label{sec:symlinking}
Creating a symlink, for example from the \texttt{result\_directory} to
current workdir, may be implemented in a simple and safe way like this
\begin{python}
from accelerator.extras import symlink

def synthesis(job):

    # create file and write it to jobid
    ...
    with open(filename, 'wb') as fh:
        fh.write(...)

    # create a symlink to filename in result_directory
    symlink(filename, job.result_directory)
\end{python}
The \texttt{extras.symlink} function will write a soft link
to \texttt{filename} in \texttt{job.result\_directory}, overwriting it if
it already exists.



\section{job\_post}
The \texttt{job\_post} function returns a job's post execution
information as a Python \texttt{dict}
\begin{python}
from accelerator.extras import job_post
postinfo = job_post(jobid)
\end{python}
The post data contains mainly profiling information.



\section{json\_encode}
\begin{python}
from accelerator.extras import json_encode
json_encode(variable, sort_keys=True, as_str=False)
\end{python}

\starttabletwo
\RPtwo \texttt{variable} & variable to be serialised.  \texttt{set}s and
\texttt{tuple}s will be converted to \texttt{list}s.\\

\RPtwo \texttt{sort\_keys} & Sort keys if \pyTrue.\\

\RPtwo \texttt{as\_str} & return a \texttt{str} if \pyTrue, \texttt{bytes}
otherwise.\\
\stoptabletwo



\section{json\_decode}
\begin{python}
from accelerator.extras import json_decode
x = json_decode(s)
\end{python}
Return a datastructure defined by the string \texttt{s}.




\section{json\_save}
\begin{python}
from accelerator.extras import json_save
json_save(variable,
    filename='result',
    jobid=None,
    sliceno=None,
    sort_keys=True,
    _encoder=json_encode,
    temp=False
)
\end{python}




\section{json\_load}
\begin{python}
from accelerator.extras import json_load
x = json_load(
    filename='result',
    jobid='',
    sliceno=None,
    default=None,
    unicode_as_utf8bytes=PY2
)
\end{python}
The \texttt{unicode\_as\_utf8bytes} flag is active in Python2 in order
to let strings decode as bytes.  Override this for unicode.  In
Python3, strings decode as unicode.





\section{DotDict}
\begin{verbatim}
    """Like a dict, but with d.foo as well as d['foo'].
    d.foo returns '' for unset values by default, but you can specify
    _attr_default and _item_default constructors (or None to get errors).
    Normally you should specify _default to set them both to the same thing.
    The normal dict.f (get, items, ...) still return the functions.
\end{verbatim}



\section{gzutil}
\begin{python}
with gzutil.GzUnicodeLines(filename, strip_bom=True) as fh:
\end{python}



\section{profile\_jobs}
Returns a float corresponding to the total execution time (in seconds)
of an input list of jobids.
\begin{python}
from accelerator.automata_common import profile_jobs
...
print(profile_jobs(urd.joblist))
\end{python}
