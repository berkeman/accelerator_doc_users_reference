
This chapter is dedicated to the Accelerator's helper functions.
Some, like the \texttt{blob} module, are very useful, while others are
less likely to be included in a project.

\section{Share Data Between Jobs:  the \texttt{blob} Module}

The simplest way to share reasonable amounts of data between jobs is
by using the \texttt{blob} module.  This module is a
convenience-wrapper around the Python \texttt{pickle} module.

Note that the Accelerator will set the ``current work directory'' to
the current job directory when building a method, so all files created
by a job will be stored in the current job directory, unless the
filename contains a path pointing elsewhere.

\subsection*{Storing/Loading a  Single File}
Data is saved in this way
\begin{python}
import blob
def synthesis():
    data = ...  # some data created here
    blob.save(data, filename)
\end{python}
The data is loaded like this
\begin{python}
import blob
def synthesis()
    data = blob.load(jobid, filename)
\end{python}
The \texttt{jobid} in \texttt{blob.load()} is not mandatory.  It
defaults to the current workdir unless specified.



\subsection{Storing/Loading a Sliced File}
It is also possible to use the \texttt{blob} module in \analysis.
From a user's perspective it will look like a single file is being
handled, but there is actually one file per slice.  This is how to do
it
\begin{python}
def analysis(sliceno):
    # save data in slices like this
    blob.save(data, filename, sliceno=sliceno)
    # load like this
    data = blob.load(filename, sliceno=sliceno)
\end{python}

Data can be passed ``in parallel'' between different jobs using this
feature.



\subsection{Default Value}

The value of the \texttt{default} parameter is returned if trying to
load a file that does not exist, for example
\begin{python}
x = blob.load('thisfiledoesnotexist', default=dict())
\end{python}
will set \texttt{x} to an empty dict if loading fails.



\subsection{Save Files for Debugging}
The \texttt{temp} argument controls persistence of the stored files.
By default it is being set to \pyFalse, which implies that the stored
file is \textsl{not} temporary.  But setting it to \pyTrue, like in
the following
\begin{python}
    blob.save(data, filename, temp=True)
\end{python}
will cause the stored file to be deleted upon job completion.  The
argument takes two additional values, \texttt{DEBUG} and
\texttt{DEBUGTEMP}, working like this
\vspace{3ex}

\begin{center}
\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}ll}
  \texttt{temp=}  & ``normal'' mode     & debug mode  \\\hline
  \pyFalse           & stored              & \\
  \pyTrue            & stored and removed  & stored and removed\\
  \texttt{DEBUG}     &                     & stored\\
  \texttt{DEBUGTEMP}\hspace{4ex} & stored and removed  & stored\\
\end{tabular*}
\end{center}
Debug mode is active if the Accelerator is started with the
\texttt{--debug} flag.

\noindent Example
\begin{python}
from extras import Temp
def analysis(sliceno):
  # save only if --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUG)
  # save always, but remove unless --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUGTEMP)
\end{python}



\section{Find the Full Path to a File in Another Job}
Accessing a file stored in another job from within a method or build
script is simple, and the functionality is implemented
in \texttt{resolve\_jobid\_filename()}.  The function takes two
arguments, a \textsl{jobid} and a \textsl{filename}.  See the example
below
\begin{python}
from extras import resolve_jobid_filename

jobids = ('oldjob',)

def synthesis():
    filename = resolve_jobid_filename(jobids.oldjob, 'nameoffile')
\end{python}
Note that this function works in a build script as well.



\section{Symlinking}
\label{sec:symlinking}
Creating a symlink, for example from the \texttt{result\_directory} to
current workdir, may be implemented in a simple and safe way like this
\begin{python}
from extras import symlink

def synthesis(RESULT_DIRECTORY):

    # create file and write it to jobid
    ...
    with open(filename, 'wb') as fh:
        fh.write(...)

    # create a symlink to filename in RESULT_DIRECTORY
    symlink(filename, RESULT_DIRECTORY)
\end{python}
The \texttt{extras.symlink} function will write a soft link
to \texttt{filename} in \texttt{RESULT\_DIRECTORY}, overwriting it if
it already exists.




\section{job\_params}
\comment{missing!}


\section{job\_post}
The \texttt{job\_post} function returns a job's post execution
information as a Python \texttt{dict}
\begin{python}
from extras import job_post
postinfo = job_post(jobid)
\end{python}
The post data contains mainly profiling information.



\section{json\_encode}
\begin{python}
from extras import json_encode
json_encode(variable, sort_keys=True, as_str=False)
\end{python}

\starttabletwo
\RPtwo \texttt{variable} & variable to be serialised.  \texttt{set}s and
\texttt{tuple}s will be converted to \texttt{list}s.\\[1ex]

\RPtwo \texttt{sort\_keys} & Sort keys if \pyTrue.\\[1ex]

\RPtwo \texttt{as\_str} & return a \texttt{str} if \pyTrue, \texttt{bytes}
otherwise.
\stoptabletwo



\section{json\_decode}
\begin{python}
from extras import json_decode
x = json_decode(s)
\end{python}
Return a datastructure defined by the string \texttt{s}.




\section{json\_save}
\begin{python}
from extras import json_save
json_save(variable,
    filename='result',
    jobid=None,
    sliceno=None,
    sort_keys=True,
    _encoder=json_encode,
    temp=False
)
\end{python}




\section{json\_load}
\begin{python}
from extras import json_load
x = json_load(
    filename='result',
    jobid='',
    sliceno=None,
    default=None,
    unicode_as_utf8bytes=PY2
)
\end{python}






\section{DotDict}
\begin{verbatim}
    """Like a dict, but with d.foo as well as d['foo'].
    d.foo returns '' for unset values by default, but you can specify
    _attr_default and _item_default constructors (or None to get errors).
    Normally you should specify _default to set them both to the same thing.
    The normal dict.f (get, items, ...) still return the functions.
\end{verbatim}




\section{OptionEnum}
\begin{verbatim}
A little like Enum in python34, but string-like.
(For JSONable method option enums.)

>>> foo = OptionEnum('a b c*')
>>> foo.a
'a'
>>> foo.a == 'a'
True
>>> foo.a == foo['a']
True
>>> isinstance(foo.a, OptionEnumValue)
True
>>> isinstance(foo['a'], OptionEnumValue)
True
>>> foo['cde'] == 'cde'
True
>>> foo['abc']
Traceback (most recent call last):
...
KeyError: 'abc'

Pass either foo (for a default of None) or one of the members
as the value in options{}. You get a string back, which compares
equal to the member of the same name.

Set none_ok if you accept None as the value.

If a value ends in * that matches all endings. You can only access
these as foo['cde'] (for use in options{}).
\end{verbatim}



\section{OptionString}
\begin{verbatim}
    """Marker value to specify in options{} for requiring a non-empty string.
    You can use plain OptionString, or you can use OptionString('example'),
    without making 'example' the default.
\end{verbatim}



\section{RequiredOption}
\begin{verbatim}
Specify that this option is mandatory (that the caller must specify a value).
    None is accepted as a specified value if you pass none_ok=True.
\end{verbatim}



\section{OptionDefault}
\begin{verbatim}
"''Default selection for complexly typed options.
foo={'bar': OptionEnum(...)} is a mandatory option.
foo=OptionDefault({'bar': OptionEnum(...)}) isn't.
(Default None unless specified.)
\end{verbatim}



\section{gzutil}
\begin{python}
with gzutil.GzUnicodeLines(filename, strip_bom=True) as fh:
\end{python}



\section{profile\_jobs}
\comment{see redmine}
