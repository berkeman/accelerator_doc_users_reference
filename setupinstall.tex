
\section{Install the Accelerator}

\subsection{Using the \texttt{pip} command}
The easiest way to install the Accelerator is by fetching it from the
PyPi repository
\begin{shell}
pip install accelerator
\end{shell}
Some prefer to install to a virtual environment and do something in
line with the following
\begin{shell}
python3 -m venv accvenv
source accvenv/bin/activate
pip install accelerator
\end{shell}
This will install the Accelerator to the \texttt{accvenv} virtual
environment.  Now, use for example the following command
\begin{shell}
ax --help
\end{shell}
to check that the installation worked.  The next step is to set up a
project.



\section{Set up a New Project}
In order to run, the Accelerator needs to have these things in place
\begin{itemize}
\item[] at least one \textsl{workdir} to store data in,
\item[] most likely a new \textsl{method package} directory to store new code in, and
\item[] a \textsl{configuration file} to set things up.
\end{itemize}
This is all taken care of by the \texttt{init} command.

A typical project setup will look like this
\begin{shell}
myproject/
    accelerator.conf
    dev/
        methods.conf
        a_method.py
        build.py
\end{shell}
where methods are stored in the \texttt{dev} directory.



\section{Run the Tests}
A rather extensive test suite is included in the Accerator
installation.  To run this, enable the test package in the
configuration file:
\begin{verbatim}
method packages:
    ...
    accelerator.test_methods
\end{verbatim}
start the daemon using
\begin{python}
ax daemon
\end{python}
and in another shell start the test
\begin{python}
ax run tests
\end{python}
Since the tests include testing of different character encodings, you may end up with a
\begin{verbatim}
Exception: Failed to enable numeric_comma, please install at least one
of the following locales: da_DK nb_NO nn_NO sv_SE fi_FI en_ZA es_ES
es_MX fr_FR ru_RU de_DE nl_NL it_IT
\end{verbatim}
On a Debian-based machine, locales can be configured using
\begin{shell}
dpkg-reconfigure locales 
\end{shell}
which has to be run with root privileges.



\section{Daemon Configuration File}
\label{sec:configfile}
The configuration file specifies which method packages and workdirs
that are available for a project.  A template configuration file can
be generated using the \texttt{init} command as described on
page~\ref{init_command}.  Below is an example of a configuration file.
%\begin{leftbar}
\begin{snugshade}
\begin{verbatim}
# The configuration is a collection of key value pairs.
#
# Values are specified as
# key: value
# or for several values
# key:
#       value 1
#       value 2
#       ...
# (any leading whitespace is ok)
#
# Use ${VAR} or ${VAR=DEFAULT} to use environment variables.

slices: 23
workdirs:
        test /zbd/workdirs/test
        import ${HOME}/workdirs/import

# Target workdir defaults to the first workdir, but you can override it.
# target workdir: dev
# (this is where jobs without a workdir override are built)

method packages:
        dev
        accelerator.standard_methods
#       accelerator.test_methods   

urd: http://localhost:9000

result directory: ${HOME}/accelerator/results
source directory: /zbd/data/backblaze
logfile: ${HOME}/accelerator/daemon.log

# If you want to run methods on different python interpreters you can
# specify names for other interpreters here, and put that name after
# the method in methods.conf.
# You automatically get four names for the interpreter that started
# the daemon: DEFAULT, 3, 3.7 and 3.7.3 (adjusted to the actual
# version used). You can override these here, except DEFAULT.
# interpreters:
#       2.7 /path/to/python2.7
#       test /path/to/beta/python  
\end{verbatim}
\end{snugshade}%
%\end{leftbar}%

The configuration file above specifies 23 slices and two workdirs,
called \texttt{test} and \texttt{import}.  The \texttt{test} workdir
is specified using an absolute path, while the \texttt{import} workdir
is specified relative to the user's home directory using the shell
environment variable \texttt{\$HOME}.

The workdir that is specified first is the \textsl{target workdir},
where jobs are written to by default.  All other specified workdirs
will by default only be used for reading.  Any of the workdirs
specified could be written to, though, using the
\texttt{set\_workdir=} option to the \texttt{build} command, as
described on page~\ref{set_workdir}.

Methods packages available for use are the \texttt{standard\_methods} bundled
with the Accelerator, and methods defined in the
directory \texttt{dev} (if defined in \texttt{dev/methods.conf}).

\starttabletwo

\RPtwo \texttt{slices} & Number of slices used for the project.\\[1ex]

\RPtwo \texttt{workdirs} & A list of paths to workdir directories.  At
least one workdir needs to be defined.  All workdirs that are used
together must have the same number of slices.  It is possible to use
shell environment variables such as \texttt{\$\{HOME\}} when
specifying workdirs.

Unless overridden by the \texttt{target workdir}, the first workdir in
the list will be the default \textsl{target workdir} that is used for
all writing.  Other specified workdirs will only be read from, unless
overrided by the \texttt{build} call as described on
page~\ref{set_workdir}.\\[1ex]

\RPtwo \texttt{target workdir} & Name of the \textsl{target workdir}.
If specified this overrides the first item in the \texttt{workdirs}
list.\\[1ex]

\RPtwo \texttt{method packages} & A list of directories containing
methods.  These will be the only directories where the Accelerator can
``see'' methods.  \texttt{standard\_methods} is bundled with the
Accelerator and is commonly used.\\[1ex]

\RPtwo \texttt{urd} & If present, an URL to the Urd server.\\[1ex]

\RPtwo \texttt{result directory} & A common path that is available to
all jobs.  It has been used sparsely by the Accelerator team since it
voids the possibility to see where a file comes from.  Nevertheless,
in some projects it is valuable to have a common place where methods
store results, see section~\ref{sec:RESULT_DIR}.  A way to keep
traceability is to store the file in the job directory as usual, and
then create a soft link to it in \texttt{result\_directory}.\\[1ex]

\RPtwo \texttt{source directory} & Default root path for
\texttt{csvimport}.  This is to avoid rebuilds of imports if source
files are moved to another directory.  (This typically happens when
setting up a similar system on another physical machine.)  See
section~\ref{sec:SOURCE_DIR} on how to get access to
\texttt{source\_directory} from any method.\\[1ex]

\RPtwo \texttt{logfile} & Location of the Accelerator's log.\\[1ex]

\RPtwo \texttt{interpreters} & Name and path to python executables.
These are used in \texttt{methods.conf} to specify specific Python
versions (or virtual environments) for individual methods.  If
unspecified, methods will be executed using the same binary that runs
the Accelerator's daemon process.\\[1ex]

\stoptabletwo

It is possible to assign values in the configuration file using shell
environment variables.  In the example above, workdirs are specified
relative to \texttt{\$\{HOME\}}, for example.  In general, the
assignment is \texttt{\$\{VAR=DEFAULT\}}.



\section{Setting up Urd}
\label{sec:urd_setup}

To run Urd two things are needed
\begin{itemize}
\item[] a directory where it can put its database, and
\item[] a \texttt{passwd} file to store user-password pairs in.
\end{itemize}
The \texttt{passwd} file is stored in the urd database directory.  The
default name of the database directory is \texttt{urd.db}, so
\begin{shell}
mkdir urd.db
cd urd.db
<editor> passwd
\end{shell}
where \texttt{<editor>} should be replaced by the editor of choice.


\subsubsection{Starting Urd}
Urd is running as a daemon.  It is started like this
\begin{shell}
ax urd --port=<port> --path=<path>
\end{shell}
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


\subsection{The Urd Database}
The Urd database has the following structure
\begin{verbatim}
database_root/
    passwd
    database/
        user1/
            list1
            list2
        user2/
            list3
\end{verbatim}



\subsubsection{The \texttt{passwd} file}
The \texttt{passwd} file stores write access authentication.  The file
format is straightforward, each line is a user--password pair as follows
\begin{verbatim}
user:password
\end{verbatim}
For example, if the file contains the following line
\begin{verbatim}
ab:secret
\end{verbatim}



\subsection{}
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
A build script issued like this
\begin{shell}
URD_AUTH=ab:secret ax run test
\end{shell}
will have write access to all lists belonging to the user \texttt{ab},
such as for example the \texttt{ab/test} and \texttt{ab/import} lists.
But it can not write to lists belonging to other users, such
as \texttt{cd/import}.  It can read all lists, though.



\section{Workdirs}
Jobs are stored in \textsl{workdirs}.  Workdirs are defined in the
Accelerator's configuration file, where at least one workdir must be
specified.

By default, the only workdir that is written to is the target workdir,
while all other defined workdirs are for reading.  It is possible to
override this, however, by setting the \mintinline{python}|workdir=|
option in the \texttt{urd.build()} call, see
section~\ref{sec:urd_build}.

Jobdirs are stored in the workdir by the daemon, and jobdirs will
inherit the workdir name and add a suffix that is an incremental job
counter.  Here is an example of a workdir named \texttt{test}, that
contains three jobdirs.
\begin{shell}
test/
    .slices.conf
    test-0/
    test-1/
    test-2/
    test-LATEST -> test-2
\end{shell}
The \texttt{.slices.conf} file contains the number of slices used for
the workdir.  The link \texttt{<workdir>-LATEST} is always pointing to
the last jobdir created.  This is useful for example when iteratively
testing a method and accessing its data for example for plotting
purposes.  Each new build of the (modified) method will create a new
job, and the link will always point to the most recent
version.


\subsection{Creating a Workdir}
If a workdir defined in the configuration file does not exist on disk
at the stated location, the Daemon will exit and print an error
stating that a directory is missing.  The first time the Daemon
encounters a new directory it will initialise it in accordance with
the configuration file.  So, new workdirs are created by adding them
to the configuration file \textsl{and} creating the corresponding
directories.  The Accelerator will then initiate these directories on
the next startup.

The initiation process creates a file named \texttt{.slices.conf} that
indicates that the directory is now a workdir.  This file contains the
number of slices that is used for the workdir.



\section{Status (Progress) Reporting}
During job building, it is possible to press \texttt{C-t},
i.e.\ \texttt{Ctrl} + \texttt{t} simultaneously, in the \texttt{run}
shell to get status information.  The built in status will report the
processing state, if it is in \prepare, \analysis, or \synthesis.
Iterators report which dataset (perhaps in a chain) that is currently
being iterated, and the \texttt{blob} functions report status of file
pickling.

The \texttt{status} module makes it possible to insert
status reporting into any method.  For more information about status
reporting, see section~\ref{sec:statusreporting}.



\section{Generate Progress Messages:  the \texttt{status} Module}
\label{sec:statusreporting}
The status module is used by the Accelerator to report processing
state.  It is also used by various functions to report iterator and
file access progress.  Status messages are presented in
the \texttt{run} shell by pressing \texttt{C-t}, i.e.\
the \texttt{Ctrl} and \texttt{t} keys simultaneously.

The status module can be used to write progress and status messages
for any function.  Here is an example of how to use the status module
\begin{python}
from accelerator.status import status
...
def analysis(sliceno):
    msg = "reached line %d already!"
    with status(msg % (0,) as update:
        for ix, data in enumerate(datasets.source.iterate(sliceno, 'data')):
            if ix % 1000000 == 0:
                update(msg % (ix,))
\end{python}
In the example above, the status message will be updated once every
million iteration.  By pressing \texttt{C-t} during its execution, the
user will get a message telling how many lines the iterator has
reached.



\section{Working with Relative Paths}
In some situations, like importing data from files, it is convenient
to store the absolute path of the files as a configuration parameter
and then work only with relative paths in the source code.  This has
two advantages.
\begin{itemize}
\item[] First, it makes it possible to move source files around without
forcing a re-build of the import jobs, and
\item[] second, absolute paths will not be stored in the source code.
\end{itemize}
In order to make use of relative paths, store the ``system dependent''
left part of the path in the Accelerator's configuration file.  There
are two variables in the configuration file that can be used for this,
and they have different purposes.  The \texttt{source\_directory}
variable is intended for reading source files, and
the \texttt{result\_directory} is intended for writing output.  See
the following subsections for details.


\subsection{The \texttt{source\_directory}}
\label{sec:SOURCE_DIR}
The \texttt{job.source\_directory} variable is used by the
\texttt{csvimport} method, but could be used by any method reading
input files, like in this example
\begin{python}
import os
options = dict(filename=Optionstring)

def synthesis(job)
    fname = os.path.join(job.source_directory, options.filename)
\end{python}
here, the \texttt{fname} is a concatenation of the
\texttt{job.source\_directory} specified in the Accelerator's
configuration file (see section ~\ref{sec:symlinking}) and the input
option \texttt{filename}.


\subsection{The \texttt{result\_directory}}
\label{sec:RESULT_DIR}
It is possible to define a shared directory
named \texttt{result\_directory} in the Accelerator's configuration
file.  In a method, this variable may be accessed like in this example
\begin{python}
def synthesis(job):
    print(job.result_directory)
\end{python}
Methods could use this for storing for example plots and reports for a
project in one easy accessible common location.  Note however, that
tracking these files is not possible, there is no information linking
back from the result directory to a specific job.  This may be
overcome using for example soft file links, however, see
section~\ref{sec:symlinking}.



\section{Symlinking}
\label{sec:symlinking}
Creating a symlink, for example from the \texttt{result\_directory} to
current workdir, may be implemented in a simple and safe way like this
\begin{python}
from accelerator.extras import symlink @@@@@@@@@@@@@@@@@@@@@@@@

def synthesis(job):

    # create file and write it to the job directory
    ...
    with job.open(filename, 'wb') as fh:
        fh.write(...)

    # create a symlink to filename in result_directory
    symlink(filename, job.result_directory)
\end{python}
The \texttt{extras.symlink} function will write a soft link
to \texttt{filename} in \texttt{job.result\_directory}, overwriting it if
it already exists.
