%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


This chapter covers most aspects of jobs and methods.  It describes
what methods are and how they are stored.  When they are built and
when not.  What input parameters look like in full detail.  How to
access another job's parameters.  Parallel processing.  Return values
and result merging.  Storing and retreiving data.  Building subjobs.



\section{Definitions}

\subsection{Methods and jobs}

In general, doing a computation on a computer follows the following equation
\[
  \text{source code} \quad+\quad \text{input data and parameters} \quad+\quad \text{execution time} \quad\rightarrow\quad \text{result}
\]
In the Accelerator context, the equation becomes
\[
  \text{method} \quad+\quad \text{input data} \quad+\quad \text{input parameters} \quad+\quad \text{execution time} \quad\rightarrow\quad \text{job}
\]
where the \textbf{method} is the source code, and the \textbf{job} is
a directory containing
\begin{itemize}
\item[--] any number of output files created by the running method, as well as
\item[--] a number of job meta information files.
\end{itemize}
The exact contents of the job directory will be discussed in section~\ref{sec:job_directories}.

Computing a job is denoted job \textsl{building}.
Jobs are \textbf{built} from methods.  When a job has been built, it
is \textsl{static}, and cannot be altered or removed by the
Accelerator.  Jobs are built either by
\begin{itemize}
\item[--] a \textsl{build script}, see chapter~\ref{chap:urd}, or
\item[--] by a method, using \textsl{subjobs}.
\end{itemize}
subjobs will be the topic of section~\ref{sec:subjobs}.  The following figure
illustrates how a job \texttt{example-0} is built from the
method \texttt{a\_method.py}
\begin{figure}[h!]
  \begin{center}
    \input{figures/method.pdftex_t}
    \label{fig:method}
  \end{center}
\end{figure}

\noindent The name is always unique so that it
can be used as a reference to the job, and the reasons for the actual
name (in this case \texttt{example-0}) will be apparent later in this
chapter.



\subsection{Jobids}
A \textbf{jobid} is a reference to a job.  Since all job directories
have unique names, the name of the job is used as the jobid.  In the
example above, the job is uniquely identified by the
string \texttt{example-0}.



\section{Python Packages}
Methods are stored in standard Python packages, i.e.\ in a directory that is
\begin{itemize}
\item[--] reachable by the Python interpreter, and
\item[--] contains the (empty) file \texttt{\_\_init\_\_.py}.
\end{itemize}
In addition, for the Accelerator to accept a package, the following
constraints need to be satisfied
\begin{itemize}
\item[--] the package must contain a file named \texttt{methods.conf}, and
\item[--] the package must be added in the Accelerator's configuration file.
\end{itemize}
A package is reachable by Python if it is included in
the \texttt{PYTHONPATH} variable.  By default, the directory where the
Accelerator daemon is started is included in this path.  The next
section guides through the steps required to create a new package.



\subsection{Creating a new Package}
The following shell commands illustrate how to create a new package
directory
\begin{shell}
% mkdir <dirname>
% touch <dirname>/__init__.py
% touch <dirname>/methods.conf
\end{shell}
The first two lines create a Python package, and the third line adds
the file \texttt{methods.conf}, which is required by the Accelerator.

For security reasons, the Accelerator does not look for packages by
itself.  Instead, packages to be used need to be explicitly specified
in the Accelerator's configuration file using the
\mintinline{shell}{method_directories=}
assignment.  Basically, the assignment should be a comma separated
list of packages, see chapter~\ref{sec:configfile} for detailed information about
the configuration file.


\section{Method Source Files}
Method source files are stored in Python packages as described in the
previous section.  The Accelerator searches all packages for methods
to execute, and therefore \textsl{method names need to be globally
unique}!  In order to reduce risk of executing the wrong file, there
are three limitations that apply to methods:
\begin{enumerate}
\item For a method file to be accepted by the Accelerator, the
  filename has to start with the prefix ``\texttt{a\_}'';
\item the method name, without this prefix must be present on a
  separate line in the \texttt{methods.conf} file for the package, see
  section~\ref{sec:methods_conf}; and
\item the method name must be \emph{globally} unique, i.e.\ there can
  not be a method with the same name in any other method directory
  visible to the Accelerator.
\end{enumerate}


\subsection{Creating a New Method}
In order to create a new method, follow these steps
\begin{enumerate}
\item Create the method in a package viewable to the Accelerator
    using an editor.  Make sure the filename is \texttt{a\_<name>.py} if
    the method's name is \texttt{<name>}.
\item Add the method name \texttt{<name>} (without the prefix ``\texttt{a\_}'' and
    suffix ``\texttt{.py}'') to the \texttt{methods.conf} file in the
    same method directory.  See section~\ref{sec:methods_conf}.
  \item (Make sure that the method directory is in the Accelerator's
    configuration file.)
\end{enumerate}


\subsection{Methods.conf}
\label{sec:methods_conf}
For a package to be useful to the Accelerator, it requires
a \texttt{methods.conf} file.  This file specifies which methods in
the directory that are available for building.  Files not specified
in \texttt{methods.conf} cannot be executed.  The
file \texttt{methods.conf} provides an easy way to specify and limit
which source files that can be executed, which is something that makes
a lot of sense in any production environment.

The \texttt{methods.conf} is a plain text file with one entry per
line.  Any characters from a hash sign (``\texttt{\#}'') to the end of
the line is considered to be a comment.  It is allowed to have any
number of empty lines in the file.  Available methods are entered on a
line by first stating the name of the method, without the \texttt{a\_}
prefix and \texttt{.py} suffix, followed by one or more whitespaces
and a token ``\texttt{py2}'' or ``\texttt{py3}'', indicating which
Python version that should be used when executing the method.  Here is an example
\begin{shell}
# this is a comment

test2           py2

test3           py3  # newer
#bogusmethod    py3
\end{shell}
This file declares two methods corresponding to the
filenames \texttt{a\_test2.py} and \texttt{a\_test3.py}, written in
Python2 and Python3 respectively.  Another method \texttt{bogusmethod}
is commented out and trying to build this will issue an error.





\section{Job Already Built Check}

From chapter~\ref{chap:urd} it is known that a method is built using
the \texttt{build()} function in a build script, like this
\begin{python}
def main(urd):
    urd.build('themethod', options=..., ...)
\end{python}

Prior to building a method, the Accelerator checks if an equivalent
job has been built in the past.  If it has, it will not be executed
again.  This check is based on two things:
\begin{enumerate}
\item  the output of a hash function applied to the method source code, and
\item  the method's input parameters.
\end{enumerate}
The hash value is combined with the input arguments and compared to
all jobs already built.  Only if the hash and input parameter
combination is unique will the method be executed.



\section{Depend on More Files:  \texttt{depend\_extra}}

A method may import code located in other files, and these other files
can be included in the hash calculation as well.  This will ensure
that a change to an imported file will indeed force a re-execution of
the method if a build is requested.  Additional files are specified in
the method using the \texttt{depend\_extra} list, as for example:
\begin{python}
from . import my_python_module

depend_extra = (my_python_module, 'mystuff.data',)
\end{python}
As seen in the example, it is possible to specify either Python module
objects or filenames relative to the method's location.



\section{Avoiding Rebuild: \texttt{equivalent\_hashes}}
\label{sec:equivalent_hashes}

A change to a method's source code will cause a new job to be built
upon running \mintinline{python}{build()}, but sometimes it is
desireable to modify the source code and \textsl{not} causing a
re-build.  This happens, for example, when new functionality is added
to an existing method.  The way it worked before the code change is
the same, so existing jobs do not need to be re-built.  For this
situation, there is an \texttt{equivalent\_hashes} dictionary that can
be used to specify which versions of the source code that are
equivalent.  The Accelerator helps creating this, if needed.  This is
how it works.
\begin{enumerate}
\item Find the hash \texttt{<old\_hash>} of the existing job in that job's \texttt{setup.json}.
\item Add the following line to the method's source code
\begin{python}
equivalent_hashes = {'whatever': (<old_hash>,)}
\end{python}
\item Run the build script.  The daemon will print something like
\begin{shell}
===========================================================
WARNING: test_methods.a_test_rechain has equivalent_hashes,
but missing verifier <current_hash>
===========================================================
\end{shell}
\item Enter the \texttt{current\_hash} into the \texttt{equivalent\_hashes}:
\begin{python}
equivalent_hashes = {<current_hash>: (<old_hash>,)}
\end{python}
\end{enumerate}
This line now tells that \texttt{current\_hash} is equivalent
to \texttt{old\_hash}, so if a job with the old hash exists, the
method will not be built again.  Note that the right part of the
assignment is actually a list, so there could be any number of
equivalent versions of the source code.  Some Accelerator standard
methods like \texttt{csvimport} are good examples of this.





\section{Method Execution}

Methods are not executed from top to bottom.  Instead, there are three
functions that are called by the method dispatcher that controls the
execution flow.  These functions are
\begin{itemize}
\item [] \texttt{prepare()},
\item [] \texttt{analysis()}, and
\item [] \texttt{synthesis()}.
\end{itemize}



\subsection{Execution Flow}

The three functions \prepare, \analysis, and \synthesis are called one
at a time in that order.  \prepare and \synthesis executes as single
processes, while \analysis provides parallel execution.  None of them
is mandatory, but at least one must be present for the method to
execute.

\subsection{Function Arguments}
There are a few constants that can be passed into the executing
functions at run time.  Here is a complete list
\starttabletwo
\RPtwo \texttt{params} & A dictionary containing all parameters input
       from both the calling function and the Accelerator.  This will be
       explained in detail in section~\ref{sec:params}.\\[2mm]
\RPtwo \texttt{sliceno} & (For \analysis only.)  A unique identifier for each parallell \analysis process.\\[2mm]
\RPtwo \texttt{SOURCE\_DIRECTORY} & A path defined in the Accelerator's configuration file, see~\ref{sec:SOURCE_DIR}.\\[2mm]
\RPtwo \texttt{RESULT\_DIRECTORY} & A path defined in the Accelerator's configuration file, see~\ref{sec:RESULT_DIR}.\\
%\RPtwo \texttt{jobids}, \texttt{options}, \texttt{datasets} & Defined globally in the method, but may be 
\stoptabletwo

\noindent The input parameters \options, \jobids, and \datasets are
global, so they do not need to be explicit in a function call.  The
\analysis function is special and takes a required argument
\texttt{sliceno}, which is an integer between zero and the total
number of slices minus one.  This is the unique identifier for each
\analysis process, and is described in the next section.


\subsection{Parallel Processing:  The \analysis function, Slices, and Datasets}

The number of analysis processes is always equal to the number of
dataset slices that the Accelerator has in its configuration file.
The idea is that each slice in a dataset should have exactly one
corresponding \analysis process, so that all slices in a dataset can
be processed in parallel.  The input parameter \texttt{sliceno} to
the \analysis function is in the range from zero to the number of
slices minus one.  The number of slices is defined in the
Accelerator's configuration file.



\subsection{Return Values}
Return values may be passed from one function to another.  What is
returned from prepare is called \prepareres, and may be used as input
argument to \analysis and \synthesis.  The return values from
\analysis is available as \analysisres in \synthesis.  The
\analysisres variable is an iterator, yielding the results from each
slice in turn.  Finally, the return value from \synthesis is stored
permanently in the job directory.  Here is an example of return value
passing
\begin{python}
# return a set of all users in the source dataset
options = dict(length=4)
datasets = (source',)

def prepare(options):
    return options.length * 2

def analysis(sliceno, prepare_res):
    return set(u for u in datasets.source.iterate(sliceno, 'user'))

def synthesis(analysis_res, prepare_res):
     return analyses_res.merge_auto()
\end{python}
In the current implementation, all return values are stored as Python
\texttt{pickle} files.

Note that when a job completes, it is not possible to retrieve the
results from \prepare or \analysis anymore.  Only results from
\synthesis are kept.

\subsection{Merging Results from \analysis}
In the example above, each analysis process returns a \texttt{set}
that is constructed from a single slice.  In order to create a set of
all users in the \texttt{source} dataset, all these sets have to be
merged.  Merging could be done using a \texttt{for}-loop, but merging
is dependent of the actual type, and writing merging functions is
error prone.  Therefore, \analysisres has a function called
\texttt{merge\_auto()}, that is used for merging.  This function can
merge most data types, and even merge container variables in a
recursive fashion.  For example,
\begin{python}
h = defaultdict(lambda: defaultdict(set))
\end{python}
is straightforward to merge using \texttt{merge\_auto}.



\section{Method Input Parameters}
\label{sec:input_params}

There are three kinds of method input parameters assign by
the \texttt{build} call: \jobids, \datasets, and \options.  These
parameters are stated early in the method source code, such as for
example
\begin{python}
jobids = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are populated by the
builder, see~\ref{chap:urd}.

The \texttt{jobids} parameter list is used to input links, or jobids,
to other jobs, while the \datasets parameter list is used to input
links to datasets.  These parameters must be populated by the build
call.

The \options dictionary, on the other hand, is used to input any other
type of parameters to be used by the method at run time.  Options must
not be populated by the build call, and this can be used for ``global
constants'' in the method.  An option assigned by the build call will
override this, however.

Note that \jobids and \datasets are \texttt{tuple}s (or \texttt{list}s
or \texttt{set}s); and a single entry has to be followed by a comma as
in the example above; while \options is a dictionary.  Individual
elements of the input parameters may be accessed with dot notation
like this
\begin{python}
jobids.accumulated_cost
datasets.transaction_log
options.length
\end{python}
Each of these parameters will be described in more detail in following
sections.




\subsection*{Input Jobids}
The \jobids parameter is a tuple of jobids linking this job to other
jobs.  Inside the running method, each item in the \jobids tuple is of
type \texttt{str} and may be used as a reference to the corresponding
job.  All items in the \jobids tuple must be assigned by the builder to
avoid run time errors.
It is also possible to specify lists of jobids, see this example
\begin{python}
jobids = ('source', ['alistofjobs'],)
\end{python}
where \texttt{source} is a single \texttt{jobid}, whereas
\texttt{alistofjobs} is a list of \texttt{jobid}s.



\subsection*{Input Datasets}
The \datasets parameter is a tuple of links to Datasets.  In the
running method, each item in the \datasets variable is a tuple of
objects from the \texttt{Dataset} class that works like
a \texttt{list} with some add-on functionality.  The \texttt{Dataset}
class is described in a dedicated chapter~\ref{chap:datasets}.
All items in the \datasets tuple must be assigned by the builder to
avoid run time errors.
It is also possible to specify lists of jobids, see this example
\begin{python}
datasets = ('source', ['alistofdatasets'],)
\end{python}
where \texttt{source} is a single \texttt{dataset}, whereas
\texttt{alistofdatasets} is a list of \texttt{dataset}s.




\subsection*{Input Options}

The \options parameter is of type \texttt{dict} and used to pass
various information from the builder to a job.  This information could
be integers, strings, enumerations, sets, lists, and dictionaries in a
recursive fashion, with or without default values.  Assigning options
from the build call is not necessary, but an assignment will override
the ``default'' that is specified in the method.  Options are
specified like this
\begin{python}
  options = dict(key=value, ... )  # or
  options = {key: value, ...}
\end{python}

Options are straightforward to use, but actually quite advanced.  A
formal overview is presented in section~\ref{sec:formal_options}.





\section{Reading all Method Parameters:  \texttt{params}}
\label{sec:params}

There are two sources of parameters to a running method,
\begin{itemize}
\item [] parameters from the caller, i.e.\ the \texttt{build()}-call, and
\item [] parameters assigned by the Accelerator when the job starts building.
\end{itemize}
All parameters are available in the \texttt{params} dictionary.  Build
parameters will be described thoroughly in
section~\ref{sec:input_params}, and parameters from the Accelerator
will be brought up at the end of this section.

Consider the following example method that pretty-prints
the \texttt{params} variable.  Note that the method explicitly expects
input parameters to be assigned by the caller in the \jobids
and \datasets constants, while \options keeps the default value unless
assigned in the build call.

\begin{python}
import json

jobids = ('jid',)
datasets = ('source', 'parent',)
options = dict(mode='testing', length=3)

def synthesis(params):
    print(json.dumps(params, indent=4))
\end{python}
The corresponding output may look something like this
%\begin{leftbar}
\begin{json}
{
    "package": "dev",
    "method": "my_example_method",
    "jobid": "EXAMPLE-12",
    "starttime": 1520652213.4547446,
    "slices": 16,
    "options": {
        "mode": "testing",
        "length": 3
    },
    "datasets": {
        "source": "EXAMPLE-3",
        "parent": "EXAMPLE-2"
    },
    "caption": "fsm_my_example_method",
    "seed": 53231916470152325,
    "jobids": {
        "jid": "EXAMPLE-0"
    },
    "hash": "42af401251840b3798e9e78da5b5c5b4ef525ecc"
}
\end{json}
%\end{leftbar}
\noindent and a description of its keys
\starttabletwo
\RPtwo   \texttt{package} & Python package for this method\\
\RPtwo   \texttt{method} & name of this method\\
\RPtwo   \texttt{jobid} & jobid of this job\\[1ex]

\RPtwo   \texttt{starttime}& start time in epoch format\\
\RPtwo   \texttt{caption} & a caption\\
\RPtwo   \texttt{slices} & number of slices of current Accelerator configuration \\
\RPtwo   \texttt{seed} & a random seed available for use$^1$\\
\RPtwo   \texttt{hash} & source code hash value\\[1ex]

\RPtwo   \texttt{options} & input parameter\\
\RPtwo   \texttt{dataset} & input parameter\\
\RPtwo   \texttt{jobids} &  input parameter\\[1ex] \hline\\[-2mm]

\multicolumn{2}{l}{\small{$^1$ The Accelerator team recommends \emph{not} using
  \texttt{seed}, unless non-determinism is actually a goal.}}
\stoptabletwo



\section{Accessing Another Job's Parameters:  \texttt{job\_params}}

\index{\texttt{job\_params}|textbf}
The previous sections show that the the \params data structure
contains all input parameters and initialization data for a job.
Sometimes it is useful to access another job's \params.  There is a
special function for that, called \texttt{job\_params}, and it is used
like this
\begin{python}
from extras import job_params

jobids = ('anotherjob',)

def synthesis():
    print(jobids.anotherjob)
    # will print something like 'jid-0_0'
    if jobids.anotherjob is not None:
        print(job_params(jobids.anotherjob).options)
        # will print the options of anotherjob
\end{python}
Note that if \texttt{jobids.anotherjob} is not defined, i.e.\ \pyNone,
\texttt{job\_params} will return the parameters of the
\textsl{current} job.  So in order to be safe, it makes sense to first
check that the job exists.



\section{Accessing Another job's Datasets}
Accessing another job's dataset parameters is done using the
\texttt{Dataset} constructor like this
\begin{python}
from dataset import Dataset

jobids = ('previous',)

def synthesis():
    # use the "default" dataset if unspecified
    ds_default = Dataset(jobids.previous)
    # specify the "source" dataset
    ds_source = Dataset(jobids.previous, 'source')
\end{python}
assuming that \texttt{jobids.previous} has the datasets
\texttt{default} and \texttt{source}.




\section{Job Directories}
\label{sec:job_directories}

A successfully build of a method results in a new job directory on
disk.  The job directory will be stored in the current workdir and
have a structure as follows, assuming the current workdir
is \texttt{test}, and the current jobid is \texttt{test-0}.
\begin{verbatim}
workdirs/test/
    test-0/
        setup.json
        method.tar.gz
        result.pickle
        post.json
\end{verbatim}
The \texttt{setup.json} will contain information for the job,
including name of method, input parameters, and, after execution, some
profiling information.  \texttt{post.json} contains profiling
information, and is written only of the job builds successfully.  All
source files, i.e.\ the method's source and \texttt{depend\_extra}s
are stored in the \texttt{tar}-archive \texttt{method.tar.gz}.
Finally, the return value from \texttt{synthesis} is stored as a
Python pickle file with the name \texttt{result.json}.

If the job contains datasets, these will be stored in directories,
such as for example \texttt{default/}, in the root of the job
directory.



\section{Subjobs}
\label{sec:subjobs}
\index{subjobs}

Jobs may launch subjobs, i.e.\ methods may build other methods in a
recursive manner.  As always, if the jobs have been built already,
they will immediately be linked in.  The syntax for building a job
inside a method is as follows, assuming we build the jobs in \prepare
\begin{python}
import subjobs

def prepare():
    subjobs.build('count_items', options=dict(length=3))
\end{python}
It is possible to build subjobs in \prepare and \synthesis, but not in
\analysis.    The \texttt{subjobs.build} call uses the
same syntax as \texttt{urd.build} described in chapter~\ref{chap:urd}, so
the input parameters \options, \datasets, \jobids, and
\texttt{caption} are available here too.  Similarly, the return value
from a subjob build is a jobid to the built job.

There are two catches, tough.
\begin{enumerate}
  \item
If there are datasets built in a subjob,
these will not be explicitly available to Urd.  A workaround is to
link the dataset to the building method like this
\index{\texttt{link\_to\_here}}
\begin{python}
from dataset import Dataset

def synthesis():
    jid = subjobs.build('create_dataset')
    Dataset(jid).link_to_here(name='thename')
\end{python}
with the effect that the building job will act like a Dataset, even
though the dataset is actually created in the subjob.  The
\texttt{name} argument is optional, the name \texttt{default} is used
if left empty, corresponding to the default dataset.  It is possible
to override the dataset's previous using
the \texttt{override\_previous} option, which takes a jobid
(or \pyNone) to be the new \texttt{previous}.
\index{\texttt{override\_previous}}
\begin{python}
Dataset(jid).link_to_here(name='thename', override_previous=xxx)
\end{python}


\item
Currently there is no dependency checking on subjobs, so if a subjob
method is changed, the calling method will not be updated.  The
current remedy is to use \texttt{depend\_extra} in the building
method, like this
\begin{python}
import subjobs

depend_extra = ('a_childjob.py',)

def prepare():
    subjobs.build('childjob')
\end{python}
\end{enumerate}
There is a limit to the recursion depth of subjobs, to avoid creating
unlimited number of jobs by accident.




\section{Formal Option Rules}
\label{sec:formal_options}
This section covers the formal rules for the \options parameter.
\begin{enumerate}

\item Typing may be specified using the class name (i.e.\ \texttt{int}), or as
  a value that will construct into such a class object (i.e.\ the
  number 3).  See this example
  \begin{python}
options = dict(
    a = 3,     # typed to int
    b = int,   #          int
    c = 3.14,  #          float
    d = '',    #          str
)
  \end{python}
  Values will be default values, and this is described thoroughly in
  the other rules.

 \item An input option value is required to be of the correct type.
   This is, if a type is specified for an option, this must be
   respected by the builder.  Regardless of type,
   \mintinline{python}/None/ is always accepted.

\item An input may be left unassigned, unless
  \begin{itemize}
  \item the option is typed to \mintinline{python}/RequiredOptions()/, or
  \item the option is typed to \mintinline{python}/OptionEnum()/ without a default.
  \end{itemize}
  So, except for the two cases above, it is not necessary to supply
  option values to a method at build time.

\item If typing is specified as a value, this is the default value if
  left unspecified.

\item If typing is specified as a class name, default is
  \mintinline{python}/None/.

\item Values are accepted if they are valid input to the type's
  constructor, i.e.\ 3 and '3' are valid input for an integer.

\item \mintinline{python}/None/ is always a valid input unless
  \begin{itemize}
  \item RequiredOptions() and not none\_ok set
    \index{input options!\texttt{RequiredOptions}}
  \item OptionEnum() and not none\_ok set
    \index{input options!\texttt{OptionEnum}}
  \end{itemize}
  This means that for example something typed to \texttt{int} can be
  overridden by the builder by assigning it to
  \mintinline{python}/None/.  Also, \mintinline{python}/None/ is also
  accepted in typed containers, so a type defined as
  \mintinline{python}/[int]/ will accept the input
  \mintinline{python}/[1, 2, None]/.

\item All containers can be specified as empty, for example
  \mintinline{python}/{}/ which expects a \texttt{dict}.

\item Complex types (like \texttt{dict}s, \texttt{dict}s of
  \texttt{list}s of \texttt{dict}s, \dots) never enforce specific
  keys, only types.  For example, \mintinline{python}/{'a': 'b'}/
  defines a dictionary from strings to strings, and for example
  \mintinline{python}/{'foo': 'bar'}/ is a valid
  assignment.

\item Containers with a type in the values default to empty containers.
  Otherwise the specified values are the default contents.  Example
  \begin{python}
options = dict(
    x = dict,           # will be empty dict as default
    y = {'foo': 'bar'}  # will be {'foo': 'bar'} as default
)
  \end{python}
\end{enumerate}

The following sections will describe typing in more detail.



\subsubsection{Unspecifieds}
An option with no typing may be specified by assigning \texttt{None}.
\begin{python}
options = dict(length=None)  # accepts anything, default is None
\end{python}
Here, \texttt{length} could be set to anything.



\subsubsection*{Scalars}
Scalars are either explicitly typed, as
\begin{python}
options = dict(length=int)   # Requires an intable value or None
\end{python}
or implicitly with default value like
\begin{python}
options = dict(length=3)     # Requires an intable value or None,
                             # default is 3 if left unassigned
\end{python}
In these examples, intable means that the value provided should be
valid input to the \texttt{int} constructor, for example the number~3
or the string \texttt{'3'} both yield the integer number 3.



\subsubsection*{Strings}
A (possibly empty) string with default value \mintinline{python}{None} is typed as
\begin{python}
options = dict(name=str)     # requires string or None, defaults to None
\end{python}
A default value may be specified as follows
\begin{python}
options = dict(name='foo')   # requires string or None, provides default value
\end{python}
And a string required to be specified and none-empty as
\index{input options!\texttt{OptionString}}
\begin{python}
from extras import OptionString
options = dict(name=OptionString)       # requires non-empty string
\end{python}
In some situations, an example string is convenient
\begin{python}
from extras import OptionString
options = dict(name=OptionString('bar') # Requires non-empty string,
                                        # provides example (NOT default value)
\end{python}
Note that ``\texttt{bar}'' is not default, it just gives the
programmer a way to express what is expected.



\subsubsection*{Enums}
Enumerations are convenient in a number of situations.  An option with
three enumerations is typed as
\begin{python}
# Requires one of the strings 'a', 'b' or 'c'
from extras import OptionEnum
options = dict(foo=OptionEnum('a b c'))
\end{python}
and there is a flag to have it accept \mintinline{python}/None/ too
\begin{python}
# Requires one of the strings 'a', 'b', or 'c'; or None
from extras import OptionEnum
options = dict(foo=OptionEnum('a b c', none_ok=True))
\end{python}
A default value may be specified like this
\begin{python}
# Requires one of the strings 'a', 'b' or 'c', defaults to 'b'
from extras import OptionEnum
options = dict(foo=OptionEnum('a b c').b
\end{python}
(The \texttt{none\_ok} flag may be combined with a default value.)
Furthermore, the asterisk-wildcard could be used to accept a wide
range of strings
\begin{python}
# Requires one of the strings 'a', 'b', or any string starting with 'c'
options = dict(foo=OptionEnum('a b c*'))
\end{python}
The example above allows the strings ``\texttt{a}'', ``\texttt{b}'',
and all strings starting with the character ``\texttt{c}''.



\subsubsection*{Lists and Sets}
Lists are specified like this
\begin{python}
# Requires list of intable or None, defaults to empty list
options=dict(foo=[int])
\end{python}
Empty lists are accepted, as well as \mintinline{python}/None/.  In
addition, \mintinline{python}/None/ is also valid inside the list.
Sets are defined similarly
\begin{python}
# Requires set of intable or None, defaults to empty set
options=dict(foo={int})
\end{python}
Here too, both \mintinline{python}/None/ or the empty \texttt{set} is
accepted, and \mintinline{python}/None/ is a valid set member.



\subsubsection*{Date and Time}
The following date and time related types are supported:
\begin{itemize}
\item[] \texttt{datetime},
\item[] \texttt{date},
\item[] \texttt{time}, and
\item[] \texttt{timedelta}.
\end{itemize}
A typical use case is as follows
\begin{python}
# a datetime object if input, or None
from datetime import datetime
options = dict(ts=datetime)
\end{python}
and with a default assignment
\begin{python}
#  a datetime object if input, defaults to a datetime(2014, 1, 1) object
from datetime import datetime
options = dict(ts=datetime(2014, 1, 1))
\end{python}



\subsubsection*{More Complex Stuff:  Containing Types}
It is possible to have more complex types, such as dictionaries of
dictionaries and so on, for example
\begin{python}
# Requires dict of string to string
options = dict(foo={str: str})
\end{python}
or another example
\begin{python}
# Requires dict of string to dict of string to int
options = dict(foo={str: {str: int}})
\end{python}
As always, containers with a type in the values default to empty
containers.  Otherwise, the specified values are the default contents.



\subsubsection*{A File From Another Job:  \texttt{JobWithFile}}

\index{input options!\texttt{JobWithFile}}
Any file residing in a jobdir may be input to a method like this
\begin{python}
from extras import JobWithFile
options = dict(usefile=JobWithFile(jid, 'user.txt')
\end{python}
There are two additional arguments, \texttt{sliced} and
\texttt{extras}.  The \texttt{extras} argument is used to pass any
kind of information that is helpful when using the specified file, and
\texttt{sliced} tells that the file is stored in parallel slices.
\begin{python}
options = dict(usefile=JobWithFile(jid, 'user.txt', sliced=True, extras={'uid': 37}))
\end{python}
(Creating sliced files is described in section~\ref{sec:create_dataset_in_analysis}.)  In a
running method, the \texttt{JobWithFile} object has these members
\begin{python}
usefile.jobid
usefile.filename
usefile.sliced
usefile.extras
\end{python}
%% Where the full filename of the file is available through
%% \begin{python}
%% from extras import full_filename
%% print(full_filename(filename, ''))
%% \end{python}
%% The second option to \texttt{full\_filename} is mandatory, it may be
%% empty as in the example above, and should hold the filename extension.
%% \comment{Will be changed?!}
%% \comment{Example of how to use jobwithfile in analysis and prepare/synthesis.  how does slice work?  How does extras work?}





\section{Jobs - a Summary}
The concepts relating to Accelerator jobs are fundamental, and this
section provides a shorter summary about the basic concepts.

\begin{itemize}
\item[1.]  Data and metadata relating to a job is stored in a
job directory.
\item[2.]  Jobids are pointers to such job directories.
\end{itemize}
The files stored in the job directory at dispatch are complete in the
sense that they contain all information required to run the job.  So
the Accelerator job dispatcher actually just creates processes and
points them to the job directory.  New processes have to go and figure
out their purpose by themselves by looking in this directory.

A running job has the process' \textsl{current working directory
(CWD)} pointing into the job directory, so any files created by the
job (including return values) will by default be stored in the job's
directory.

When a job completes, the meta data files are updated with profiling
infomation, such as execution time spent in single and parallel
processing modes.

All code that is directly related to the job is also stored in the job
directory in a compressed archive.  This archive is typically limited
to the method's source, but the code may have manually added
dependencies to any other files, and in that case these will be added
too.  This way, source code and results are always connected and
conveniently stored in the same directory for future reference.
\begin{itemize}
\item[3.]  Unique jobs are only executed once.
\end{itemize}
Among the meta information stored in the job directory is a hash
digest of the method's source code (including manually added
dependencies).  This hash, together with the input parameters, is used
to figure out if a result could be re-used instead of re-computed.
This brings a number of attractive advantages.
\begin{itemize}
\item[4.]  Jobs may link to eachother using jobids.
\end{itemize}
Which means that jobs may share results and parameters with eachother.
\begin{itemize}
\item[5.]  Jobs are stored in workdirs.
\item[6.]  There may be any number of workdirs.
\end{itemize}
This adds a layer of ``physical separation''.  All jobs relating to
importing a set of data may be stored in one workdir, perhaps named
\texttt{import}, and development work may be stored in a workdir
\texttt{dev}, etc.  Jobids are created by appending a counter to the
workdir name, so a job \texttt{dev-42} may access data in
\texttt{import-37}, and so on, which helps manual inspection.
\begin{itemize}
\item[7.] Jobs may dispatch other jobs.
\end{itemize}
It is perfectly fine for a job to dispatch any number of new jobs, and
these jobs are called \textsl{subjobs}.  A maximum allowed recursion
depth is defined to avoid infinite recursion.



