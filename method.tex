%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% This chapter covers most aspects of jobs and methods.  It describes
%% what methods are and how they are stored; when they are built and when
%% not; what input parameters look like in full detail; how to access
%% another job's parameters; parallel processing; return values and
%% result merging; storing and retrieving data; and how to build subjobs.






\section{Definitions}

\subsection{Methods and Jobs}

In general, doing a computation on a computer follows the following
equation
\[
  \text{source code} \quad+\quad \text{input data and parameters}
  \quad+\quad \text{execution time} \quad\rightarrow\quad
  \text{result}
\]
In the Accelerator context, the notation is as follows
\[
  \text{method} \quad+\quad \text{input data} \quad+\quad \text{input
    parameters} \quad+\quad \text{execution time}
  \quad\rightarrow\quad \text{job}
\]
where the \textbf{method} is the source code, and the \textbf{job} is
a directory containing
\begin{itemize}
\item[--] any number of output files created by the running method, as well as
\item[--] a number of job meta information files containing all
  information needed to reproduce the job.
\end{itemize}
The exact contents of the job directory will be discussed in
section~\ref{sec:job_directories}.

Computing a job is denoted job \textsl{building}.
Jobs are \textbf{built} from methods.  When a job has been built, it
is \textsl{static}, and cannot be altered or removed by the
Accelerator.  Jobs are built either by
\begin{itemize}
\item[--] a \textsl{build script}, see chapter~\ref{chap:urd}, or
\item[--] by a method, using \textsl{subjobs}, see~section~\ref{sec:subjobs}
\end{itemize}
The following figure illustrates how a job ``\texttt{example-0}'' is
built from the method \texttt{a\_method.py}.  The job is stored in the
\texttt{example} work directory.  The job identifier (in this case
\texttt{example-0}) is always unique so that it can be used as a
reference to that particular job.
\begin{figure}[h!]
  \begin{center}
    \hspace{2cm}\input{figures/method.pdftex_t}
    \label{fig:method}
  \end{center}
  \caption{When a method is built, a job directory is created in the
    target work directory, containing files with all data an meta
    information regarding the job.}
\end{figure}


\subsection{Jobids}
A \textbf{jobid} is a string that can be used as a reference to a job.
Since all job directories have unique names, the name of the job
directory is used as the jobid.  In the example above, the job is
uniquely identified by the string \texttt{example-0}.  Jobids are
composed by the name of the workdir and an integer that increments by
one for each new job in that workdir.


\subsection{Work Directories and Job Directories}
\label{sec:job_directories}
A successfully build of a method results in a new job directory on
disk.  The job directory will be stored in the current workdir and
have a structure as follows, assuming the current workdir is
\texttt{test}, and the current jobid is \texttt{test-0}.
\begin{verbatim}
workdirs/test/
    test-0/
        setup.json
        method.tar.gz
        result.pickle
        post.json
        OUTPUT/
        datasets.txt
        default/
\end{verbatim}
The following table shows examples of files commonly found in a job
directory.

\starttabletwo

\texttt{setup.json} & Contains information about the job build,
including name of method, input parameters, and, after execution, some
profiling information.\\

\texttt{post.json} & Contains profiling information, and is written
only if the job builds successfully.\\

\texttt{method.tar.gz} & All source files, i.e.\ the method's source
and any \texttt{depend\_extra}s are stored in this gziped
\texttt{tar}-archive.\\

\texttt{result.pickle} & The return value from \synthesis stored in the
Python ``pickle'' format.\\

\texttt{default/} & If the job contains datasets, these will be stored
in directories, such as for example \texttt{default/}, in the root of
the job directory.\\

\texttt{datasets.txt} & List of all datasets in job in a human
readable format.\\

\texttt{OUTPUT/} & Any output to \texttt{stdout} and \texttt{stderr} will be
stored in the \texttt{OUTPUT/} directory.\\
\stoptabletwo


\subsection{The \texttt{Job} and \texttt{CurrentJob} Convenience Wrappers}
In order to simplify access to job directory data, common job data
operations have been factored into the \texttt{Job} class.  There is
also an extended version of this, called the \texttt{CurrentJob}
class, that also contains information and helper functions to a
running job.  See section~\ref{} for details about these classes.



\section{Python Packages}
Methods are stored in standard Python packages, i.e.\ in directories
that are
\begin{itemize}
\item[--] reachable by the Python interpreter, and
\item[--] contain the (perhaps empty) file \texttt{\_\_init\_\_.py}.
\end{itemize}
In addition, for the Accelerator to accept a package, the following
constraints need to be satisfied
\begin{itemize}
\item[--] the package must contain a file named \texttt{methods.conf}, and
\item[--] the package must be added in the Accelerator's configuration file.
\end{itemize}
A package is reachable by The Accelerator if it is included in the
Accelerator's configuration file using the key
``\mintinline{shell}{method packages}'', see section~\ref{}.


\subsection{Creating a new Package}
The following shell commands illustrate how to create a new package
directory
\begin{shell}
% mkdir <dirname>
% touch <dirname>/__init__.py
% touch <dirname>/methods.conf
\end{shell}
The first two lines create a Python package, and the third line adds
the file \texttt{methods.conf}, which is required by the Accelerator.

For security reasons, the Accelerator only looks for packages
explicitly specified in the configuration file using the
\mintinline{shell}{method_directories} assignment.  See
chapter~\ref{sec:configfile} for detailed information about the
configuration file.



\section{Method Source Files}
Method source files are stored in Python packages as described in the
previous section.  The Accelerator searches all packages for methods
to execute, and therefore \textsl{method names need to be globally
unique}!  In order to reduce risk of executing the wrong file, there
are three limitations that apply to methods:
\begin{enumerate}
\item For a method file to be accepted by the Accelerator, the
  filename has to start with the prefix ``\texttt{a\_}'';
\item the method name, without this prefix must be present on a
  separate line in the \texttt{methods.conf} file for the package, see
  section~\ref{sec:methods_conf}; and
\item the method name must be \emph{globally} unique, i.e.\ there can
  not be a method with the same name in any other method directory
  visible to the Accelerator.
\end{enumerate}


\subsection{Creating a New Method}
In order to create a new method, follow these steps
\begin{enumerate}
\item Create the method in a package viewable to the Accelerator
  using an editor.  Make sure the filename is \texttt{a\_<name>.py} if
  the method's name is \texttt{<name>}.
\item Add the method name \texttt{<name>} (without the prefix ``\texttt{a\_}'' and
  suffix ``\texttt{.py}'') to the \texttt{methods.conf} file in the
  same method directory.  See section~\ref{sec:methods_conf}.
\item (Make sure that the method directory is in the Accelerator's
  configuration file.)
\end{enumerate}


\subsection{Limiting Execution:  \texttt{methods.conf}}
\label{sec:methods_conf}
The file \texttt{methods.conf} provides an easy way to specify and
limit which source files that can be executed, which is something that
makes a lot of sense in any production environment.  Files not
specified in \texttt{methods.conf} cannot be executed.  It also
optionally specifies which Python interpreter each method should use.

The \texttt{methods.conf} is a plain text file with one entry per
line.  Any characters from a hash sign (``\texttt{\#}'') to the end of
the line is considered to be a comment.  It is permitted to have any
number of empty lines in the file.  Available methods are entered
first on a line by stating the name of the method, without the
\texttt{a\_} prefix and \texttt{.py} suffix.

The method name can optionally be followed by one or more whitespaces
and a name specifying the actual Python interpreter that will be used
to execute the method.  A list of valid Python interpreters is defined
in the configuration file using the key
\mintinline{shell}{interpreters}, see section~\ref{sec:configfile}.

The default interpreter is selected if this field is left empty, where
default corresponds to the one that the currently running Accelerator
daemon is using.  The Accelerator and its \texttt{standard\_methods}
library are compatible with both Python 2 and Python 3.

Here is an example \texttt{methods.conf}
\begin{shell}
# this is a comment

test2                # will use default Python
test3           py3  # py3 as specified in accelerator.conf
testx           tf   # the Tensorflow virtual environment Python
#bogusmethod    py3
\end{shell}
This file declares three methods corresponding to the files
\texttt{a\_test2.py}, \texttt{a\_test3.py}, and \texttt{a\_testx.py}.
These are the only methods that can be built in this method package.
Note that it is possible to specify an interpreter from a virtual
environment.  This makes it straightforward to install any package in
a dedicated environment and making it accessible only to a set of
specified mehods.



\section{Job Building or Job Recycling}
Since the Accelerator keeps track of a job's dependencies and results,
it can in an instant determine if a job to be built has been built
before.  If it has been built before, the Acce will immediately return
a job instance to the existing job.  Otherwise, the job will first be
built, and then a job instance will be returned.


\subsection{Job Already Built Check}
From chapter~\ref{chap:urd} it is known that a method is built using
the \texttt{.build()} function in a build script, like this
\begin{python}
def main(urd):
    urd.build('themethod', options=..., ...)
\end{python}

Prior to building a method, the Accelerator checks if an equivalent
job has been built in the past.  This check is based on two things:
\begin{enumerate}
\item  the output of a hash function applied to the method source code, and
\item  the method's input parameters.
\end{enumerate}
The hash value is combined with the input arguments and compared to
all jobs already built.  Only if the hash and input parameter
combination is unique will the method be executed.  The
\texttt{.build()}-function returns an instance of type \texttt{Job}.
To the caller, it is not apparent if the job was just built or if it
was built at an earlier time.


\subsection{Depend on More Files:  \texttt{depend\_extra}}
A method may import code located in other files, and such files can be
included in the build check hash calculation as well.  This will
ensure that a change to an imported file will indeed force a
re-execution of the method if a build is requested.  Additional files
are specified in the method using the \texttt{depend\_extra} list, as
for example:
\begin{python}
from . import my_python_module

depend_extra = (my_python_module, 'mystuff.data',)
\end{python}
As seen in the example, it is possible to specify either Python module
objects or filenames relative to the method's location.

The Accelerator daemon will suggest adding modules to a source file in
the output log like this:
\begin{snugshade}
\begin{verbatim}
=================================================================
WARNING: dev.a_test should probably depend_extra on myfuncs
=================================================================
\end{verbatim}
\end{snugshade}
\noindent The point of this is to make the user aware that the method
depends on additional files that are currently not taken into account
in the build check hashing.  The warning is removed by putting the
\texttt{myfuncs} file in a \texttt{depend\_extra} list of the
\texttt{test} method.


\subsection{Avoiding Rebuild: \texttt{equivalent\_hashes}}
\label{sec:equivalent_hashes}

A change to a method's source code will cause a new job to be built
upon running \mintinline{python}{.build()}, but sometimes it is
desirable to modify the source code \textsl{without} causing a
re-build.  This happens, for example, when new functionality is added
to an existing method, and re-computing all jobs is not an option.  If
functionality remains the same, existing jobs strictly do not need to
be re-built.  For this situation, there is an
\texttt{equivalent\_hashes} dictionary that can be used to specify
which versions of the source code that are equivalent.  The
Accelerator helps creating this, if needed.  This is how it works.
\begin{enumerate}
\item Find the hash \texttt{<old\_hash>} of the existing job in that
  job's \texttt{setup.json}.
\item Add the following line to the updated method's source code
\begin{python}
equivalent_hashes = {'whatever': (<old_hash>,)}
\end{python}
\item Run the build script.  The daemon will print something like
\begin{shell}
===========================================================
WARNING: test_methods.a_test_rechain has equivalent_hashes,
but missing verifier <current_hash>
===========================================================
\end{shell}
\item Enter the \texttt{current\_hash} into the
  \texttt{equivalent\_hashes}:
\begin{python}
equivalent_hashes = {<current_hash>: (<old_hash>,)}
\end{python}
\end{enumerate}
This line now tells that \texttt{current\_hash} is equivalent to
\texttt{old\_hash}, so if a job with the old hash exists, the method
will not be built again.  Note that the right part of the assignment
is actually a list, so there could be any number of equivalent
versions of the source code.  This has been used frequently during
development of the Accelerator's \texttt{standard\_methods}, when new
features have been added that do not interfere with existing use.



\section{Method Execution}
Methods are executed using the \texttt{build()} call, either from a
\texttt{build script}, or from another method as a \texttt{subjob}.
Methods typically takes input parameters, and they may generate return
values and produce output files as well as output to \texttt{stdout}
and \texttt{stderr}.

During execution, methods are not run from top to bottom.  Instead,
there are three reserved functions that are called by the method
dispatcher controlling the execution flow.  These functions are
\begin{itemize}
\item [] \prepare,
\item [] \analysis, and
\item [] \synthesis.
\end{itemize}


\subsection{Execution Order}
The three functions \prepare, \analysis, and \synthesis are called one
at a time in that order.  \prepare and \synthesis execute as single
processes, while \analysis provides parallel execution.  None of them
is mandatory, but at least one must be present for the method to
execute.  It is discouraged to use \prepare only.


\subsection{Input Parameters}
There are three kinds of method input parameters assigned by the
\texttt{build()} call: \jobs, \datasets, and \options.  These
parameters are stated early in the method source code and are
\textsl{global}, meaning that they do not need to be included as
parameters to the functions in a method.  Here is an example parameter
set
\begin{python}
jobs = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are populated by the builder when the
\texttt{run} command is executed.  Sections~\ref{} and~\ref{} provide
detailed descriptions of all parametes.



\subsection{Function Arguments}
There are two constants that can be passed into the executing
functions \prepare, \analysis, and \synthesis at run time.
\begin{itemize}
\item[--] \texttt{job}, which is an instance of the current job, and
\item[--] \texttt{sliceno}, which provides a unique number to each
  parallel \analysis-process.
\end{itemize}

The \texttt{job} instance contains information and helper functions
regarding the current job.  The object is of type \texttt{CurrentJob},
which is an extension of the \texttt{Job} class used for job instances
that are not in the execution stage.  See section~\ref{} for details.

The \analysis function (and only the \analysis function) takes the
optional argument \texttt{sliceno}, which is an integer between zero
and the total number of slices minus one.  This is the unique
identifier for each \analysis process, and it is discussed further in
section~\ref{}.


\subsection{Parallel Processing:  The \analysis function, Slices, and Datasets}
The number of parallel analysis processes is set by the
\texttt{slices} parameter in the Accelerator's configuration file.
The idea is that when the Accelerator is processing a dataset, each
dataset slice should have exactly one corresponding \analysis process,
so that all the slices in the dataset can be processed in parallel.
The input parameter \texttt{sliceno} to the \analysis function is the
unique identifier for each parallel function call, and its value is in
the range from zero to the number of slices minus one.


\subsection{Return Values}
Return values may be passed from one function to any function that
will execute later.  In particular, what is returned from prepare is
called \prepareres, and can be used as input argument to \analysis and
\synthesis.  Furthermore, the return values from \analysis are
available as \analysisres in \synthesis.  The \analysisres variable is
an iterator, yielding the results from each slice in turn.  Finally,
the return value from \synthesis is stored permanently in the job
directory using the name ``\texttt{result.pickle}''.  Here is an
example of return value passing
\begin{python}
options = dict(length=4)

def prepare():
    # options is a global variable
    return options.length * 2

def analysis(sliceno, prepare_res)
    return prepare_res + sliceno

def synthesis(analysis_res, prepare_res):
     return sum(analysis_res) + prepare_res
\end{python}
Note that when a job completes, it is not possible to retrieve the
results from \prepare or \analysis anymore.  Only results from
\synthesis are kept.  Creating permanent files is the topic of
section~\ref{}.


\subsection{Merging Results from \analysis}
Consider this example
\begin{python}
# create a set of all users
datasets = ('source',)
  
def analysis(sliceno):
    return(datasets.source.iterate(sliceno, 'user'))

def prepare(analysis_res):
    return analysis_res.merge_auto()
\end{python}
Here, each \analysis process creates a set of \texttt{user}s seen in
that \textsl{slice} of the \texttt{source} dataset.  In order to
create a set of all \texttt{user}s in the dataset, all slice-sets have
to be merged.  Merging can be implemented using for example a
\texttt{for}-loop, but the actual merging operation is dependendent of
the actual data type, and writing merging functions is error prone.
Therefore, \analysisres has a function called \texttt{merge\_auto()},
that is recommended for merging.  This function can merge most data
types, and even merge container variables in a recursive fashion.  For
example, a variable defined like this
\begin{python}
h = defaultdict(lambda: defaultdict(set))
\end{python}
(a \texttt{dict} of \texttt{dict}s of \texttt{set}s) is
straightforward to merge using \texttt{merge\_auto()}.  The function
works on many data types and is less error-prone than writing special
mergers every time they are needed.


\subsection{Standard Out and Standard Error}
\label{sec:OUTPUT}
In a method, anything that is sent to \texttt{stdout} or
\texttt{stderr} will be sent \textsl{both} to the termininal in which
the Accelerator daemon was started \textsl{and} to a file in the
current job directory.  This covers, for example, anything output from
Python's \texttt{print()}-function.

Output is collected in the job directory in a subdirectory named
\texttt{OUTPUT}, and it is made available using the \texttt{.output()}
function, see section~\ref{sec:jobclass:output}.  The \texttt{OUTPUT}
directory is created \textsl{only} if anything was output from the job
to \texttt{stdout} or \texttt{stderr}, otherwise it does not exist.
Inside the directory there may be files like this
\begin{verbatim}
job-x/
   OUTPUT/
      prepare    # created if output in prepare()
      synthesis  #                      synthesis()
      0          #                      analysis() slice 0
      3          #                                       3
\end{verbatim}
No empty files will be created.




\section{The \texttt{Job} and \texttt{CurrentJob} Classes}
The \texttt{Job} and \texttt{CurrentJob} classes provide functionality
for easy access to data and datasets stored in a job directory.
(Datasets will be covered in section~\ref{}).  The \texttt{CurrentJob}
is an extension of \texttt{Job} that adds special functions that are
useful to a method during execution.  \textsl{This section just
  provides a taste of the most common operations that are provided.
  See section~\ref{} for a complete list of the functionality.}

Instances of these two classes are used extensibly.  In a build script
every reference to a job, such as the return value of the
\texttt{.build()} function or any job retreival using the Urd database
are of type \texttt{Job}.  Any job passed as input parameter to a
\texttt{.build()}-call will appear as a \texttt{Job} instance inside
the running method.  There is only one way to get a variable of type
\texttt{CurrentJob}, though, and that is to ask for a \texttt{job}
input parameter in one of \prepare, \analysis, or \synthesis.


\subsection{Writing and Reading Serialised Data}
Data structures may be serialsed and written to disk using
\texttt{job.save()} and \texttt{job.json\_save()}, with corresponding
\texttt{.load()} and \texttt{.json\_load()}, where the first writes a
Python ``pickle'' file, and the latter uses \texttt{json} encoding.
Here is an example
\begin{python}
def synthesis(job):
   job.save('a string to be written', 'stringfile')
   job.json_save(dict(key='value'), 'jsonfile')
\end{python}
The corresponding \texttt{job.load()} and \texttt{job.json\_load()}
functions that can be called \textsl{both} in methods \textsl{and}
build scripts.  For example
\begin{python}
jobs = ('anotherjob',)

def synthesis():
    jobs.anotherjob.load('stringfile')
\end{python}
will load a file from another job into the currently running method, while
\begin{python}
def main(urd):
    job = urd.build('example')
    x = job.load('thefile')
\end{python}
will load data stored by the \texttt{example} method using the
filename \texttt{thefile.pickle} into the build script.


\subsection{Writing and Reading Serialised Data in Parallel}
\label{sec:sliced_files}
If data is read and written in the parallel \analysis-function, the
argument \texttt{slices=} may be used to write one file for each
slice.  For example
\begin{python}
def analysis(sliceno, job)
    data = ...
    job.save(data, 'filename', sliceno=sliceno)
\end{python}
Similarly, another job can then read one of these files per slice as follows
\begin{python}
def analysis(sliceno, job):
    data = job.load('filename', sliceno=sliceno)
\end{python}
Writing ``sliced'' data results in $n$ files on disk, where $n$ is
equal to the number of slices set in the configuration file.  Each
filename is extended with a human readable number that corresponds to
the slice that the file's data belongs to.



\subsection{General File Access}
The \texttt{.open()} function corresponds to the built in
\texttt{open()} with the addition that it cannot write to existing
jobs, and that files written using it are book-kept in the job.  It
has to be used as a context manager, i.e.\ using the \texttt{with}
statement, for example like this
\begin{python}
def synthesis(job):
    with job.open('filename, 'wb') as fh:
        fh.write(...)
\end{python}


\subsection{Accessing A Job's Return Value}
The default behaviour of a job instance's \texttt{.load()} function is
to read the return value from the job's \synthesis function, like this
\begin{python}
def main(urd):
    job = urd.build('example')
    x = job.load()
\end{python}
This works both in build scripts and inside methods.


\subsection{Accessing A Job's Datasets}
Using the \texttt{Job} class, it is straightforward to access datasets
in other jobs.  For example
\begin{python}
def main(urd):
    job = urd.build(...)

    # This will print a list of all dataset instances in the job.
    print(job.datasets())

    # This will return a dataset instance to the job/training
    # dataset.
    ds = job.dataset('training')
\end{python}
This works both in running methods and in build scripts.


\subsection{Accessing A Job's Options and Parameters}
\label{sec:params}
There are two sources of parameters to a running method,
\begin{itemize}
\item [] parameters from the caller, i.e.\ the \texttt{.build()}-call,
  and
\item [] parameters assigned by the Accelerator when the job starts
  building.
\end{itemize}
All these parameters are available using the \texttt{job.params}
function.  This is useful for example in methods that needs to find the
options to its input jobs.  For example
\begin{python}
import json

jobs = ('anotherjob',)

def synthesis():
    print(jobs.anotherjob.params.options)
\end{python}
will print the \texttt{options} dictionary that was fed to the
\texttt{anotherjob} at build time, for example
\begin{snugshade}
\begin{verbatim}
{'message': 'Hello world!'}
\end{verbatim}
\end{snugshade}
\noindent A complete print of a job's \texttt{.params} may look like this
\begin{json}
  {
    "slices": 8,
    "caption": "fsm_example2",
    "jobs": {
      "anotherjob": "dev-695"
    },
    "version": 1,
    "jobid": "dev-755",
    "package": "dev",
    "python": "/home/eaenbrd/accvenv/bin/python",
    "starttime": 1574239517.6100385,
    "hash": "0f9f40063568c896244a63e6073d2803a071fc2a",
    "options": {},
    "method": "example2",
    "seed": 7731745544325830724,
    "datasets": {}
  }
\end{json}

\noindent and a description of its keys
\starttabletwo
\texttt{package} & Python package for this method\\
\texttt{method} & name of this method\\
\texttt{jobid} & jobid of this job\\

\texttt{starttime}& start time in epoch format\\
\texttt{caption} & a caption\\
\texttt{slices} & number of slices of current Accelerator configuration \\
\texttt{seed} & a random seed available for use$^1$\\
\texttt{hash} & source code hash value\\
\texttt{python} & Python interpreter for this job\\

\texttt{options} & input parameter\\
\texttt{datasets} & input parameter\\
\texttt{jobs} &  input parameter\\
\hline\\

\multicolumn{2}{l}{\small{$^1$ The Accelerator team recommends \emph{not} using
    \texttt{seed}, unless non-determinism is actually a goal.}}\\
\stoptabletwo


\subsection{Accessing Job Output}
\label{sec:jobclass:output}
Anything written to \texttt{stderr} or \texttt{stdout} during job
execution is available using the \texttt{.output()} function.  Here is
an example
\begin{python}
def main(urd):
    job = urd.build('example')
    print(job.output())
\end{python}
With no argument, the \texttt{.output()} function returns all output.
Particular parts of the output can be selected using the options,
\texttt{'prepare'}, \texttt{'analysis'}, \texttt{'synthesis'}, or a
digit specifying a particular slice.


\subsection{Reading Post Data}
The \texttt{.post} attribute contains information such as starttime,
execution time (per function and slice), written files and subjobs for
a job.  For example
\begin{python}
def main(urd):
    job = job.build('example')
    print(job.post.exectime)
\end{python}



\section{Method Input Parameters}
\label{sec:input_params}

There are three kinds of method input parameters assign by
the \texttt{build} call: \jobs, \datasets, and \options.  These
parameters are stated early in the method source code, such as for
example
\begin{python}
jobs = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are populated by the builder when the
\texttt{run} command is executed, see~\ref{chap:urd}.

The \texttt{jobs} parameter list is used to input references to
other jobs, while the \datasets parameter list is used to input
references to datasets.  These parameters must be populated by the
build call.

The \options dictionary, on the other hand, is used to input any other
type of parameters to be used by the method at run time.  Options does
not necessarily be populated by the build call, and this can be used
for ``global constants'' in the method.  An option assigned by the
build call will override the default assignment.

Note that \jobs and \datasets are \texttt{tuple}s (or \texttt{list}s
or \texttt{set}s), and a single entry has to be followed by a comma as
in the example above, while \options is a dictionary.  Individual
elements of the input parameters may be accessed inside the method
using dot notation like this
\begin{python}
jobs.accumulated_cost
datasets.transaction_log
options.length
\end{python}
Each of these parameters will be described in more detail in following
sections.


\subsection{Input Jobs}
The \jobs parameter is a tuple of job references linking other jobs
to this job.  In a running method, each item in the \jobs tuple is
of type \texttt{Job} that is used as a reference to the corresponding
job.  All items in the \jobs tuple must be assigned by the builder
to avoid run time errors.

It is possible to specify lists of jobs, see this example
\begin{python}
jobs = ('source', ['alistofjobs'],)
\end{python}
where \texttt{source} is a single job reference, whereas
\texttt{alistofjobs} is a list of job references.


\subsection{Input Datasets}
The \datasets parameter is a tuple of links to datasets.  In a running
method, each item in the \datasets variable is of type
\texttt{Dataset}.  The \texttt{Dataset} class is described in a
dedicated chapter~\ref{chap:datasets}.  All items in the \datasets
tuple must be assigned by the builder to avoid run time errors.

It is possible to specify lists of datasets, see this example
\begin{python}
datasets = ('source', ['alistofdatasets'],)
\end{python}
where \texttt{source} is a single dataset, whereas
\texttt{alistofdatasets} is a list of datasets.



\subsection{Input Options}

The \options parameter is of type \texttt{dict} and is used to pass
various information from the builder to a job.  This information could
be integers, strings, enumerations, sets, lists, and dictionaries in a
recursive fashion, with or without default values.  Assigning options
from the build call is not necessary, but an assignment will override
the ``default'' that is specified in the method.  Options are
specified like this
\begin{python}
  options = dict(key=value, ... )  # or
  options = {key: value, ...}
\end{python}

Options are straightforward to use and quite flexible.  A formal
overview is presented in section~\ref{sec:formal_options}.



\section{Subjobs}
\index{subjobs}
\label{sec:subjobs}
Jobs may launch subjobs, i.e.\ methods may build other methods in a
recursive manner.  As always, if the jobs have been built already,
they will immediately be linked in.  If the build of a subjob fails,
the buiding job will be invalidated.

The syntax for building a job inside a method is as follows, assuming
we build the jobs in \prepare
\begin{python}
from accelerator import subjobs

def prepare():
    subjobs.build('count_items', options=dict(length=3))
\end{python}
It is possible to build subjobs in \prepare and \synthesis, but not in
\analysis.  The \texttt{subjobs.build()} call uses the same syntax as
\texttt{urd.build()} described in chapter~\ref{chap:urd}, so the input
parameters \options, \datasets, \jobs, and \texttt{caption} are
available here too.  Similarly, the return value from a subjob
\texttt{build()} is a job instance corresponding to the built job.

There are three catches, though.
\begin{enumerate}
\item
Dataset instances to datasets created in subjobs will not be
explicitly available to the build script.  The workaround is to link
the dataset to the building method like this
\index{\texttt{link\_to\_here}}
\begin{python}
def synthesis():
    job = subjobs.build('create_a_dataset')
    ds = job.dataset(<name>)
    ds.link_to_here(name=<anothername>)
\end{python}
with the effect that the building job will act like a dataset, even
though the dataset is actually created and stored in the subjob.  The
\texttt{name} argument is optional, the name \texttt{default} is used
if left empty, corresponding to the default dataset.

It is possible to override the dataset's previous using the
\texttt{override\_previous} option, which takes a job reference (or \pyNone)
to be the new \texttt{previous}.  \index{\texttt{override\_previous}}
\begin{python}
    ds.link_to_here(name='thename', override_previous=xxx)
\end{python}
The \texttt{link\_to\_here} call returns a dataset instance.

\item
Currently there is no dependency checking on subjobs, so if a subjob
method is changed, the calling method will not be updated.  The
current remedy is to use \texttt{depend\_extra} in the building
method, like this
\begin{python}
from accelerator import subjobs

depend_extra = ('a_childjob.py',)

def prepare():
    subjobs.build('childjob')
\end{python}

\item
  Subjobs will not appear in build script in the \texttt{urd.joblist}.

\end{enumerate}
There is a limit to the recursion depth of subjobs, to avoid creating
unlimited number of jobs by accident.



\section{Formal Option Rules}
\label{sec:formal_options}
This section covers the formal rules for the \options parameter.
\begin{enumerate}

\item Typing may be specified using the class name (i.e.\ \texttt{int}), or as
  a value that will construct into such a class object (i.e.\ the
  number 3).  See this example
  \begin{python}
options = dict(
    a = 3,     # typed to int
    b = int,   #          int
    c = 3.14,  #          float
    d = '',    #          str
)
  \end{python}
  Values will be default values, and this is described thoroughly in
  the other rules.

 \item An input option value is required to be of the correct type.
   This is, if a type is specified for an option, this must be
   respected by the builder.  Regardless of type,
   \mintinline{python}/None/ is always accepted.

\item An input may be left unassigned, unless
  \begin{itemize}
  \item the option is typed to \mintinline{python}/RequiredOptions()/, or
  \item the option is typed to \mintinline{python}/OptionEnum()/ without a default.
  \end{itemize}
  So, except for the two cases above, it is not necessary to supply
  option values to a method at build time.

\item If typing is specified as a value, this is the default value if
  left unspecified.

\item If typing is specified as a class name, default is
  \mintinline{python}/None/.

\item Values are accepted if they are valid input to the type's
  constructor, i.e.\ 3 and '3' are valid input for an integer.

\item \mintinline{python}/None/ is always a valid input unless
  \begin{itemize}
  \item RequiredOptions() and not none\_ok set
    \index{input options!\texttt{RequiredOptions}}
  \item OptionEnum() and not none\_ok set
    \index{input options!\texttt{OptionEnum}}
  \end{itemize}
  This means that for example something typed to \texttt{int} can be
  overridden by the builder by assigning it to
  \mintinline{python}/None/.  Also, \mintinline{python}/None/ is also
  accepted in typed containers, so a type defined as
  \mintinline{python}/[int]/ will accept the input
  \mintinline{python}/[1, 2, None]/.

\item All containers can be specified as empty, for example
  \mintinline{python}/{}/ which expects a \texttt{dict}.

\item Complex types (like \texttt{dict}s, \texttt{dict}s of
  \texttt{list}s of \texttt{dict}s, \dots) never enforce specific
  keys, only types.  For example, \mintinline{python}/{'a': 'b'}/
  defines a dictionary from strings to strings, and for example
  \mintinline{python}/{'foo': 'bar'}/ is a valid
  assignment.

\item Containers with a type in the values default to empty containers.
  Otherwise the specified values are the default contents.  Example
  \begin{python}
options = dict(
    x = dict,           # will be empty dict as default
    y = {'foo': 'bar'}  # will be {'foo': 'bar'} as default
)
  \end{python}
\end{enumerate}

The following sections will describe typing in more detail.



\subsection{Options with no Type}
An option with no typing may be specified by assigning \texttt{None}.
\begin{python}
options = dict(length=None)  # accepts anything, default is None
\end{python}
Here, \texttt{length} could be set to anything.



\subsection{Scalar Options}
Scalars are either explicitly typed, as
\begin{python}
options = dict(length=int)   # Requires an intable value or None
\end{python}
or implicitly with default value like
\begin{python}
options = dict(length=3)     # Requires an intable value or None,
                             # default is 3 if left unassigned
\end{python}
In these examples, intable means that the value provided should be
valid input to the \texttt{int} constructor, for example the number~3
or the string \texttt{'3'} both yield the integer number 3.



\subsection{String Options}
A (possibly empty) string with default value \mintinline{python}{None} is typed as
\begin{python}
options = dict(name=str)     # requires string or None, defaults to None
\end{python}
A default value may be specified as follows
\begin{python}
options = dict(name='foo')   # requires string or None, provides default value
\end{python}
And a string required to be specified and none-empty as
\index{input options!\texttt{OptionString}}
\begin{python}
from accelerator import OptionString
options = dict(name=OptionString)       # requires non-empty string
\end{python}
In some situations, an example string is convenient
\begin{python}
from accelerator import OptionString
options = dict(name=OptionString('bar') # Requires non-empty string,
                                        # provides example (NOT default value)
\end{python}
Note that ``\texttt{bar}'' is not default, it just gives the
programmer a way to express what is expected.



\subsection{Enumerated Options}
Enumerations are convenient in a number of situations.  An option with
three enumerations is typed as
\begin{python}
# Requires one of the strings 'a', 'b' or 'c'
from accelerator import OptionEnum
options = dict(foo=OptionEnum('a b c'))
\end{python}
and there is a flag to have it accept \mintinline{python}/None/ too
\begin{python}
# Requires one of the strings 'a', 'b', or 'c'; or None
from accelerator import OptionEnum
options = dict(foo=OptionEnum('a b c', none_ok=True))
\end{python}
A default value may be specified like this
\begin{python}
# Requires one of the strings 'a', 'b' or 'c', defaults to 'b'
from accelerator import OptionEnum
options = dict(foo=OptionEnum('a b c').b)
\end{python}
(The \texttt{none\_ok} flag may be combined with a default value.)
Furthermore, the asterisk-wildcard could be used to accept a wide
range of strings
\begin{python}
# Requires one of the strings 'a', 'b', or any string starting with 'c'
options = dict(foo=OptionEnum('a b c*'))
\end{python}
The example above allows the strings ``\texttt{a}'', ``\texttt{b}'',
and all strings starting with the character ``\texttt{c}''.



\subsection{List and Set Options}
Lists are specified like this
\begin{python}
# Requires list of intable or None, defaults to empty list
options=dict(foo=[int])
\end{python}
Empty lists are accepted, as well as \mintinline{python}/None/.  In
addition, \mintinline{python}/None/ is also valid inside the list.
Sets are defined similarly
\begin{python}
# Requires set of intable or None, defaults to empty set
options=dict(foo={int})
\end{python}
Here too, both \mintinline{python}/None/ or the empty \texttt{set} is
accepted, and \mintinline{python}/None/ is a valid set member.



\subsection{Date and Time Options}
The following date and time related types are supported:
\begin{itemize}
\item[] \texttt{datetime},
\item[] \texttt{date},
\item[] \texttt{time}, and
\item[] \texttt{timedelta}.
\end{itemize}
A typical use case is as follows
\begin{python}
# a datetime object if input, or None
from datetime import datetime
options = dict(ts=datetime)
\end{python}
and with a default assignment
\begin{python}
#  a datetime object if input, defaults to a datetime(2014, 1, 1) object
from datetime import datetime
options = dict(ts=datetime(2014, 1, 1))
\end{python}



\subsection{More Complex Stuff:  Types Containing Types}
It is possible to have more complex types, such as dictionaries of
dictionaries and so on, for example
\begin{python}
# Requires dict of string to string
options = dict(foo={str: str})
\end{python}
or another example
\begin{python}
# Requires dict of string to dict of string to int
options = dict(foo={str: {str: int}})
\end{python}
As always, containers with a type in the values default to empty
containers.  Otherwise, the specified values are the default contents.



\subsection{A Specific File From Another Job:  \texttt{JobWithFile}}
\index{input options!\texttt{JobWithFile}}
\label{sec:jobwithfile}
Any specific file from an existing job can be input to a new job at
build time using \texttt{job.withfile()}.  Here is an example
\begin{python}
def main(urd):
    job = urd.build('example4')
    urd.build('example5',
              firstfile=job.withfile('myfile1', sliced=True),
              secondfile=job.withfile('myfile2'))
\end{python}
Inside the method, the \texttt{option} part is defined like this
\begin{python}
from accelerator import JobWithFile
options=dict(firstfile=JobWithFile, secondfile=JobWithFile)
\end{python}
The \texttt{.withfile()} function requires a filename and takes two
optional arguments: \texttt{sliced} and \texttt{extras}.  The
\texttt{extras} argument is used to pass any kind of information that
is helpful when using the specified file, and \texttt{sliced} tells
that the file is stored in parallel slices.  (Creating sliced files is
described in section~\ref{sec:sliced_files}.)  In the
running method, the \texttt{JobWithFile} object is an extension of the
\texttt{Job} object with the following extra attributes
\begin{python}
job.filename
job.sliced
job.extra
\end{python}
Loading the file is done using \texttt{.load()}, like this
\begin{python}
from accelerator import JobWithFile
options=dict(firstfile=JobWithFile, secondfile=JobWithFile)

def analysis(sliceno):
    print(options.firstfile.load(sliceno=sliceno))

def synthesis():
    print(options.secondfile.load())
\end{python}



\section{Jobs - a Summary}
The concepts relating to Accelerator jobs are fundamental, and this
section provides a shorter summary about the basic concepts.

\begin{itemize}
\item[1.]  Data and metadata relating to a job is stored in a
job directory.
\item[2.]  Jobs are objects that wraps such job directories.
\end{itemize}
The files stored in the job directory at dispatch are complete in the
sense that they contain all information required to run the job.  So
the Accelerator job dispatcher actually just creates processes and
points them to the job directory.  New processes have to go and figure
out their purpose by themselves by looking in this directory.

A running job has the process' \textsl{current working directory
(CWD)} pointing into the job directory, so any files created by the
job (including return values) will by default be stored in the job's
directory.

When a job completes, the meta data files are updated with profiling
information, such as execution time spent in single and parallel
processing modes.

All code that is directly related to the job is also stored in the job
directory in a compressed archive.  This archive is typically limited
to the method's source, but the code may have manually added
dependencies to any other files, and in that case these will be added
too.  This way, source code and results are always connected and
conveniently stored in the same directory for future reference.
\begin{itemize}
\item[3.]  Unique jobs are only executed once.
\end{itemize}
Among the meta information stored in the job directory is a hash
digest of the method's source code (including manually added
dependencies).  This hash, together with the input parameters, is used
to figure out if a result could be re-used instead of re-computed.
This brings a number of attractive advantages.
\begin{itemize}
\item[4.]  Jobs may link to eachother using job references.
\end{itemize}
Which means that jobs may share results and parameters with eachother.
\begin{itemize}
\item[5.]  Jobs are stored in workdirs.
\item[6.]  There may be any number of workdirs.
\end{itemize}
This adds a layer of ``physical separation''.  All jobs relating to
importing a set of data may be stored in one workdir, perhaps named
\texttt{import}, and development work may be stored in a workdir
\texttt{dev}, etc.  Jobids are created by appending a counter to the
workdir name, so a job \texttt{dev-42} may access data in
\texttt{import-37}, and so on, which helps manual inspection.
\begin{itemize}
\item[7.] Jobs may dispatch other jobs.
\end{itemize}
It is perfectly fine for a job to dispatch any number of new jobs, and
these jobs are called \textsl{subjobs}.  A maximum allowed recursion
depth is defined to avoid infinite recursion.



