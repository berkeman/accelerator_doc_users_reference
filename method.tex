%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



Methods are source files that can be executed by the Accelerator.
They may be built by a build script or by another method as a subjob.
When methods are build, i.e.\ assigned input arguments and executed,
the result is called a job.


\section{Method Directories}

Methods are stored in and grouped into method directories,
corresponding to file system directories.  The recommended location of
method directories is one level above the Accelerator's home
directory.  This is set up automatically if the Accelerator is
installed using the Project Skeleton repository~\ref{xx}.

Method directories are just like any directories, with the addition of
two specific files, \texttt{\_\_init\_\_.py} and \texttt{methods.conf}.
The first file is required for Python to see it as a library, and the
latter is there to limit execution to permitted files only.  If a
method is not listed in the method directory's \texttt{methods.conf},
it can not be executed by the Accelerator.  The Accelerator has a
history of use in live production environments, and in such scenarios
it is important to keep track of which code that is allowed to
execute.

Here is a typical setup
\begin{verbatim}
project_directory/
    accelerator                     # accelerator home
    accelerator/standard_methods    # method directory for bundled methods
    dev                             # a method directory, here be methods
    my_dev                          # another method directory
\end{verbatim}
For a method directory to be visible by the Accelerator, two things are needed
\begin{enumerate}
\item it must be specified in the Accelerator's configuration file,
  see section~\ref{configfile};
\item the method directory must contain an empty file named
  \texttt{\_\_init\_\_.py}; and
\item the method directory must have a \texttt{methods.conf} file.
\end{enumerate}

\subsection*{Creating a New Method Directory}
Make sure the current directory is either one step above the
Accelerator directory (preferred), or is the Accelerator directory.
\begin{enumerate}
\item Create and populate the directory, here called \texttt{<dirname>}:
  \begin{shell}
% mkdir <dirname>
% touch <dirname>/__init__.py
% touch <dirname>/methods.conf
  \end{shell}
\item Add the method directory \texttt{<dirname>} to the Accelerator's
  configuration file, see~\ref{xx}.
\item Restart the Accelerator
\end{enumerate}


\section{Method Source Files}

The Accelerator searches method directories for methods to execute.
In order to reduce risk of executing the wrong files, there are three
limitations that apply to methods:
\begin{enumerate}
\item For a method file to be accepted by the Accelerator, the
  filename has to start with the prefix ``\texttt{a\_}'';
\item the method name, without this prefix must be present on a
  separate line in the \texttt{methods.conf} file for the package, see
  section~\ref{methods_conf}; and
\item the method name must be \emph{globally} unique, i.e.\ there can
  not be a method with the same name in any other method directory
  visible to the Accelerator.
\end{enumerate}


\subsection*{Adding a New Method}
\begin{enumerate}
  \item Create the method in a method directory using an editor.  Make
    sure the filename is \texttt{a\_<name>.py} if the method's name is
    \texttt{<name>}.
  \item Add the method name \texttt{<name>} (without the prefix ``\texttt{a\_}'' and
    suffix ``\texttt{.py}'') to the \texttt{methods.conf} file in the
    same method directory.  See section~\ref{xx}.
  \item (Make sure that the method directory is in the Accelerator's
    configuration file.)
\end{enumerate}


\section{Method Already Built Check}

Prior to building a method, the Accelerator checks if an equivalent
job has been build in the past.  If it has, it will not be executed
again.  This check is based on two things:
\begin{enumerate}
\item  the output of a hash function applied to the method source code, and
\item  the method's input parameters.
\end{enumerate}
The hash value is combined with the input arguments and compared to
all jobs already built.  Only if the hash and input parameter
combination is unique will the method be executed.

A method may import code located in other files, and these other files
can be included in the hash calculation as well.  This will ensure
that a change to an imported file will indeed force a re-execution of
the method if a build is requested.  Additional files are specified in
the method using the \texttt{depend\_extra} list, as for example:
\begin{python}
import my_python_module
depend_extra = (my_python_module, 'mystuff.data',)
\end{python}
It is possible to specify either Python module objects or a filename
relative to the method's location.

\section{Avoiding Rebuild}

A different scenario is when the method source code needs to be
modified, but the modification does not alter its behavior, so
rebuilding jobs should be actively avoided.  A simple example would be
changing the name of a variable in the method.  For this situation,
there is an \texttt{equivalent\_hashes} \texttt{dict} that can be used
to specify which versions of the source code that are equivalent.  For
example
\begin{python}
equivalent_hashes = {'verifier': ('0c573685f713ac6500a6eda7df1a7b3f',)}
\end{python}
The verifier string is a hash that depends on everything in the method
except the \texttt{equivalent\_hashes} block.  When the hash value of
the method does not correspond to the verifier (as will happen after
editing the method) the Accelerator will tell what the correct
verifier would be, so that it can be added to the list of verifiers if
appropriate.  The verifier list can contain any number of old hashes.

\comment{KOLLA DETTA}



\clearpage
\section{Method Parameters}

A jobs parameters are available in the \texttt{params} variable.  It
contains both input parameters that are assigned from the caller, and
parameters that are created when the job starts building.  Input
parameters will be described thoroughly in section~\ref{xx}.  Consider
the following example method that pretty-prints the \texttt{params}
variable.  Note that the method explicitly expects input parameters to
be assigned by the caller in the \jobids, \datasets, and \options
variables.

\begin{python}
import json

jobids = ('jid',)
datasets = ('source', 'parent',)
options = dict(x='testing', length=3)

def synthesis(params):
    print(json.dumps(params, indent=4))
\end{python}
And here is the corresponding output.
\begin{leftbar}
\begin{minted}{json}
{
    "package": "dev",
    "method": "my_example_method",
    "jobid": "EXAMPLE-12",
    "starttime": 1520652213.4547446,
    "slices": 16,
    "options": {
        "mode": "testing",
        "length": 3
    },
    "datasets": {
        "source": "EXAMPLE-3",
        "parent": "EXAMPLE-2"
    },
    "caption": "fsm_my_example_method",
    "seed": 53231916470152325,
    "jobids": {
        "jid": "EXAMPLE-0"
    },
    "hash": "42af401251840b3798e9e78da5b5c5b4ef525ecc"
}
\end{minted}
\end{leftbar}
\comment{remove ``link''}
and a description of its keys
\begin{snugshade}
\begin{tabular}{ll}
  \textsl{key} & \textsl{description}\\[2ex]\hline\\[1ex]
  \texttt{package} & method directory for this method\\
  \texttt{method} & name of this method\\
  \texttt{jobid} & jobid of this job\\[1ex]

  \texttt{starttime}& start time in epoch format\\
  \texttt{caption} & a caption\\
  \texttt{slices} & number of slices of current Accelerator configuration \\
  \texttt{seed} & a random seed available for use$^1$\\
  \texttt{hash} & current source code hash value\\[1ex]

  \texttt{options} & input parameter\\
  \texttt{dataset} & input parameter\\
  \texttt{jobids} &  input parameter\\[1ex] \hline
\end{tabular}\\
\small{$^1$ The Accelerator team recommends \emph{not} using
  \texttt{seed}, since it voids determinism.}
\end{snugshade}


\section{Input Parameters}
A method is typically provided with input parameters at build time.
There are three kinds of method input parameters: \jobids, \datasets,
and \options.  These parameters are specified early in the method
source code, such as for example
\begin{python}
jobids = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are then populated by the builder~\ref{xx>}.

The \texttt{jobids} parameter list is used to input links, or jobids,
to other jobs, while the \datasets parameter list is used to input
links to datasets. The \options dictionary is used to input any other
type of parameters to be used by the method at run time.  Note that
\jobids and \datasets are tuples (and a single entry has to be
followed by a comma as in the example above), while \options is a
dictionary.  Individual elements of the input parameters may be
accessed with dot notation like this
\begin{python}
jobids.accumulated_cost
datasets.transaction_log
options.length
\end{python}
Each of these parameters will be described in more detail in following
sections.




\subsection*{Jobids}
The \jobids parameter is a tuple of jobids linking this job to other
jobs.  Inside the running method, each item in the \jobids tuple may
be used as a reference to the corresponding job.

\comment{SHOULD WE ALLOW EMPTY JOBIDS?}



\subsection*{Input Datasets}
The \datasets parameter is a tuple of links to Datasets.  In the
running method, each item in the \datasets variable is a tuple of
objects from the dataset class.  The dataset class is described in a
dedicated chapter~\ref{xx}.

All items in the \datasets tuple must be assigned by the builder to
avoid run time errors.

\comment{BUT WE NEED TO BE ABLE TO INDICATE START OF CHAIN, FOR EXAMPLE BY None.}



\subsection*{Input Options}

The \options parameter is of type \texttt{dict} and used to pass
various information from the builder to a job.  This information could
be integers, strings, enumerations, sets, lists, and dictionaries in a
recursive fashion, with or without default values.

Options are straightforward to use, but actually quite advanced, and
the following sections introduce how to use them both from a formal
perspective, and from a set of examples.  On the highest level,
options are specified like this
\begin{python}
  options = dict(key=value, ... )  # or
  options = {key: value, ...}
\end{python}


\subsubsection{Formal Option Rules}
Here are the formal rules for the \options parameter.
\begin{enumerate}

\item Typing may be specified using the class name (i.e.\ \texttt{int}), or as
  a value that will construct into such a class object (i.e.\ the
  number 3).  See this example
  \begin{python}
options = dict(
    a = 3,     # typed to int
    b = int,   #          int
    c = 3.14,  #          float
    d = '',    #          str
)
  \end{python}
  Values will be default values, and this is described thoroughly in
  the other rules.

 \item An input option value is required to be of the correct type.
   This is, if a type is specified for an option, this must be
   respected by the builder.  Regardless of type,
   \mintinline{python}/None/ is always accepted.

\item An input may be left unassigned, unless
  \begin{itemize}
  \item the option is typed to \mintinline{python}/RequiredOptions()/, or
  \item the option is typed to \mintinline{python}/OptionEnum()/ without a default.
  \end{itemize}
  So, except for the two cases above, it is not necessary to supply
  option values to a method at build time.

\item If typing is specified as a value, this is the default value if
  left unspecified.

\item If typing is specified as a class name, default is
  \mintinline{python}/None/.

\item Values are accepted if they are valid input to the type's
  constructor, i.e.\ 3 and '3' are valid input for an integer.

\item \mintinline{python}/None/ is always a valid input unless
  \begin{itemize}
  \item RequiredOptions() and not none\_ok set
  \item OptionEnum() and not none\_ok set
  \end{itemize}
  This means that for example something typed to \texttt{int} can be
  overridden by the builder by assigning it to
  \mintinline{python}/None/.  Also, \mintinline{python}/None/ is also
  accepted in typed containers, so a type defined as
  \mintinline{python}/[int]/ will accept the input
  \mintinline{python}/[1, 2, None]/.

\item All containers can be specified as empty, for example
  \mintinline{python}/{}/.  \comment{?}

\item Complex types (like \texttt{dict}s, \texttt{dict}s of
  \texttt{list}s of \texttt{dict}s, \dots) never enforce specific
  keys, only types.  For example, \mintinline{python}/{'a': 'b'}/
  defines a dictionary from strings to strings, and for example
  \mintinline{python}/{'foo': 'bar'}/ is a valid
  assignment.

\item Containers with a type in the values default to empty containers.
  Otherwise the specified values are the default contents.  Example
  \begin{python}
options = dict(
    x = dict,           # will be empty dict as default
    y = {'foo': 'bar'}  # will be {'foo': 'bar'} as default
)
  \end{python}
\end{enumerate}

The following sections will describe typing in more detail.

\subsubsection{Unspecifieds}
An option with no typing may be specified by assigning \texttt{None}.
\begin{python}
options = dict(length=None)  # accepts anything, default is None
\end{python}
Here, \texttt{length} could be set to anything.



\subsubsection*{Scalars}
Scalars are either explicitly typed, as
\begin{python}
options = dict(length=int)   # Requires an intable value or None
\end{python}
or implicitly with default value like
\begin{python}
options = dict(length=3)     # Requires an intable value or None,
                             # default is 3 if left unassigned
\end{python}
In these examples, intable means that the value provided should be
valid input to the \texttt{int} constructor, for example the number~3
or the string \texttt{'3'} both yield the integer number 3.



\subsubsection*{Strings}
A (possibly empty) string with default value \mintinline{python}{None} is typed as
\begin{python}
options = dict(name=str)     # requires string or None, defaults to None
\end{python}
A default value may be specified as follows
\begin{python}
options = dict(name='foo')   # requires string or None, provides default value
\end{python}
And a string required to be specified and none-empty as
\begin{python}
from extras import OptionString
options = dict(name=OptionString)       # requires non-empty string
\end{python}
In some situations, an example string is convenient
\begin{python}
from extras import OptionString
options = dict(name=OptionString('bar') # Requires non-empty string,
                                        # provides example (NOT default value)

\end{python}
Note that ``\texttt{bar}'' is not default, it just gives the
programmer a way to express what is expected.



\subsubsection*{Enums}
Enumerations are convenient in a number of situations.  An option with
three enumerations is typed as
\begin{python}
# Requires one of the strings 'a', 'b' or 'c'
from extras import OptionEnum
options = dict(foo=OptionEnum('a b c'))
\end{python}
and there is a flag to have it accept \mintinline{python}/None/ too
\begin{python}
# Requires one of the strings 'a', 'b', or 'c'; or None
from extras import OptionEnum
options = dict(foo=OptionEnum('a b c', none_ok=True))
\end{python}
A default value may be specified like this
\begin{python}
# Requires one of the strings 'a', 'b' or 'c', defaults to 'b'
from extras import OptionEnum
options = dict(foo=OptionEnum('a b c').b
\end{python}
(The \texttt{none\_ok} flag may be combined with a default value.)
Furthermore, the asterisk-wildcard could be used to accept a wide
range of strings
\begin{python}
# Requires one of the strings 'a', 'b', or any string starting with 'c'
options = dict(foo=OptionEnum('a b c*'))
\end{python}
The example above allows the strings ``\texttt{a}'', ``\texttt{b}'',
and all strings starting with the character ``\texttt{c}''.



\subsubsection*{Lists and Sets}
Lists are specified like this
\begin{python}
# Requires list of intable or None, defaults to empty list
options=dict(foo=[int])
\end{python}
Empty lists are accepted, as well as \mintinline{python}/None/.  In
addition, \mintinline{python}/None/ is also valid inside the list.
Sets are defined similarly
\begin{python}
# Requires set of intable or None, defaults to empty set
options=dict(foo={int})
\end{python}
Here too, both \mintinline{python}/None/ or the empty \texttt{set} is
accepted, and \mintinline{python}/None/ is a valid set member.


\subsubsection*{Date and Time}
The following date and time related types are supported:
\begin{itemize}
\item[] \texttt{datetime},
\item[] \texttt{date},
\item[] \texttt{time}, and
\item[] \texttt{timedelta}.
\end{itemize}
A typical use case is as follows
\begin{python}
# a datetime object if input, or None
from datetime import datetime
options = dict(ts=datetime)
\end{python}
and with a default assignment
\begin{python}
#  a datetime object if input, defaults to a datetime(2014, 1, 1) object
from datetime import datetime
options = dict(ts=datetime(2014, 1, 1))
\end{python}



\subsubsection*{More Complex Stuff:  Containing Types}
It is possible to have more complex types, such as dictionaries of
dictionaries and so on, for example
\begin{python}
# Requires dict of string to string
options = dict(foo={str: str})
\end{python}
or another example
\begin{python}
# Requires dict of string to dict of string to int
options = dict(foo={str: {str: int}})
\end{python}
As always, containers with a type in the values default to empty
containers.  Otherwise, the specified values are the default contents.



\subsubsection*{A File From Another Job:  \texttt{JobWithFile}}
Any file residing in a jobdir may be input to a method like this
\begin{python}
from extras import JobWithFile
options = dict(usefile=JobWithFile(jid, 'user.txt')
\end{python}
There are two additional arguments, \texttt{sliced} and
\texttt{extras}.  The \texttt{extras} argument is used to pass any
kind of information that is helpful when using the specified file, and
\texttt{sliced} tells that the file is stored in parallel slices.
\begin{python}
options = dict(usefile=JobWithFile(jid, 'user.txt', sliced=True, extras={'uid': 37}))
\end{python}
(Creating sliced files is described in section~\ref{xx}.)  In a
running method, the \texttt{JobWithFile} object has these members
\begin{python}
usefile.jobid
usefile.filename
usefile.sliced
usefile.extras
\end{python}
Where the full filename of the file is available through
\begin{python}
from extras import full_filename
print(full_filename(filename, ''))
\end{python}
The second option to \texttt{full\_filename} is mandatory, it may be
empty as in the example above, and should hold the filename extension.
\comment{Will be changed?!}
\comment{Example of how to use jobwithfile in analysis and prepare/synthesis.  how does slice work?  How does extras work?}



\section{Accessing Another Job's Parameters}

The previous sections show that the the \params data structure
contains all input parameters and initialization data for a job.
Sometimes it is useful to access another job's \params.  There is a
special function for that, called \texttt{job\_params}, and it is used
like this
\begin{python}
from extras import job_params

jobids = ('anotherjob',)

def synthesis():
    print(jobids.anotherjob)
    # will print something like 'jid-0_0'
    print(job_params(jobids.anotherjob).options)
    # will print the options of anotherjob, perhaps something like {length: 3}
\end{python}

\comment{if we ``objectify'' options, we could just do jobids.anotherjob.params}



\newpage
\section{Execution Flow:  \prepare, \analysis, and \synthesis}

There are three pre-defined functions in a method that are called by
the Accelerator at build time.  These are \prepare, \analysis, and
\synthesis, and they are always run in that order.  \prepare and
\synthesis executes as single processes, while \analysis provides
parallel execution.  None of them is mandatory, but at least one must
be present for the method to execute.

All the three functions take the \params variable as optional
argument.  The input parameters \options, \jobids, and \datasets are
global, so they do not need to be explicit in a function call.  The
\analysis function is special and takes a required argument
\texttt{sliceno}, which is an integer between zero and the total
number of slices minus one.  This is the unique identifier for each
\analysis process, and is described in the next section.


\subsection{Parallel Processing:  The \analysis function, Slices, and Datasets}
The number of analysis processes is always equal to the number of
dataset slices that the Accelerator has in its configuration file.
The idea is that each slice in a dataset should have exactly one
corresponding analysis process, so that all slices in a dataset can be
processed in parallel.


\subsection{Return Values}
Return values may be passed from one function to another.  What is
returned from prepare is called \prepareres, and may be used as input
argument to \analysis and \synthesis.  The return values from
\analysis is available as \analysisres in \synthesis.  The
\analysisres variable is an iterator, yielding the results from each
slice in turn.  Finally, the return value from \synthesis is stored
permanently in the job directory.  Here is an example of return value
passing
\begin{python}
# return a set of all users in the source dataset
options = dict(length=4)
datasets = (source',)

def prepare(options):
    return options.length * 2

def analysis(sliceno, prepare_res):
    return set(u for u in datasets.source.iterate(sliceno, 'user'))

def synthesis(analysis_res, prepare_res):
     return analyses_res.merge_auto()
\end{python}
\comment{a better example actually using result passing would be nice}
In the current implementation, all return values are stored as Python
\texttt{pickle} files.

Note that when a job completes, it is not possible to retrieve the
results from \prepare or \analysis anymore.  Only results from
\synthesis are kept.

\subsection{Merging Results from \analysis}
In the example above, each analysis process returns a \texttt{set}
that is constructed from a single slice.  In order to create a set of
all users in the \texttt{source} dataset, all these sets have to be
merged.  Merging could be done using a \texttt{for}-loop, but merging
is dependent of the actual type, and writing merging functions is
error prone.  Therefore, \analysisres has a function called
\texttt{merge\_auto()}, that is used for merging.  This function can
merge most data types, and even merge container variables in a
recursive fashion.  For example,
\begin{python}
h = defaultdict(lambda: defaultdict(set))
\end{python}
is straightforward to merge using \texttt{merge\_auto}.


\subsection{Share Data Between Jobs:  the \texttt{blob} Module}

The simplest way to share reasonable amounts of data between jobs is by using the
\texttt{blob} module.

\subsubsection*{Storing a  Single File}
Data is saved in this way
\begin{python}
import blob
def synthesis():
    data = ...  # some data created here
    blob.save(data, filename)
\end{python}
The data is loaded like this
\begin{python}
import blob
def synthesis()
    blob.load(filename)
\end{python}

\subsubsection*{Storing a Sliced File}
It is also possible to use the \texttt{blob} module in \analysis.
From a user's perspective it will look like a single file is being
handled, but there is actually one file per slice.  This is how to do
it
\begin{python}
def analysis(sliceno):
    # save data in slices like this
    blob.save(data, filename, sliceno=sliceno)
    # load like this
    data = blob.load(filename, sliceno=sliceno)
\end{python}

\subsubsection*{Save Files for Debugging}
There is one more argument, \texttt{temp}, which controls persistence
of the files.  It is by default set to \mintinline{python}/False/,
which implies that the stored file is not temporary.  But setting it
to \mintinline{python}/True/, like in the following
\begin{python}
    blob.save(data, filename, temp=True)
\end{python}
will cause the stored file to be deleted upon job completion.  The
argument takes two additional values, \texttt{DEBUG} and
\texttt{DEBUGTEMP}, working like this
\begin{itemize}
\item[] \texttt{DEBUG} -- file will be stored \emph{only} in debug
  mode
\item[] \texttt{DEBUGTEMP} -- file will always be stored, but
  \emph{removed} upon job completion only in debug mode.
\end{itemize}
Example
\begin{python}
from extras import Temp
def analysis(sliceno):
  # save only if --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUG)
  # save always, but remove unless --debug
  blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUGTEMP)
\end{python}


\newpage
\section{Subjobs}

Jobs may launch subjobs, i.e.\ methods may build other methods in a
recursive manner.  As always, if the jobs have been built already,
they will immediately be linked in.  The syntax for building a job
inside a method is as follows, assuming we build the jobs in \prepare
\begin{python}
import subjobs

def prepare():
  subjobs.build('count_items', options=dict(length=3))
\end{python}
It is possible to build subjobs in \prepare and \synthesis, but not in
\analysis.  \comment{right?}  The \texttt{subjobs.build} call uses the
same syntax as \texttt{urd.build} described in section~\ref{xx}, so
the input parameters \options, \datasets, \jobids, and
\texttt{caption} are available here too.  Similarly, the return value
from a subjob build is a jobid to the built job.

There are two catches, tough.
\begin{enumerate}
  \item
If there are datasets built in a subjob,
these will not be explicitly available to Urd.  A workaround is to
copy the dataset to the building method like this
\begin{python}
from Dataset import dataset

def synthesis():
    jid = subjobs.build('create_dataset')
    Dataset(jid).link_to_here(name='thename')
\end{python}
with the effect that the building job will act like a Dataset, even
though the dataset is actually created in the subjob.  The
\texttt{name} argument is optional, the name \texttt{default} is used
if left empty, corresponding to the default dataset.

\item
Currently there is no dependency checking on subjobs, so if a subjob
method is changed, the calling method will not be updated.  The
current remedy is to use \texttt{depend\_extra} in the building
method, like this
\begin{python}
import subjobs

depend_extra = ('a_childjob.py',)

def prepare():
  subjobs.build('childjob')
\end{python}
\end{enumerate}
