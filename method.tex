%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


This chapter covers most aspects of jobs and methods.  It describes
what methods are and how they are stored; when they are built and when
not; what input parameters look like in full detail; how to access
another job's parameters; parallel processing; return values and
result merging; storing and retrieving data; and how to build subjobs.



\section{Definitions}

\subsection{Methods and jobs}

In general, doing a computation on a computer follows the following
equation
\[
  \text{source code} \quad+\quad \text{input data and parameters}
  \quad+\quad \text{execution time} \quad\rightarrow\quad
  \text{result}
\]
In the Accelerator context, the equation becomes
\[
  \text{method} \quad+\quad \text{input data} \quad+\quad \text{input
    parameters} \quad+\quad \text{execution time}
  \quad\rightarrow\quad \text{job}
\]
where the \textbf{method} is the source code, and the \textbf{job} is
a directory containing
\begin{itemize}
\item[--] any number of output files created by the running method, as well as
\item[--] a number of job meta information files.
\end{itemize}
The exact contents of the job directory will be discussed in section~\ref{sec:job_directories}.

Computing a job is denoted job \textsl{building}.
Jobs are \textbf{built} from methods.  When a job has been built, it
is \textsl{static}, and cannot be altered or removed by the
Accelerator.  Jobs are built either by
\begin{itemize}
\item[--] a \textsl{build script}, see chapter~\ref{chap:urd}, or
\item[--] by a method, using \textsl{subjobs}.
\end{itemize}
subjobs will be the topic of section~\ref{sec:subjobs}.  The following
figure illustrates how a job ``\texttt{example-0}'' is built from the
method \texttt{a\_method.py}
The job identifier (in this case \texttt{example-0}) is always unique
so that it can be used as a reference to the particular job, and the
reasons for the actual name will be apparent later in this chapter.
\begin{figure}[h!]
  \begin{center}
    \input{figures/method.pdftex_t}
    \label{fig:method}
  \end{center}
  \caption{When a method is built, a job directory is created,
    containing files with all data an meta information regarding the
    job.}
\end{figure}



\subsection{Jobids}
A \textbf{jobid} is a reference to a job.  Since all job directories
have unique names, the name of the job is used as the jobid.  In the
example above, the job is uniquely identified by the
string \texttt{example-0}.



\section{Python Packages}
Methods are stored in standard Python packages, i.e.\ in a directory that is
\begin{itemize}
\item[--] reachable by the Python interpreter, and
\item[--] contains the (perhaps empty) file \texttt{\_\_init\_\_.py}.
\end{itemize}
In addition, for the Accelerator to accept a package, the following
constraints need to be satisfied
\begin{itemize}
\item[--] the package must contain a file named \texttt{methods.conf}, and
\item[--] the package must be added in the Accelerator's configuration file.
\end{itemize}
A package is reachable by The Accelerator if it is included in the
Accelerator's configuration file, see section~\ref{}.  The next
section guides through the steps required to create a new package.



\subsection{Creating a new Package}
The following shell commands illustrate how to create a new package
directory
\begin{shell}
% mkdir <dirname>
% touch <dirname>/__init__.py
% touch <dirname>/methods.conf
\end{shell}
The first two lines create a Python package, and the third line adds
the file \texttt{methods.conf}, which is required by the Accelerator.

For security reasons, the Accelerator does not look for packages by
itself.  Instead, packages to be used need to be explicitly specified
in the Accelerator's configuration file using the
\mintinline{shell}{method_directories} assignment.  Basically, the
assignment should be a comma separated list of packages, see
chapter~\ref{sec:configfile} for detailed information about the
configuration file.


\section{Method Source Files}
Method source files are stored in Python packages as described in the
previous section.  The Accelerator searches all packages for methods
to execute, and therefore \textsl{method names need to be globally
unique}!  In order to reduce risk of executing the wrong file, there
are three limitations that apply to methods:
\begin{enumerate}
\item For a method file to be accepted by the Accelerator, the
  filename has to start with the prefix ``\texttt{a\_}'';
\item the method name, without this prefix must be present on a
  separate line in the \texttt{methods.conf} file for the package, see
  section~\ref{sec:methods_conf}; and
\item the method name must be \emph{globally} unique, i.e.\ there can
  not be a method with the same name in any other method directory
  visible to the Accelerator.
\end{enumerate}


\subsection{Creating a New Method}
In order to create a new method, follow these steps
\begin{enumerate}
\item Create the method in a package viewable to the Accelerator
  using an editor.  Make sure the filename is \texttt{a\_<name>.py} if
  the method's name is \texttt{<name>}.
\item Add the method name \texttt{<name>} (without the prefix ``\texttt{a\_}'' and
  suffix ``\texttt{.py}'') to the \texttt{methods.conf} file in the
  same method directory.  See section~\ref{sec:methods_conf}.
\item (Make sure that the method directory is in the Accelerator's
  configuration file.)
\end{enumerate}
For example, \texttt{a\_example.py} is a valid method filename.


\subsection{Methods.conf}
\label{sec:methods_conf}
For a package to be valid, it must contain a \texttt{methods.conf}
file.  This file specifies which methods in the package directory that
are available for building.  It also optionally specifies which Python
interpreter each method should use.  Files not specified in
\texttt{methods.conf} cannot be executed.  The file
\texttt{methods.conf} provides an easy way to specify and limit which
source files that can be executed, which is something that makes a lot
of sense in any production environment.

The \texttt{methods.conf} is a plain text file with one entry per
line.  Any characters from a hash sign (``\texttt{\#}'') to the end of
the line is considered to be a comment.  It is permitted to have any
number of empty lines in the file.  Available methods are entered
first on a line by stating the name of the method, without the
\texttt{a\_} prefix and \texttt{.py} suffix.

The method name can optionally be followed by one or more whitespaces
and a name specifying the actual Python executable that will be used
to execute the method.  A list of valid Python executables is defined
in the configuration file, see section~\ref{sec:configfile}.  If this
field is left empty, the default executable is selected, which always
corresponds to the same executable as the currently runing Accelerator
daemon is using.  The Accelerator and its \texttt{standard\_methods}
library are compatible with both Python 2 and Python 3.

Here is an example \texttt{methods.conf}
\begin{shell}
# this is a comment

test2                # will use default Python
test3           py3  # py3 as specified in accelerator.conf
testx           tf   # the Tensorflow virtual environment Python
#bogusmethod    py3
\end{shell}
This file declares three methods corresponding to the files
\texttt{a\_test2.py}, \texttt{a\_test3.py}, and \texttt{a\_testx.py}.
These are the only methods that can be built in this package
directory.  Note that it is possible to define an executable from a
virtual environment.  This makes it straightforward to install any
package in a dedicated environment and making it accessible only to a
set of specified mehods.



\section{Job Already Built Check}

From chapter~\ref{chap:urd} it is known that a method is built using
the \texttt{.build()} function in a build script, like this
\begin{python}
def main(urd):
    urd.build('themethod', options=..., ...)
\end{python}

Prior to building a method, the Accelerator checks if an equivalent
job has been built in the past.  If it has, it will not be executed
again.  This check is based on two things:
\begin{enumerate}
\item  the output of a hash function applied to the method source code, and
\item  the method's input parameters.
\end{enumerate}
The hash value is combined with the input arguments and compared to
all jobs already built.  Only if the hash and input parameter
combination is unique will the method be executed.  The
\texttt{.build()}-function returns a reference object of type
\texttt{Job}.  It does not matter if the job is just built or if it
was build at an earlier time.



\section{Depend on More Files:  \texttt{depend\_extra}}

A method may import code located in other files, and these other files
can be included in the build check hash calculation as well.  This
will ensure that a change to an imported file will indeed force a
re-execution of the method if a build is requested.  Additional files
are specified in the method using the \texttt{depend\_extra} list, as
for example:
\begin{python}
from . import my_python_module

depend_extra = (my_python_module, 'mystuff.data',)
\end{python}
As seen in the example, it is possible to specify either Python module
objects or filenames relative to the method's location.

The Accelerator daemon will suggest adding modules to a source file in
the output log like this:
\begin{shell}
=================================================================
WARNING: dev.a_test should probably depend_extra on myfuncs
=================================================================
\end{shell}
The point of this is to make the user aware that the method depends on
additional files that are currently not taken into account in the
build check hashing.



\section{Avoiding Rebuild: \texttt{equivalent\_hashes}}
\label{sec:equivalent_hashes}

A change to a method's source code will cause a new job to be built
upon running \mintinline{python}{.build()}, but sometimes it is
desirable to modify the source code \textsl{without} causing a
re-build.  This happens, for example, when new functionality is added
to an existing method, and re-computing all jobs is considered to be
too time consuming.  The way the existing code behaves has not
changed, so existing jobs do not need to be re-built.  For this
situation, there is an \texttt{equivalent\_hashes} dictionary that can
be used to specify which versions of the source code that are
equivalent.  The Accelerator helps creating this, if needed.  This is
how it works.
\begin{enumerate}
\item Find the hash \texttt{<old\_hash>} of the existing job in that
  job's \texttt{setup.json}.
\item Add the following line to the method's source code
\begin{python}
equivalent_hashes = {'whatever': (<old_hash>,)}
\end{python}
\item Run the build script.  The daemon will print something like
\begin{shell}
===========================================================
WARNING: test_methods.a_test_rechain has equivalent_hashes,
but missing verifier <current_hash>
===========================================================
\end{shell}
\item Enter the \texttt{current\_hash} into the
  \texttt{equivalent\_hashes}:
\begin{python}
equivalent_hashes = {<current_hash>: (<old_hash>,)}
\end{python}
\end{enumerate}
This line now tells that \texttt{current\_hash} is equivalent to
\texttt{old\_hash}, so if a job with the old hash exists, the method
will not be built again.  Note that the right part of the assignment
is actually a list, so there could be any number of equivalent
versions of the source code.  This has been used frequently during
development of the Accelerator's \texttt{standard\_methods}, when new
features have been added that does not interfere with existing use.



\section{Method Execution}

Methods are not executed from top to bottom.  Instead, there are three
functions that are called by the method dispatcher that controls the
execution flow.  These functions are
\begin{itemize}
\item [] \prepare,
\item [] \analysis, and
\item [] \synthesis.
\end{itemize}
These functions, their execution order, and how to pass data between
them, is the topic of this section.


\subsection{Execution Flow}
The three functions \prepare, \analysis, and \synthesis are called one
at a time in that order.  \prepare and \synthesis executes as single
processes, while \analysis provides parallel execution.  None of them
is mandatory, but at least one must be present for the method to
execute.  It is discouraged to use \prepare only.


\subsection{Function Arguments}
There are two constants that can be passed into the executing
functions \prepare, \analysis, and \synthesis at run time.
\begin{itemize}
\item[--] \texttt{job}, which is a job instance, and
\item[--] \texttt{sliceno}, which provides a unique number to each
  parallel \analysis-process.
\end{itemize}

The \texttt{job} instance contains all information about the running
job.  The object is of type \texttt{CurrentJob}, which is an extension
of the \texttt{Job} class used for job instances that are not in the
execution stage.  This will be discusse in detail in section~\ref{}.

The \analysis function (and only the \analysis function) takes the
optional argument \texttt{sliceno}, which is an integer between zero
and the total number of slices minus one.  This is the unique
identifier for each \analysis process, and it is discussed further in
section~\ref{}.

Note that the method input parameters \options, \jobids, and \datasets
are global, so they do not need to be explicit in these function
calls.


\subsection{Parallel Processing:  The \analysis function, Slices, and Datasets}
The number of parallel analysis processes is set by the
\texttt{slices} parameter in the Accelerator's configuration file.
The idea is that when the Accelerator is processing dataset, each
dataset slice should have exactly one corresponding \analysis process,
so that all the slices in a dataset can be processed in parallel.  The
input parameter \texttt{sliceno} to the \analysis function is in the
range from zero to the number of slices minus one.


\subsection{Return Values}
Return values may be passed from one function to another.  What is
returned from prepare is called \prepareres, and may be used as input
argument to \analysis and \synthesis.  The return values from
\analysis is available as \analysisres in \synthesis.  The
\analysisres variable is an iterator, yielding the results from each
slice in turn.  Finally, the return value from \synthesis is stored
permanently in the job directory using the name
``\texttt{result.pickle}''.  Here is an example of return value passing
\begin{python}
# return a set of all users in the source dataset
options = dict(length=4)
datasets = ('source',)

def prepare(options):
    return options.length * 2

def analysis(sliceno, prepare_res)
    return prepare_res + sliceno

def synthesis(analysis_res, prepare_res):
     return sum(analysis_res) + prepare_res
\end{python}
In the current implementation, all return values are stored as Python
\texttt{pickle} files.  Note that when a job completes, it is not
possible to retrieve the results from \prepare or \analysis anymore.
Only results from \synthesis are kept.  Creating permanent files is
the topic of section~\ref{}.


\subsection{Merging Results from \analysis}
Consider this example
\begin{python}
datasets = ('source',)
  
def analysis(sliceno):
    return(datasets.source.iterate(sliceno, 'user'))

def prepare(analysis_res):
    return analysis_res.merge_auto()
\end{python}
Here, each \analysis process creates a set of users seen in that
\textsl{slice} of the \texttt{source} datasets.  In order to create a
set of all users in dataset, all these sets have to be merged.
Merging could be done using for example a \texttt{for}-loop, but the
merging operation is dependent of the actual type, and writing merging
functions is error prone.  Therefore, \analysisres has a function
called \texttt{merge\_auto()}, that is recommended for merging.  This
function can merge most data types, and even merge container variables
in a recursive fashion.  For example,
\begin{python}
h = defaultdict(lambda: defaultdict(set))
\end{python}
is straightforward to merge using \texttt{merge\_auto}.



\subsection{Accessing Return Values}
Values returned by a job can be retrieved in a build script or another
method using the \texttt{.load()} function from the \texttt{Job}
class, like in the following build script example:
\begin{python}
def main(urd):
    job = urd.build('example')
    x = job.load()
\end{python}
The \texttt{Job} class is designed to be a user friendly encapsulation
of the job directory and its files and meta data.



\section{Standard Out and Standard Error}
\label{sec:OUTPUT}

In a method, anything that is sent to \texttt{stdout} or
\texttt{stderr} will be sent \textsl{both} to the termininal in which
the Accelerator daemon was started \textsl{and} to a file in the job
directory.  This covers, for example, anything output from Python's
\texttt{print()}-function.

Output is collected in the job directory in a subdirectory named
\texttt{OUTPUT}.  This directory is created \textsl{only} if anything
was output from the job to \texttt{stdout} or \texttt{stderr},
otherwise it does not exist.  Inside the directory there may be files
like this
\begin{verbatim}
job-x/
   OUTPUT/
      prepare    # created if output in prepare()
      synthesis  #                      synthesis()
      0          #                      analysis() slice 0
      3          #                                       3
\end{verbatim}
No empty files will be created.



\section{Method Input Parameters}
\label{sec:input_params}

There are three kinds of method input parameters assign by
the \texttt{build} call: \jobids, \datasets, and \options.  These
parameters are stated early in the method source code, such as for
example
\begin{python}
jobids = ('accumulated_costs',)
datasets = ('transaction_log', 'access_log',)
options = dict(length=4)
\end{python}
The input parameters are populated by the builder when the
\texttt{run} command is executed, see~\ref{chap:urd}.

The \texttt{jobids} parameter list is used to input references to
other jobs, while the \datasets parameter list is used to input
references to datasets.  These parameters must be populated by the
build call.

The \options dictionary, on the other hand, is used to input any other
type of parameters to be used by the method at run time.  Options does
not necessarily be populated by the build call, and this can be used
for ``global constants'' in the method.  An option assigned by the
build call will override the default assignment.

Note that \jobids and \datasets are \texttt{tuple}s (or \texttt{list}s
or \texttt{set}s), and a single entry has to be followed by a comma as
in the example above, while \options is a dictionary.  Individual
elements of the input parameters may be accessed inside the method
using dot notation like this
\begin{python}
jobids.accumulated_cost
datasets.transaction_log
options.length
\end{python}
Each of these parameters will be described in more detail in following
sections.


\subsection*{Input Jobids}
The \jobids parameter is a tuple of job references linking other jobs
to this job.  In a running method, each item in the \jobids tuple is
of type \texttt{Job} that is used as a reference to the corresponding
job.  All items in the \jobids tuple must be assigned by the builder
to avoid run time errors.

It is possible to specify lists of jobids, see this example
\begin{python}
jobids = ('source', ['alistofjobs'],)
\end{python}
where \texttt{source} is a single job reference, whereas
\texttt{alistofjobs} is a list of job references.


\subsection*{Input Datasets}
The \datasets parameter is a tuple of links to datasets.  In a running
method, each item in the \datasets variable is of type
\texttt{Dataset}.  The \texttt{Dataset} class is described in a
dedicated chapter~\ref{chap:datasets}.  All items in the \datasets
tuple must be assigned by the builder to avoid run time errors.

It is possible to specify lists of datasets, see this example
\begin{python}
datasets = ('source', ['alistofdatasets'],)
\end{python}
where \texttt{source} is a single dataset, whereas
\texttt{alistofdatasets} is a list of datasets.




\subsection*{Input Options}

The \options parameter is of type \texttt{dict} and is used to pass
various information from the builder to a job.  This information could
be integers, strings, enumerations, sets, lists, and dictionaries in a
recursive fashion, with or without default values.  Assigning options
from the build call is not necessary, but an assignment will override
the ``default'' that is specified in the method.  Options are
specified like this
\begin{python}
  options = dict(key=value, ... )  # or
  options = {key: value, ...}
\end{python}

Options are straightforward to use and quite flexible.  A formal
overview is presented in section~\ref{sec:formal_options}.



\section{The User Friendly \texttt{Job} and \texttt{Currentjob} Classes}
All data and metadata associated with a job is stored in the job's job
directory.  In order to simplify access to a job's data, common job
operations have been factored into the \texttt{Job} class.  There is
also an extended version of this, called the \texttt{CurrentJob}
class, that also contains information and helper functions to a
running job.  These classes are the topics of section~\ref{}
and~\ref{}.

Examples of how the \texttt{Job} class can be used has been shown
earlier in this chapter.  For example, the result resturned by a
method could be loaded in a build script or another method like this
\begin{python}
def main(urd):
    job = urd.build('example')
    x = job.load()
\end{python}
Inside a method, an object of type \texttt{CurrentJob} is available as
input parameter to any of the \prepare, \analysis, and \synthesis
functions.  An example use is storing files, like in the following example
\begin{python}
def synthesis(job):
    data = 'xxxx'
    job.save(data, 'mydata')
\end{python}
This example will store \texttt{data} in a file \texttt{mydata.pickle}
in the current job directory.



\subsection{Easy Access to All Job Parameters}
\label{sec:params}
There are two sources of parameters to a running method,
\begin{itemize}
\item [] parameters from the caller, i.e.\ the \texttt{.build()}-call,
  and
\item [] parameters assigned by the Accelerator when the job starts
  building.
\end{itemize}
All these parameters are available to the running method by the
\texttt{job.params()} function, wher \texttt{job} is the input
parameter introduced in the previous section.  Note that the
\texttt{.params()} function exists both on the \texttt{Job} and the
\texttt{CurrentJob} class, meaning that parameters can be extracted
both from existing and currently running jobs.

Consider the following example method that pretty-prints the
\texttt{job.params()} variable.
\begin{python}
import json

jobids = ('anotherjob',)

def synthesis(params, job):
    # first, print parameters belonging to the input job
    print(json.dumps(jobids.anotherjob.params(), indent=4))
    # then, print parameters belonging to THIS job
    print(json.dumps(job.params(), indent=4))
\end{python}


@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@222 move part below
The corresponding output will look something like this
%\begin{leftbar}
\begin{json}
{
    "starttime": 1572082453.0400746,
    "caption": "fsm_example_method",
    "hash": "eea8591fefc636eb6d13df22118e4d9ff303163e",
    "jobid": "test-20",
    "method": "example_method",
    "package": "dev",
    "python": "/home/ab/accvenv/bin/python",
    "seed": 411200712313859665,
    "slices": 23,
    "version": 1,
    "options": {
        "mode": "testing",
        "length": 3
    },
    "datasets": {
        "source": "EXAMPLE-3",
        "parent": "EXAMPLE-2"
    },
    "jobids": {
        "jid": "EXAMPLE-0"
    },
}
\end{json}
%\end{leftbar}
\noindent and a description of its keys
\starttabletwo
\texttt{package} & Python package for this method\\
\texttt{method} & name of this method\\
\texttt{jobid} & jobid of this job\\

\texttt{starttime}& start time in epoch format\\
\texttt{caption} & a caption\\
\texttt{slices} & number of slices of current Accelerator configuration \\
\texttt{seed} & a random seed available for use$^1$\\
\texttt{hash} & source code hash value\\

\texttt{options} & input parameter\\
\texttt{dataset} & input parameter\\
\texttt{jobids} &  input parameter\\
\hline\\

\multicolumn{2}{l}{\small{$^1$ The Accelerator team recommends \emph{not} using
    \texttt{seed}, unless non-determinism is actually a goal.}}\\
\stoptabletwo
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ move part above to detailed explanation of Job and CurrentJob .params()!


\subsection{Writing and Reading Files}
The \texttt{CurrentJob} class provides the recommended way to write
data and datasets to disk.  Files may be written using the
\texttt{job.save()} and \texttt{job.json\_save()} functions.  The
first writes a \texttt{pickle} file, and the latter uses \texttt{json}
encoding.  For example
\begin{python}
def synthesis(job):
   job.save('a string to be written', 'stringfile')
   job.json_save(dict(key='value'), 'jsonfile')
\end{python}
There are corresponding \texttt{job.load()} and
\texttt{job.json\_load()} functions that can be called \textsl{both}
in methods \textsl{and} build scripts.  For example
\begin{python}
jobids = ('anotherjob',)

def synthesis():
    jobids.anotherjob.load('stringfile')
\end{python}
will load a file from another job into the currently running method, while
\begin{python}
  def main(urd):
      job.urd.build('example')
      x = job.load('thefile')
\end{python}
will load data stored by the \texttt{example} method using the
filename \texttt{thefile.pickle} into the build script.


\subsection{Accessing Other Job's Datasets}
Using the \texttt{Job} class, it is straightforward to access datasets
in other jobs.  For example
\begin{python}
def main(urd):
    job = urd.build(...)

    print(job.datasets())@@@@@@@@@@@@@@@@@@@@@@@@@@2

    print(job.dataset('training')) @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@2
\end{python}
This works both in running methods and in build scripts.



\section{Job Directories}
\label{sec:job_directories}

A successfully build of a method results in a new job directory on
disk.  The job directory will be stored in the current workdir and
have a structure as follows, assuming the current workdir
is \texttt{test}, and the current jobid is \texttt{test-0}.
\begin{verbatim}
workdirs/test/
    test-0/
        setup.json
        method.tar.gz
        result.pickle
        post.json
\end{verbatim}
The \texttt{setup.json} will contain information for the job,
including name of method, input parameters, and, after execution, some
profiling information.  \texttt{post.json} contains profiling
information, and is written only of the job builds successfully.  All
source files, i.e.\ the method's source and \texttt{depend\_extra}s
are stored in the \texttt{tar}-archive \texttt{method.tar.gz}.
Finally, the return value from \texttt{synthesis} is stored as a
Python pickle file with the name \texttt{result.json}.

If the job contains datasets, these will be stored in directories,
such as for example \texttt{default/}, in the root of the job
directory.

Note that the \texttt{Job} and \texttt{CurrentJob} classes encapsulate
the job directory and provides helper functions to work on the job
data.  See section~\ref{} and~\ref{}.



@@@@@@@@@@@@@@@@@@@@@@@@@@@2 here now


\section{Subjobs}
\label{sec:subjobs}
\index{subjobs}

Jobs may launch subjobs, i.e.\ methods may build other methods in a
recursive manner.  As always, if the jobs have been built already,
they will immediately be linked in.  The syntax for building a job
inside a method is as follows, assuming we build the jobs in \prepare
\begin{python}
from accelerator import subjobs

def prepare():
    subjobs.build('count_items', options=dict(length=3))
\end{python}
It is possible to build subjobs in \prepare and \synthesis, but not in
\analysis.    The \texttt{subjobs.build} call uses the
same syntax as \texttt{urd.build} described in chapter~\ref{chap:urd}, so
the input parameters \options, \datasets, \jobids, and
\texttt{caption} are available here too.  Similarly, the return value
from a subjob build is a jobid to the built job.

There are two catches, though.
\begin{enumerate}
  \item
Dataset ids to datasets created in subjobs will not be explicitly
available to the build script.  The workaround is to link the dataset
to the building method like this \index{\texttt{link\_to\_here}}
\begin{python}
from accelerator.dataset import Dataset

def synthesis():
    jid = subjobs.build('create_dataset')
    Dataset(jid).link_to_here(name='thename')
\end{python}
with the effect that the building job will act like a Dataset, even
though the dataset is actually created in the subjob.  The
\texttt{name} argument is optional, the name \texttt{default} is used
if left empty, corresponding to the default dataset.  It is possible
to override the dataset's previous using
the \texttt{override\_previous} option, which takes a jobid
(or \pyNone) to be the new \texttt{previous}.
\index{\texttt{override\_previous}}
\begin{python}
Dataset(jid).link_to_here(name='thename', override_previous=xxx)
\end{python}
The \texttt{link\_to\_here} call returns a dataset object.

\item
Currently there is no dependency checking on subjobs, so if a subjob
method is changed, the calling method will not be updated.  The
current remedy is to use \texttt{depend\_extra} in the building
method, like this
\begin{python}
from accelerator import subjobs

depend_extra = ('a_childjob.py',)

def prepare():
    subjobs.build('childjob')
\end{python}
\end{enumerate}
There is a limit to the recursion depth of subjobs, to avoid creating
unlimited number of jobs by accident.




\section{Formal Option Rules}
\label{sec:formal_options}
This section covers the formal rules for the \options parameter.
\begin{enumerate}

\item Typing may be specified using the class name (i.e.\ \texttt{int}), or as
  a value that will construct into such a class object (i.e.\ the
  number 3).  See this example
  \begin{python}
options = dict(
    a = 3,     # typed to int
    b = int,   #          int
    c = 3.14,  #          float
    d = '',    #          str
)
  \end{python}
  Values will be default values, and this is described thoroughly in
  the other rules.

 \item An input option value is required to be of the correct type.
   This is, if a type is specified for an option, this must be
   respected by the builder.  Regardless of type,
   \mintinline{python}/None/ is always accepted.

\item An input may be left unassigned, unless
  \begin{itemize}
  \item the option is typed to \mintinline{python}/RequiredOptions()/, or
  \item the option is typed to \mintinline{python}/OptionEnum()/ without a default.
  \end{itemize}
  So, except for the two cases above, it is not necessary to supply
  option values to a method at build time.

\item If typing is specified as a value, this is the default value if
  left unspecified.

\item If typing is specified as a class name, default is
  \mintinline{python}/None/.

\item Values are accepted if they are valid input to the type's
  constructor, i.e.\ 3 and '3' are valid input for an integer.

\item \mintinline{python}/None/ is always a valid input unless
  \begin{itemize}
  \item RequiredOptions() and not none\_ok set
    \index{input options!\texttt{RequiredOptions}}
  \item OptionEnum() and not none\_ok set
    \index{input options!\texttt{OptionEnum}}
  \end{itemize}
  This means that for example something typed to \texttt{int} can be
  overridden by the builder by assigning it to
  \mintinline{python}/None/.  Also, \mintinline{python}/None/ is also
  accepted in typed containers, so a type defined as
  \mintinline{python}/[int]/ will accept the input
  \mintinline{python}/[1, 2, None]/.

\item All containers can be specified as empty, for example
  \mintinline{python}/{}/ which expects a \texttt{dict}.

\item Complex types (like \texttt{dict}s, \texttt{dict}s of
  \texttt{list}s of \texttt{dict}s, \dots) never enforce specific
  keys, only types.  For example, \mintinline{python}/{'a': 'b'}/
  defines a dictionary from strings to strings, and for example
  \mintinline{python}/{'foo': 'bar'}/ is a valid
  assignment.

\item Containers with a type in the values default to empty containers.
  Otherwise the specified values are the default contents.  Example
  \begin{python}
options = dict(
    x = dict,           # will be empty dict as default
    y = {'foo': 'bar'}  # will be {'foo': 'bar'} as default
)
  \end{python}
\end{enumerate}

The following sections will describe typing in more detail.



\subsection{Options with no Type}
An option with no typing may be specified by assigning \texttt{None}.
\begin{python}
options = dict(length=None)  # accepts anything, default is None
\end{python}
Here, \texttt{length} could be set to anything.



\subsection{Scalar Options}
Scalars are either explicitly typed, as
\begin{python}
options = dict(length=int)   # Requires an intable value or None
\end{python}
or implicitly with default value like
\begin{python}
options = dict(length=3)     # Requires an intable value or None,
                             # default is 3 if left unassigned
\end{python}
In these examples, intable means that the value provided should be
valid input to the \texttt{int} constructor, for example the number~3
or the string \texttt{'3'} both yield the integer number 3.



\subsection{String Options}
A (possibly empty) string with default value \mintinline{python}{None} is typed as
\begin{python}
options = dict(name=str)     # requires string or None, defaults to None
\end{python}
A default value may be specified as follows
\begin{python}
options = dict(name='foo')   # requires string or None, provides default value
\end{python}
And a string required to be specified and none-empty as
\index{input options!\texttt{OptionString}}
\begin{python}
from accelerator.extras import OptionString
options = dict(name=OptionString)       # requires non-empty string
\end{python}
In some situations, an example string is convenient
\begin{python}
from accelerator.extras import OptionString
options = dict(name=OptionString('bar') # Requires non-empty string,
                                        # provides example (NOT default value)
\end{python}
Note that ``\texttt{bar}'' is not default, it just gives the
programmer a way to express what is expected.



\subsection{Enumerated Options}
Enumerations are convenient in a number of situations.  An option with
three enumerations is typed as
\begin{python}
# Requires one of the strings 'a', 'b' or 'c'
from accelerator.extras import OptionEnum
options = dict(foo=OptionEnum('a b c'))
\end{python}
and there is a flag to have it accept \mintinline{python}/None/ too
\begin{python}
# Requires one of the strings 'a', 'b', or 'c'; or None
from accelerator.extras import OptionEnum
options = dict(foo=OptionEnum('a b c', none_ok=True))
\end{python}
A default value may be specified like this
\begin{python}
# Requires one of the strings 'a', 'b' or 'c', defaults to 'b'
from accelerator.extras import OptionEnum
options = dict(foo=OptionEnum('a b c').b)
\end{python}
(The \texttt{none\_ok} flag may be combined with a default value.)
Furthermore, the asterisk-wildcard could be used to accept a wide
range of strings
\begin{python}
# Requires one of the strings 'a', 'b', or any string starting with 'c'
options = dict(foo=OptionEnum('a b c*'))
\end{python}
The example above allows the strings ``\texttt{a}'', ``\texttt{b}'',
and all strings starting with the character ``\texttt{c}''.



\subsection{List and Set Options}
Lists are specified like this
\begin{python}
# Requires list of intable or None, defaults to empty list
options=dict(foo=[int])
\end{python}
Empty lists are accepted, as well as \mintinline{python}/None/.  In
addition, \mintinline{python}/None/ is also valid inside the list.
Sets are defined similarly
\begin{python}
# Requires set of intable or None, defaults to empty set
options=dict(foo={int})
\end{python}
Here too, both \mintinline{python}/None/ or the empty \texttt{set} is
accepted, and \mintinline{python}/None/ is a valid set member.



\subsection{Date and Time Options}
The following date and time related types are supported:
\begin{itemize}
\item[] \texttt{datetime},
\item[] \texttt{date},
\item[] \texttt{time}, and
\item[] \texttt{timedelta}.
\end{itemize}
A typical use case is as follows
\begin{python}
# a datetime object if input, or None
from datetime import datetime
options = dict(ts=datetime)
\end{python}
and with a default assignment
\begin{python}
#  a datetime object if input, defaults to a datetime(2014, 1, 1) object
from datetime import datetime
options = dict(ts=datetime(2014, 1, 1))
\end{python}



\subsection{More Complex Stuff:  Types Containing Types}
It is possible to have more complex types, such as dictionaries of
dictionaries and so on, for example
\begin{python}
# Requires dict of string to string
options = dict(foo={str: str})
\end{python}
or another example
\begin{python}
# Requires dict of string to dict of string to int
options = dict(foo={str: {str: int}})
\end{python}
As always, containers with a type in the values default to empty
containers.  Otherwise, the specified values are the default contents.



\subsection{A File From Another Job:  \texttt{JobWithFile}}
\label{sec:jobwithfile}

\index{input options!\texttt{JobWithFile}}
Any file residing in a jobdir may be input to a method like this
\begin{python}
from accelerator.extras import JobWithFile
options = dict(usefile=JobWithFile(jid, 'user.txt')
\end{python}
There are two additional arguments, \texttt{sliced} and
\texttt{extras}.  The \texttt{extras} argument is used to pass any
kind of information that is helpful when using the specified file, and
\texttt{sliced} tells that the file is stored in parallel slices.
\begin{python}
options = dict(usefile=JobWithFile(jid, 'user.txt', sliced=True, extras={'uid': 37}))
\end{python}
(Creating sliced files is described in section~\ref{sec:create_dataset_in_analysis}.)  In a
running method, the \texttt{JobWithFile} object has these members
\begin{python}
usefile.jobid
usefile.filename
usefile.sliced
usefile.extras
\end{python}
%% Where the full filename of the file is available through
%% \begin{python}
%% from accelerator.extras import full_filename
%% print(full_filename(filename, ''))
%% \end{python}
%% The second option to \texttt{full\_filename} is mandatory, it may be
%% empty as in the example above, and should hold the filename extension.
%% \comment{Will be changed?!}
%% \comment{Example of how to use jobwithfile in analysis and prepare/synthesis.  how does slice work?  How does extras work?}





\section{Jobs - a Summary}
The concepts relating to Accelerator jobs are fundamental, and this
section provides a shorter summary about the basic concepts.

\begin{itemize}
\item[1.]  Data and metadata relating to a job is stored in a
job directory.
\item[2.]  Jobids are pointers to such job directories.
\end{itemize}
The files stored in the job directory at dispatch are complete in the
sense that they contain all information required to run the job.  So
the Accelerator job dispatcher actually just creates processes and
points them to the job directory.  New processes have to go and figure
out their purpose by themselves by looking in this directory.

A running job has the process' \textsl{current working directory
(CWD)} pointing into the job directory, so any files created by the
job (including return values) will by default be stored in the job's
directory.

When a job completes, the meta data files are updated with profiling
information, such as execution time spent in single and parallel
processing modes.

All code that is directly related to the job is also stored in the job
directory in a compressed archive.  This archive is typically limited
to the method's source, but the code may have manually added
dependencies to any other files, and in that case these will be added
too.  This way, source code and results are always connected and
conveniently stored in the same directory for future reference.
\begin{itemize}
\item[3.]  Unique jobs are only executed once.
\end{itemize}
Among the meta information stored in the job directory is a hash
digest of the method's source code (including manually added
dependencies).  This hash, together with the input parameters, is used
to figure out if a result could be re-used instead of re-computed.
This brings a number of attractive advantages.
\begin{itemize}
\item[4.]  Jobs may link to eachother using jobids.
\end{itemize}
Which means that jobs may share results and parameters with eachother.
\begin{itemize}
\item[5.]  Jobs are stored in workdirs.
\item[6.]  There may be any number of workdirs.
\end{itemize}
This adds a layer of ``physical separation''.  All jobs relating to
importing a set of data may be stored in one workdir, perhaps named
\texttt{import}, and development work may be stored in a workdir
\texttt{dev}, etc.  Jobids are created by appending a counter to the
workdir name, so a job \texttt{dev-42} may access data in
\texttt{import-37}, and so on, which helps manual inspection.
\begin{itemize}
\item[7.] Jobs may dispatch other jobs.
\end{itemize}
It is perfectly fine for a job to dispatch any number of new jobs, and
these jobs are called \textsl{subjobs}.  A maximum allowed recursion
depth is defined to avoid infinite recursion.



