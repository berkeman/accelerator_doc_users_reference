
\section{Introduction}

The Accelerator platform is a complete environment designed for high
performance parallel computing on large sets of data.  On a high
level, the platform is composed of two parts: a server providing a set
of features for efficient parallel processing of large amounts of
data, and a client that dispatches jobs on the server and provides a
lookup of all data in a project.

A user of the platform may use or write two kinds of programs, denoted
\textsl{methods} and \textsl{automatas}.  Methods are parallel
programs executed on the server, and automatas are high level scripts
used to control flow and parameters to and from the methods running on
the server.  Programs are written in the Python programming
language\footnote{ Both Python~2 and Python~3 are supported.}.  In
theory, any language may be used, and critical parts of the internals
of the Accelerator are written in the C~programming language.

Methods that are running or have completed running are called jobs.
The server stores results and parameters from all jobs in a database
that is partitioned into what is called workdirs.  The client, on the
other hand, keeps a log, called the \textsl{Urd log}, which is
bookkeeping all dispatched jobs and associated parameters and
dependencies.  Interestingly, data is also stored in jobs, so
retrieving data is no different from retrieving jobs.

Here is a very simple example
\\
\begin{python}
# a_helloworld.py:
def analysis(sliceno):
  print("Hello, world: %d" % (sliceno,))

# automata.py:
def main(urd):
  urd.begin('test')
  urd.build('helloworld')
\end{python}
\\
The program is executed by calling
\\
\begin{shell}
% automatarunner.py
\end{shell}
\\
This example will print the string \texttt{Hello, world:} plus a digit
for each slice a number of
times to standard
out.  In more detail, the automata is dispatching the



\section{Dispatch, Dependencies, and Jobids}

The framework is basically a job dispatcher and dependency tracker.
Input to a job is parameters and source code, called method.  When a
job is run, a directory is created in the workdir.  A pointer to this
directory, called a jobid, is returned upon finished execution.
Inside the directory, everything needed to run the job is stored, for
example input parameters, as well as all results created during
execution.

The framework keeps information for each job that has been
successfully completed for dependency tracking.  Dependency tracking
is used for two things: first, it avoids re-execution of jobs that
have been run before; and second, it permits jobs to depend on already
finished jobs.

Re-execution is avoided if a job is issued with exactly the same
source code and input options.  If source code is not equal, or if any
input to the job is different, the job will be executed again.  All
successful runs of a job will be kept.

A job may have reference to other jobs as input.  If these references
are valid, the running job will have access to all information from
the reference jobs.



\section{Datasets and Chaining}

A particularly efficient way to store data is by using the dataset.
Datasets reside inside jobs, abstracting databases to jobs containing
data.  Data is stored so that it can be retrieved row by row at a very
high pace and requesting from one to any number of columns.

Data in a dataset is separated into slices, where on a single server,
the number of slices is close to the number of actual CPU cores for
best performance.

A dataset may be hashed on one of its columns.  Hashing means that
each slice contains a mutually exclusive set of column data.

Typically, a dataset is created for each file that is imported.  When
several files are imported, the jobids may be chained, that is, each
new jobid contains a pointer to the previous job.  Using the
previous-pointers, datasets could be iterated one after another
without interruption.



\section{Job Bookkeeping}

A prominent feature of both the client and the server is the ability
to immediately recall instead of recompute previously executed jobs.
Any job that has been successfully executed may be recalled, provided
that the job is submitted again with exactly the same input and source
code.  The framework uses a hash of the method's source code to decide
if it has changed or not.

The combination of bookkeeping and the client/server architecture
provides a very powerful environment for accessing and depending on
previous result.  Not only can a user's own results be recalled, but
also those from another user or from the ``production'' user in a live
environment.  In a typical application, this dramatically increases
performance while minimizing the probability of errors.  It also
allows for a high level of transparency.  Past jobs could be replayed
in a controlled manner, and the complete context with respect to input
arguments of any job could be retrieved and used to answer
``what-if''-scenarios.

% @@ Jag vill inte ha exempel, utan harda definitioner!



\section{Workdirs}

A workdir is where results are stored.  One can think of it as a
directory populated with jobid-directories.  Workdirs are specified
with absolute paths in the server filesystem.  A workdir has a fixed
slicing, and all workdirs in a project need to have the same number of
slices.


\section{Method}

Code is executed on the framework is written in methods.  Methods
allow for parallel execution and more.



\section{Urd}

Urd provides a scripting environment for running and recalling jobs.
It has a transaction-log database that stores information about jobs
that have been run and their inter-dependencies.  Since data is also
stored in jobs, another of Urd's purposes is to provide lookups to all
data in a project.



\section{Automata}

An automata is a script run by urd, in which jobs and data are
recalled, and new jobs may be dispatched.
