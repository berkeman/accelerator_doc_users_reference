%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2020 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{\texttt{csvimport} -- Importing Data Files}
The \texttt{csvimport} method is used to import a text files into a
dataset.  Data is imported one row per slice in a round robin fashion.
Input data is assumed to be in a tabular format, i.e.\ it is composed
of a number of rows, each having the same number of columns separated
by a separator token.  A common format of this type is the Comma
Separated Values (CSV) format, but \texttt{csvimport} is much more
flexible, as seen in the table of options below.  For example,
\texttt{csvimport} can handle any separator character, skip or parse
labels on the first line, and supports advanced quote support.  It
also deals with ``broken'' input data in a predicted and user
controllable way.  \texttt{csvimport} jobs can be chained, so any
number of input files can be connected in a dataset chain.



\subsection{Options}
\starttable

\RP \texttt{filename} & \emph{mandatory} & Name of file to import.
The filename is mandatory and the file may either be a \textsl{plain
  text} file or a \textsl{gzipped} file.  It is also possible to
specify a filename including a path.  If the path begins with a slash,
it is absolute.  Otherwise, the path is relative to the
``\texttt{input directory}'' configuration parameter specified in the
Accelerator's configuration file, see section~\ref{sec:configfile}.  A
relative path makes it possible to relocate files to a different
directory without trigging job remake.\\

\RP \texttt{separator} & \texttt{,} & Field separator character.
Accepts a single \texttt{iso-8859-1} character.  Leave this empty to
import each line of the input file into a single column.  \\

\RP \texttt{comment} & \texttt{''} & Lines beginning with this
character are ignored.  Accepts a single \texttt{iso-8859-1}
character, or the empty string for no comments. Commented lines are
stored in the \texttt{skipped} dataset.\\

\RP \texttt{newline} & \texttt{''} & Newline character.  Empty means
"\texttt{\textbackslash{}n}" or "\texttt{\textbackslash{}r\textbackslash{}n}".
Alternatively any single \texttt{iso-8859-1} character can be chosen.\\

\RP \texttt{quotes} & \texttt{''} & Quote character.  Empty or \pyFalse means no
quotes, \pyTrue means both \texttt{'}, and \texttt{"}, any other
character means itself. \\

\RP \texttt{labelsonfirstline} & \pyTrue & If set to
\mintinline{python}{True}, data on the first line of the file will
be used as column labels.  If \mintinline{python}{False}, labels must
be entered using the \texttt{label} option, see \texttt{labels}
below.\\

\RP \texttt{labels} & \texttt{[]} & If \texttt{labelsonfirstline} (see
above) is set to \mintinline{python}{False}, labels must be provided
using this option.  For example \mintinline{python}{labels = ['foo',
'bar',]}.\\

\RP \texttt{rename} & \texttt{\{\}} & This option makes it possible to
change the column names read from the first line of the input file.
Renaming happens first.  It accepts a dictionary of type
\mintinline{python}/{old_name: new_name,}/.\\

\RP \texttt{lineno\_label} & \texttt{''} & If set,
\texttt{lineno\_label} becomes a column containing line numbers.
Line numbers start at one (1), and corresponds to line numbers in the
input file.  \\

\RP \texttt{discard} & \mintinline{python}/set()/ & Labels in the
discard set will not be stored in the dataset.\\

\RP \texttt{allow\_bad} &\mintinline{python}{False} & By default, this is
set to \pyFalse and an error will be asserted if there are problems
parsing the input data, see section~\ref{sec:csvimport-bad}.  Setting it
to \pyTrue will put all ``bad'' lines together with the corresponding
line numbers into a separate dataset named \texttt{bad}.  It is
recommended to check the resulting datasets if enabling this
option!\\

\RP \texttt{skip\_lines} & 0 & Skip this many lines at the start of the file.
This is useful for data files that starts with a header, for
example.  Skipped lines will be stored in the \texttt{skipped} dataset.\\

\RP \texttt{compression} & 6 & Compression level for the \texttt{gzip} compressor.\\

\RP \texttt{allow\_extra\_empty} & \pyFalse & Still consider a line good if it has extra empty fields at the end.\\

\RP \texttt{skip\_empty\_lines} & \pyFalse & Ignore empty lines.\\

\RP \texttt{strip\_labels} & \pyFalse & Do .strip() on all labels (happens before rename).\\

\stoptable


\subsection{Datasets}
\starttable
  \RP \texttt{previous} & \mintinline{python}{None} & Previous dataset if creating a
  chain.\\
\stoptable


\subsection{Bad Lines}
\label{sec:csvimport-bad}
A line is flagged as ``bad'' for one of two reasons
\begin{itemize}
\item[] -- there is a problem with quoting, or
\item[] -- there is an incorrect number of separators.
\end{itemize}
for example
\begin{python}
  "a","b" "c"     # invalid assuming two or three comma separated columns
  "a","b"" ""c"   # valid assuming two comma separated columns
\end{python}


\subsection{Output}
The result of the \texttt{csvimport} is a dataset named
\texttt{default}.  Lines marked as ``bad'' will be stored in the
dataset \texttt{bad}, while skipped and commented lines will be stored
in the dataset \texttt{skipped}.  All columns will be of type
\mintinline{python}{bytes}.  Typically, the dataset from
\texttt{csvimport} is fed to a \texttt{dataset\_type} job for column
typing.


\subsection{Line Numbers}
A column with line numbers is always attached to the \texttt{bad}
and \texttt{skipped} datasets, and conditionally
using \texttt{lineno\_label} to the main dataset.  Line numbers start
at one (1), and always corresponds to the lines in the input dataset.
For example, if there are labels on the first line of the input file,
this line is number 1.  Any line number can thus only appear in one of
the main, \texttt{bad}, or \texttt{skipped} datasets.


\subsection{Limitations}
Each data value is limited to 16MB maximim.  However, this is just a
constant in the code that is by default set to a value that allows the
Accelerator to run on low memory platforms.  If you need to store,
say, \texttt{bytes} values larger than 16MB, please update this
constant to a larger value.


\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build(csvimport',
    options=dict(
        filename='inputfile.txt',
        separator='\0',
    )
)
\end{python}
this will import the file \texttt{inputfile.txt} assuming that there
are labels on the first line and the column separator is a null
character (\texttt{0x00}, \mintinline{python}|'\0'|).



\clearpage
\section{\texttt{csvimport\_zip} -- Importing \texttt{zip} Archives}
The \texttt{csvimport\_zip} method is a wrapper
around \texttt{csvimport} that is used to import files stored in
\texttt{zip} archives.  One or more files in a \texttt{zip} archive
can be imported by a call to this function, and each file will be
imported to a separate dataset.


\subsection{Options}

All options to \texttt{csvimport} are available to this method as
well, and the \texttt{filename} option is used to specify the name of
the \texttt{zip} file.

\starttable
\RP \texttt{inside\_filenames} & \texttt{\{\}}&
   Dictionary from filename in zipfile to dataset
   name, \mintinline{python}/{'filename in zip': 'dataset name',
   ...}/.  If left empty, all files will be imported to datasets with
   cleaned up names.  If there is only one file imported from the zip
   (whether specified explicitly or because the zip only contains one
   file) this will also end up as the default dataset.\\
\RP \texttt{chaining}          & \texttt{on}  &
  Can be one of \texttt{off}, \texttt{on}, \texttt{by\_filename},
  or \texttt{by\_dsname}.
  \begin{itemize}
  \item[]\texttt{off} -- Don't chain the imports.
  \item[]\texttt{on} -- Chain the imports in the order the files are in the zip file.
  \item[]\texttt{by\_filename} -- Chain in filename order.
  \item[]\texttt{by\_dsname} -- Chain in dataset name order. Since \texttt{inside\_filenames} is a dict this is your only way of controlling its order.
\end{itemize}\\
\RP \texttt{include\_re}       & \texttt{''}  & Regexp of files to include, matches anywhere.\\
\RP \texttt{exclude\_re}       & \texttt{''}  & Regexp of files to exclude, takes priority over \texttt{include\_re}.\\
\RP \texttt{strip\_dirs}       & \pyFalse     & Strip directories from filename (\texttt{a/b/c} $\rightarrow$ \texttt{c}.)\\
\stoptable
\noindent If chaining is enabled, the last dataset will be the \texttt{default}
dataset.  Note that naming a non-last dataset ``\texttt{default}'' is
an error.  If \texttt{strip\_dirs} is set, the filename (as used for
both sorting and naming datasets, but not when matching regexes) will
not include directories. The default is to include directories.



\subsection{Example invocation}

\begin{python}
    jid = urd.build("csvimport_zip",
        options=dict(
            filename="data_Q2_2019.zip",
            exclude_re=r"(__MACOSX|\.DS_Store)",
            chaining="by_filename",
            strip_dirs=True,
        ),
    )
\end{python}



\clearpage
\section{\texttt{dataset\_type} -- Typing Datasets}
The \texttt{dataset\_type} method will read a source dataset or
dataset chain and type its columns.  This method is primarily used for
typing datasets created by \texttt{csvimport}, but it can type any
column of type \texttt{bytes}, \texttt{ascii}, or \texttt{unicode} to
any other type.

The method will also hash partition the output dataset if the
\texttt{hashlabel} input paramter is set, causing a new dataset to be
created.  For additional information about hash partitioning, see the
\texttt{dateset\_hashpart} method in section~\ref{sec:dataset_hash_partition}.

The default behaviour is to append new columns with typed data to the
existing source dataset.  These columns will have the same name as the
untyped version of the data, making the untyped data ``inaccessible'',
even if it is still in the dataset.  Using the \texttt{rename} option,
typed columns can be assigned a name that differs from the original
name, so that both typed and untyped data are available
simultaneously.  This brings transparency to the typing process.
(However, even if the untyped data is ``inaccessible'' in the typed
dataset, it is still available if referenced as the input dataset.)

In order to type the data, the input data is subject to parsing.  Some
datasets may contain data that is incorrect in the sense that it
causes parsing errors when typing.  Unparseable data can either be
replaced by a default value or removed from the dataset.  Since the
Accelerator's dataset type does not permit removal of rows,
i.e.\ datasets can not shrink, \texttt{dataset\_type} will in this
situation create a new dataset containing only the rows containing
typeable data.

If typing a dataset chain, any columns that do not have the same type
over all the typed datasets will be discarded.


\subsection{Datasets}
\starttable
  \RP \texttt{source} & \textsl{mandatory} & Dataset to type.\\
  \RP \texttt{previous} & \mintinline{python}{None} & Previous dataset if creating a chain.\\
\stoptable

\subsection{Options}
\starttable

  \RP \texttt{column2type} & \texttt{\{\}} & A dictionary from column
  label to type, for example \mintinline{python}/{'movie':
    'unicode:UTF-8',}/.\\

  \RP \texttt{timezone} & \pyNone & Set input timezone for
  \texttt{datetime} columns.  Output will be in UTC.  See text.\\
  
  \RP \texttt{hashlabel} & \pyNone & Hash partition dataset based on
  this column.  Leave as \pyNone to inherit hashlabel, set to
  \texttt{''} to not have a hashlabel.  Hashing causes a new dataset
  to be created.\\

  \RP \texttt{defaults} & \texttt{\{\}}& A \texttt{dict} from
  column name to default value, for example
  \mintinline{python}/{'COLNAME': value}/.  Method will fail if data
  is unconvertible unless \mintinline{python}/filter_bad = True/.\\

  \RP \texttt{rename} & \texttt{\{\}} & A dictionary from old name to
  new name, for example \mintinline{python}/{'old': 'new'}/ The old
  name and data will be preserved, unless a new dataset is created ,
  and the column with the new name will contain the typed data. \\

  \RP \texttt{caption} &  & Optional caption.  A
  reasonable caption is created automatically if left blank\\

  \RP \texttt{discard\_untyped} & \pyNone & If set to \pyTrue, force
  creation of new dataset and make untyped columns inaccessible.  If
  set to \pyFalse, an error is generated if any columns were not
  preservable.\\
  
  \RP \texttt{filter\_bad} & \pyFalse & If \pyFalse, fail when a value
  fails to convert and there is no default.  If \pyTrue, filter out
  the line with the unconvertable value.  This will create a new dataset.\\

  \RP \texttt{numeric\_comma} & \mintinline{python}{False}&
  If \mintinline{python}{True}, write decimal number as
  ``\texttt{3,14}'' instead of default ``\texttt{3.14}''.\\

  \RP \texttt{length} & \texttt{-1} & Go back at most this many
  datasets. The default is \texttt{-1}, which goes
  until \texttt{previous.source} if it exists, or first dataset in
  chain otherwise.\\

  \RP \texttt{as\_chain} & \pyFalse & If hash partitioning, avoid
  re-writing at the end by doing one dataset per slice.\\

  \RP \texttt{compression} & \texttt{6} & \texttt{gzip} compression level.\\
\stoptable

\noindent The \texttt{timezone} option is used to specify which input
timezone a \texttt{datetime} column is in.  It does not work for
\texttt{date} or \texttt{time} columns.  It can be set to anything
accepted by the system's \texttt{\$TZ}.  Note
\begin{itemize}
\item[-] It does not work for \texttt{\%s} (which is always in UTC).
\item[-] No error checking can be done on this (\texttt{tzset(3)} can not return failure).
\item[-] On most 32bit systems this will break dates outside about 1970 - 2037.
\item[-] Setting this will mask most bad dates (\texttt{mktime(3)} ``fixes'' them).
\item[-] Do not set this to 'UTC', leaving it as None is faster and safer.
\end{itemize}



\subsection{Example Invocation}
An example invocation is the following
\begin{python}
urd.build('dataset_type',
    datasets=dict(
        source=...,
        previous=...,
    ),
    options=dict(
        column2type=dict(
            auct_start_dt='datetime:%Y-%m-%d',
            brand='json',
            item_id='number',
            comp='unicode:utf-8',
        ),
    )
)
\end{python}


\subsection{Typing}
This section describes all typing possibilities in detail.  Default
behaviour when typing numbers (i.e.\ \texttt{float}s, \texttt{int}s,
and \texttt{number}s) is that any number of whitespaces before and
after the actual number are silently discarded.


\subsubsection{Numbers}
The \texttt{number} type is integer or floating point.
\starttablenotitle
\RPnotitle  \texttt{number}     && \texttt{int} or \texttt{float} \\
\RPnotitle  \texttt{number:int} && \texttt{int}, will convert \texttt{float}s to \texttt{int}s.\\
\stoptablenotitle
\noindent Integers are enforced using \texttt{number:int}, and the type accepts
trailing decimal zeroes like \texttt{7.0}, \texttt{4.000} etc.  This
is useful when typing datafiles where numbers actually are integers
but have trailing zero decimals.


\subsubsection{Floating Point Numbers}
Floating point numbers may be stored as 32 or 64 bits.  In addition,
there are six parsing options that are useful in different scenarios.
The \emph{ignore} option ignores any trailing characters after the
number.  Then there are \emph{exact} that causes error if the number
does not fit, and \emph{saturate} that silently saturates a
non-fitting number.  These can also be used in combination, see table
below for all alternatives
\starttablenotitle
\RPnotitle \texttt{float32} & \texttt{float64} & \emph{default}\\
\RPnotitle \texttt{float32i} & \texttt{float64i} & \emph{ignore}, will discard trailing garbage\\
\RPnotitle \texttt{float32e} & \texttt{float64e} & \emph{exact}, error if parsed number does not fit in type \\
\RPnotitle \texttt{float32s} & \texttt{float64s} & \emph{saturate}, saturate to min/max if number does not fit in type \\
\RPnotitle \texttt{float32ei} & \texttt{float64ei} & \emph{exact} + \emph{ignore} \\
\RPnotitle \texttt{float32si} & \texttt{float64si} & \emph{saturate} + \emph{ignore} \\
\stoptablenotitle


\subsubsection{Integers}
Integers are stored as either 32 or 64 bits.  Parsing takes base into
account, so in addition to decimal numbers, it is also straightforward
to parse octal and hexadecimal numbers.  The \emph{ignore} option
causes parsing to ignore trailing garbage characters.
\starttablenotitle
\RPnotitle   \texttt{int32\_0}   & \texttt{int64\_0}   & \emph{auto}, avoid and use a deterministic type if possible \\
\RPnotitle   \texttt{int32\_0i}  & \texttt{int64\_0i}  & \emph{auto}, ignore trailing garbage \\
\RPnotitle   \texttt{int32\_8}   & \texttt{int64\_8}   & \emph{octal} \\
\RPnotitle   \texttt{int32\_8i}  & \texttt{int64\_8i}  & \emph{octal}, ignore trailing garbage \\
\RPnotitle   \texttt{int32\_10}  & \texttt{int64\_10}  & \emph{decimal} \\
\RPnotitle   \texttt{int32\_10i} & \texttt{int64\_10i} & \emph{decimal}, ignore trailing garbage \\
\RPnotitle   \texttt{int32\_16}  & \texttt{int64\_16}  & \emph{hexadecimal} \\
\RPnotitle   \texttt{int32\_16i} & \texttt{int64\_16i} & \emph{hexadecimal}, ignore trailing garbage \\
\stoptablenotitle


\subsubsection{Integers Stored as Floats}
There are also a parsing options for integers that are represented in
a floating point format in the source data.  This is useful if integer
data is stored with decimals, such as \texttt{5.0}.  In pseudocode,
the parsing basically runs \texttt{int(float(value))} for each such
value.
\starttablenotitle
\RPnotitle   \texttt{floatint32e} & \texttt{floatint64e}  & \emph{exact}, error if parsed number does not fit in type\\
\RPnotitle   \texttt{floatint32s} & \texttt{floatint64s}  & \emph{saturate}, saturate to min/max if number does not fit in type\\
\RPnotitle   \texttt{floatint32ei}& \texttt{floatint64ei} & \emph{exact} + \emph{ignore}\\
\RPnotitle   \texttt{floatint32si}& \texttt{floatint64si} & \emph{saturate} + \emph{ignore}\\
\stoptablenotitle


\subsubsection{Convert to Boolean}
It is common that a column holds values that are to be interpreted as
either \texttt{False} or \texttt{True}.  The following types handles
strings and floats.
\starttablenotitle
\RPnotitle  \texttt{strbool} && \mintinline{python}{False} if value in
  (\pyFalse, \texttt{0}, \texttt{f}, \texttt{no}, \texttt{off},
  \texttt{nil},\\
\RPnotitle && \texttt{null}, ````)\\ && \mintinline{python}{True} otherwise\\

\RPnotitle  \texttt{floatbool} && \mintinline{python}{True} when float has
  bits set. Is \mintinline{python}{False} otherwise.\\

\RPnotitle  \texttt{floatbooli} && same + \emph{ignore}\\
\stoptablenotitle


\subsubsection{Time and Date}
There are three types relating to time available, \texttt{date},
\texttt{time}, and \texttt{datetime}.  Each of these has a
corresponding version that ignores trailing garbage characters.  All
time types require a format specification (represented here by an
asterisk) as described below\\
\starttablenotitle
\RPnotitle  \texttt{date:*}      && a date with format specifier\\
\RPnotitle    \texttt{datei:*}     && same + \emph{ignore}\\
\RPnotitle    \texttt{time:*}      && a time with format specifier\\
\RPnotitle    \texttt{timei:*}     && same + \emph{ignore}\\
\RPnotitle    \texttt{datetime:*}  && a date + time with format specifier\\
\RPnotitle    \texttt{datetimei:*} && same + \emph{ignore}\\
\stoptablenotitle
\noindent The format is standard Python time formats, like shown in
these examples
\begin{python}
# will match for example '2017-03-22'
auct_start_dt='date:%Y-%m-%d'  
# will match for example '183000', i.e. half past six in the evening
tod='time:%H%M%S'
# will match for example '2017-03-22 18:30:15'
timestamp='datetime:'%Y-%m-%d %H:%M:%S'
\end{python}


\subsubsection{Strings and Byte Sequences}
There are a number of ways to read string and byte data, depending on
how the raw input data is to be interpreted.  The basic types are
shown first, and the more advanced variations and options will be
described below.
\starttablenotitle
\RPnotitle  \texttt{bytes}      && list of bytes\\
\RPnotitle    \texttt{bytesstrip} && list of bytes, strip characters 8-13, 32 from start and end\\
\RPnotitle    \texttt{ascii}      && list of ASCII characters\\
\RPnotitle    \texttt{asciistrip} && list of ASCII characters, strip characters 8-13, 32 from start and end\\
\stoptablenotitle

\noindent When typing to unicode and ASCII, there are several ways to handle
individual unparsable characters.  For unicode, there are two types,
\starttablenotitle
\RPnotitle  \texttt{unicode:*}  && list of unicode characters\\
\RPnotitle    \texttt{unicodestrip:*} && list of unicode characters, strip
  characters 8-13, 32 from start and end\\
\stoptablenotitle
\noindent The asterisk represents options that take the form
\begin{python}
"codec" #or
"codec/errors"
\end{python}
\texttt{unicode:codec/errors} will read bytes encoded in
\texttt{codec} and write "unicode" (which is stored as utf-8, but
that's invisible to the Python side).  \texttt{codec} is often
\texttt{utf-8}, but could be for example \texttt{utf-8},
\texttt{ascii}, \texttt{iso-8859-1}, \texttt{iso-8859-15},
\texttt{cp437}, or \texttt{windows-1252} etc.  See the Python
documentation
\begin{center}
  \texttt{https://docs.python.org/2/library/codecs.html\#standard-encodings}
\end{center}
for more information.  The \texttt{errors} part is optional, and
can be one of\\

\begin{tabular}{p{2cm}p{10cm}}
  \texttt{strict} &The default, an error marks this row as bad\\
  \texttt{ignore} & All unparsable bytes are discarded.\\
  \texttt{replace} & All unparsable bytes are replaced by the unicode
  replacement character ("\texttt{\textbackslash ufffd}").\\
\end{tabular}\\

\noindent Using \texttt{strict} will cause errors if unparsable.  For
example, typing the string "\texttt{ab\textbackslash xffc}" will give
an error (\texttt{strict}), "\texttt{abc}" (ignore), or
"\texttt{ab\textbackslash ufffdc}" (replace).  \texttt{strip} will
happen before \texttt{ignore}.

ASCII is similar, there are two types
\starttablenotitle
\RPnotitle \texttt{ascii:*}      && list of ASCII characters\\
\RPnotitle \texttt{asciistrip:*} && list of ASCII characters, strip characters 8-13,32 from start and end\\
\stoptablenotitle

\noindent where the argument is one of\\

\begin{tabular}{p{2cm}p{10cm}}
  \texttt{strict} & The default, an error marks this row as bad\\
  \texttt{ignore} & All unparsable bytes are discarded\\
  \texttt{replace} & All unparsable bytes are replaced by an octal
  escapes "\texttt{\textbackslash ooo}"\\
  \texttt{encode} & Like replace except "\texttt{\textbackslash}" is also replaced by
  "\texttt{\textbackslash 134}" (for full reversibility).\\
\end{tabular}\\
\noindent Using \texttt{strict} will cause errors if unparsable.
\texttt{strip} will happen before \texttt{ignore}.



\clearpage
\section{\texttt{csvexport} -- Exporting Text Files}
The \texttt{dataset\_export} method is used to export datasets to
column based text files (CSV, Comma Separated Values).  It can export
plain files and \texttt{gzip}-compressed files, export a chain of
datasets, export one output file per slice, and more.  Read the
Options section for full details.

\subsection*{Options}
\starttable

  \RP \texttt{filename} & \textsl{mandatory} & Name of output file.
  File will by default be stored in the job's job directory.  The
  filename has to end with ``\texttt{.csv}'' for plain text files, and
  ``\texttt{.gz}'' for gzipped output.\\
                                                                                                                                %          .gz
  \RP \texttt{separator} & ',' & Column separator.\\
  
  \RP \texttt{labelsonfirstline} & \mintinline{python}{True} & If
  \mintinline{python}/True/, write column names on first row.\\
  
  \RP \texttt{chain\_source} & \mintinline{python}{False} & If
  \mintinline{python}/True/, read a dataset chain from
  \mintinline{python}/datasets.source/ back to
  \mintinline{python}/jobs.previous/\\

  \RP \texttt{quote\_fields} & \textsl{empty string} & Export quoted fields.  Must be empty
  (no quote character, default), ``\texttt{'}``, or ``\texttt{"}``.\\
  
  \RP \texttt{labels} & \mintinline{python}/[]/ & Specify which labels to
  export.  An empty list corresponds to all labels in dataset.\\
  
  \RP \texttt{sliced} & \mintinline{python}/False/ & Each slice is
  exported in a separate file when \mintinline{python}/True/.  If so,
  use \mintinline{python}/"%02d"/ or similar in filename as
  placeholder for the slice number.\\

  \RP \texttt{compression} & \texttt{6} & gzip level\\

  \RP \texttt{line\_separator} & \mintinline{python}|'\n'| & Line separator.\\
\stoptable


\subsection*{Datasets}
\starttable
  \RP \mintinline{python}{[source,]} & \textsl{mandatory} & Either A
  \textsl{single} dataset or a \textsl{list} of datasets.\\
\stoptable


\subsection*{Jobs}
\starttable
  \texttt{previous} & \mintinline{python}/None/ & Job reference to
  previous \texttt{csvexport} if chained.\\
\stoptable


\subsection{Example Invocation}
An example invocation is the following
\begin{python}
urd.build(csvexport',
    datasets=dict(
        source='test-3/foo',
    ),
    options=dict(
        filename='output.txt.gz',
        separator=' ', quote_fields="\'",
        ),
    )
)
\end{python}



\clearpage
\section{\texttt{dataset\_hashpart} -- Hash Partition a Dataset}
\label{sec:dataset_hash_partition}

The \texttt{dataset\_hashpart} method will create a new dataset based on
its \texttt{source} dataset.  The new dataset will be hash partitioned
on a column specified in the options.


\subsection*{Options}
\starttable
  \RP \texttt{hashlabel} & \textsl{mandatory} & column for hashing,
  required.  Note that columns typed as \texttt{list}, \texttt{set},
  or \texttt{json} cannot be used for hashing.\\

  \RP \texttt{length} & \mintinline{python}/-1/ & Go back at most this
  many datasets in a chain.  Default is \texttt{-1}, which goes back
  to \texttt{previous.source} if it exists, or to the first dataset in
  the chain otherwise.\\

  \RP \texttt{caption} & & Optional caption.  A
  reasonable caption is created automatically if left blank\\

  \texttt{as\_chain} & \pyFalse & True generates one dataset per slice, False
  generates one dataset.  Default \texttt{False}.\\
\stoptable


\subsection*{Datasets}
\starttable
  \RP \texttt{source} & \textsl{mandatory} & Source dataset to hash partition\\
  \RP \texttt{previous} & \mintinline{python}/None/ & Previous dataset to chain  to.\\
\stoptable


\subsection{Example Invocation}
An example invocation is the following
\begin{python}
urd.build('dataset_hashpart',
          datasets=dict(source=jid,), 
          options=dict(hashlabel='start_date',))
\end{python}


\subsection{Hashing Details}
This method will create a new dataset based on all the data in the
source dataset.  The difference between input and output is in which
slices the rows will be stored.  For each row, the target slice is
determined based on the output value of a hashing function applied to
a certain column (the \texttt{hashlabel}) of that row.  To illustrate
the operation, the code is similar to
\begin{python}
from accelerator.gzutil import siphash24

target_sliceno = siphash24(cols[hashlabel]) % params.slices
\end{python}


\subsection{Notes on Chains}
\begin{enumerate}
  \item[1.]  The default operation is to hash partition a complete
    chain of datasets from \texttt{source} back to
    \texttt{previous.source}.  This is controlled by the
    \texttt{length} option.

  \item[2.]  Internally, \texttt{dataset\_hashpart} always
    generates one dataset per slice in a chain.  This is also what is
    returned if \mintinline{python}/as_chain == True/.  Otherwise, all
    datasets will be concatenated into one.  Thus, there is a choice
    of either having the output as a chain of datasets -- or as a
    single dataset.  The chain will execute faster, since the
    concatenation step is omitted.
\end{enumerate}

%% \clearpage
%% \subsection{Internal Operation}
%% Figure~\ref{fig:dataset_rehash} shows how rehashing happens...

%% Each \texttt{analysis} reads one complete slice of the source dataset
%% and hashes it into a new dataset.  This means that there is one new
%% dataset created per slice.  Each of these datasets are chained and
%% named using an integer counter starting at zero.  The last slice,
%% however, is named \texttt{default}.

%% If the \texttt{as\_chain} is set, \texttt{dataset\_rehash} is finished.
%% Otherwise, in \texttt{synthesis}, each of the datasets are read in
%% turn and concatenated into a single dataset.  When this is finished,
%% all datasets except the concatenated one is removed from disk.


%% \begin{figure}[h!]
%%   \begin{center}
%%     \input{dataset_rehash.pdf_t}
%%   \end{center}
%%   \caption{\texttt{dataset\_rehash} data flow}
%%   \label{fig:dataset_rehash}
%% \end{figure}



\clearpage
\section{\texttt{dataset\_filter\_columns} -- Removing Columns from a Dataset}

The \texttt{dataset\_filter\_columns} method removes columns from a
dataset.  It is typically run before applying methods that operate on
all columns of a dataset and only a subset of the columns are
required.  A typical example is \texttt{dataset\_hashpart} that
operates on all columns of a dataset.  If not all columns are needed,
time and storage can be saved by removing columns using this method
prior to applying \texttt{dataset\_hashpart}.

Note that this method only updates soft links, and no data is actually
copied.  So execution time is typically a fraction of a second and no
redundant data is written to disk.

\subsection*{Options}
\starttable
  \RP \texttt{keep\_columns} & \mintinline{python}/[]/ & A list of columns to keep.\\
  \RP \texttt{discard\_columns} & \mintinline{python}/[]/ & A list of columns to remove.\\
\stoptable
Only one of \texttt{keep\_columns} and \texttt{discard\_columns} may
be populated.


\subsection*{Datasets}
\starttable
\RP \mintinline{python}{source} & \textsl{mandatory} & Source dataset to filter.\\
\RP \mintinline{python}{previous} & \pyNone & Previous dataset to chain to.\\
\stoptable


\clearpage
\section{\texttt{dataset\_sort} -- Sorting a Dataset}
The method \texttt{dataset\_sort} is used to sort relatively large
datasets.  One or more columns may be selected for sorting, and it
will sort one column at a time.  The sorting algorithm is stable,
meaning that things with equal sorting keys will keep their order.


\subsection*{Options}

\starttable
\RP \texttt{sort\_columns} & \textsl{mandatory} & A column or a list of
  columns.  If a list is specified, sorting will be carried out from left
  to right.\\

  \RP \texttt{sort\_order} & \texttt{ascending} & Could be reversed by
  specifying \texttt{descending}\\
  
  \RP \texttt{sort\_across\_slices} & \mintinline{python}/False/& If
  \mintinline{python}/False/, only sort within slices.  Otherwise sort
  across slices.\\

  \RP \texttt{trigger\_column} & \texttt{''} & If
  \texttt{sort\_across\_slices} is set, use this to delay the slice
  switches to the next line where the value in this column changes.\\

\stoptable


\subsection*{Datasets}
\starttable
\RP \texttt{source} & \textsl{mandatory}& A dataset to sort.\\
\RP \texttt{previous} & \mintinline{python}/None/ & A previous dataset to chain to.\\
\stoptable


\subsection{Sorting \pyNone and \mintinline{python}|NaN| values}
The special values \mintinline{python}|None| and \mintinline{python}|NaN| follow these rules
\begin{itemize}
\item \mintinline{python}|NaN| will sort same as \mintinline{python}|+Inf|, i.e.\ \textsl{last}.
\item \pyNone sorts as \mintinline{python}|-Inf|, i.e.\ \textsl{first} in \texttt{float} columns.  Intermingled \pyNone and \mintinline{python}|-Inf| will keep their original order due to the stable sorting algorithm.
\item \pyNone sorts \textsl{last} in \texttt{date}, \texttt{time}, and \texttt{datetime} columns.
\item For all other types, \pyNone sorts \textsl{first}.
\end{itemize}


\subsection{Spreading of Left Over Values}
If the number of rows in a dataset is not even divisible by the number
of slices, some slices will have one more row than others.  Instead of
putting this data in, say, the first slices, \texttt{dataset\_sort}
attempts to even out any bias by selecting the slices that get the
additional data row in a pseudo-random manner.  In order to have the
sorting stable, selection of slices is based on the first values of
the sorting column.  It is not perfect, if the data is already sorted
the first slices will be picked, but it is stable, which is the most
important thing.



\subsection{The Trigger Column}
If sorting across slices, i.e.\ the full dataset will be sorted from
the first to the last slice, the \texttt{trigger\_column} could be
used to delay the slice switching so that it does not appear unless
the value in the column is changing.  This is beneficial in parallel
processing tasks, because it localises values to single slices.



\subsection{A Practical Limitation}
Internally, the method works by reading the columns to sort by, and
create an indexing column that stipulates the sorting order.  Each
column is then read in turn and sorted according to the sorting
column.  Therefore, the method has limited sorting capability.
Internally, it sorts one column at a time, and it needs to hold that
complete column plus an indexing column in memory simultaneously.
Still, a standard computer can sort very large datasets without
trouble.



\clearpage
\section{\texttt{dataset\_checksum}, \texttt{dataset\_checksum\_chain}}

The \texttt{dataset\_checksum} method is used to create a single
checksum from a dataset based on one or more columns.  The chained
version returns a single checksum from a dataset chain.  It is mainly
intended as a debugging aid, enabling comparison of datasets across
machines, even if they have different slicing.

If \mintinline{python}|options.sort=False|, hashing will depend on the
actual row order of the dataset.  If, on the other
hand, \mintinline{python}|options.sort=True|, hashing will
be \textsl{slice invariant} and \textsl{row order invariant}, meaning
that the methods only look at the contents of the dataset(s).

Chain limits will affect the checksum of a chain, so if checksumming
two chains containing the same data, but with different number of
chained datasets, their checksums will differ.

Note that sorting uses about 64 bytes per row, upper limiting the size
of hashable datasets.  This corresponds to about 1GB of RAM per 20
million lines or so.

Note also that \texttt{pickle}-columns cannot be used for
checksumming, since the order of the pickled data stricture is not
defined.

The checksum will be printed to \texttt{stdout} as well as written to
\texttt{result.pickle}.

\subsection*{Options}
\starttable
\RP \texttt{columns} & \texttt{set()} & A set of columns to base the checksum on.  Leave blank for all columns\\

\RP \texttt{sort} & \texttt{True} & Sort dataset before hashing, see text. \\

\RP \texttt{chain\_length} & \texttt{-1} & Number of datasets in chain to hash. Only for \texttt{dataset\_checksum\_chain}.\\
\stoptable


\subsection*{Datasets}
\starttable
\RP \texttt{source} & \textsl{mandatory}& A dataset to checksum.\\
\RP \texttt{stop} & \pyNone & Stop hashing at this dataset.  Only for \texttt{dataset\_checksum\_chain}.\\
\stoptable






\clearpage
\section{\texttt{dataset\_merge} -- Merge Several Datasets into One}
Merge two or more datasets.  The datasets must have the same number of
lines in each slice and if they do not have a common ancestor you must
set \texttt{allow\_unrelated}=\pyTrue.  Columns from later datasets
override columns of the same name from earlier datasets.  Note that
merging is a very fast constant time operation.


\subsection*{Options}
\starttable
\texttt{allow\_unrelated} & \pyFalse & Must be \pyTrue to join datasets that do not share a common ancestor.\\
\stoptable


\subsection*{Datasets}
\starttable
\mintinline{python}/[source,]/ & \textsl{mandatory}& A list of datasets to merge.\\
\texttt{previous} & \pyNone & Previous for the merged dataset.\\
\stoptable





\clearpage
\section{\texttt{dataset\_unroundrobin} -- Transpose a Dataset}

This method will transpose the dataset so that first row from slices
0, 1, 2, \dots will end up in slice 0 rows 0, 1, 2 \dots.  It always
creates a new dataset.  If \texttt{trigger\_column} is set to the name
of a column, slice switching is delayed until the data in the
specified column changes.  Otherwise, the new dataset will have the
same number of rows per slice as the source dataset.

The \texttt{trigger\_column} will ensure that no value is spread over
more than one slice, assuming that all occurances of the same value
are grouped together, which is helpful in many parallel processing
cases.



\subsection*{Options}
\starttable
\texttt{trigger\_column} & \pyNone & Set to delay slice switching based on content.\\
\stoptable



\subsection*{Datasets}
\starttable
\texttt{source} & \textsl{mandatory}& An input dataset.\\
\texttt{previous} & \pyNone & Previous for the created dataset.\\
\stoptable
