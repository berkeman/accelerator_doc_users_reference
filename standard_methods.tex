\section{csvimport}

This method is used to import text files in tabular format.  It reads
plain text files as well as \texttt{gzip} compressed files.

\subsection{Arguments}

\noindent Options\\

\begin{tabular}{ll}
  \texttt{filename} & \\ %                  : OptionString,
  \texttt{separator} & \\ %                 : ',',
  \texttt{labelsonfirstline} & \\ %         : True,
  \texttt{labels} & \\ %                    : [], # Mandatory if not labelsonfirstline, always sets labels if set.
  \texttt{hashlabel} & \\ %                 : None,
  \texttt{quote\_support} \\ %               : False, # 'foo',''bar'' style CSV
  \texttt{rename} & \\ %                    : {},    # Labels to replace (if they are in the file) (happens first)
  \texttt{discard} & \\ %                   : set(), # Labels to not include (if they are in the file)
  \texttt{allow\_bad} & \\ %               : False, # Still succeed if some lines have too few/many fields.
\end{tabular}\\
\\

Input \texttt{filename} is mandatory and may either be a plain text
file or a gzipped file.  A path may be specified as part of the
filenam, and this path is relative to the configuration option
\texttt{source\_directory}.  This is to make input files relocateable
so that reading files from a different directoy will not trig remaking
the import jobs.

The \texttt{separator} option may contain any character to be used for
separating the columns in the input file.  The default separator is
comma.

Labels may be used if they appear on the first line.  This is the
default.  If \texttt{labelsonfirstline} is set to False, labels must
be entered using the \texttt{label} list option.

An optional \texttt{hashlabel} may be assigned, but bear in mind that
hashing will occur on the input text data.  Thus, data is to be hashed
on for example the integer value of a column, rehashing have to take
place after typing to integer.

The option \texttt{quote\_support} makes it possible to read files
with quoted values.

It is possible to rename columns while importing using the
\texttt{rename} option.  If used, a dictionary with
\texttt{source\_name} to \texttt{dest\_name} must be provided.
Columns may also be omitted from importing using the \texttt{discard}
option.  If used, a set containing columns to omit must be provided.

Finally, there is an \texttt{allow\_bad} option that relaxes the input
file strictness.  If set to \texttt{False}, which is the default,
\texttt{csvimport} will assert an error if there are format errors in
the input data.  Setting it to \texttt{True} makes importing silently
ignoring any format errors.  Using this option calls for checking the
result.


\noindent Datasets\\
\\
\begin{tabular}{ll}
  \texttt{previous}
\end{tabular}\\
\\
The only input dataset is \texttt{previous} which is set to the previous job when creating a chain.


\subsection{Output}
The result of the \texttt{csvimport} is a dataset.  There is also a
\texttt{report.txt} stored in the jobdir which contains information on
number of lines read and more (CHECK THIS!)






\clearpage
\section{dataset\_type}

A dataset imported with \texttt{csvimport} may be typed using
\texttt{dataset\_type}.  This method parses the input data...



\subsection{Arguments}

Datasets\\

\begin{tabular}{ll}
  \texttt{source} & \\
  \texttt{previous} & \\
\end{tabular}\\

\noindent Options\\

\begin{tabular}{ll}
  \texttt{column2type}               & \\%{'COLNAME': TYPENAME},
  \texttt{defaults}                  & \\%{}, # {'COLNAME': value}, unspecified -> method fails on unconvertible unless filter\_bad
  \texttt{rename}                    & \\%{}, # {'OLDNAME': 'NEWNAME'} doesn't shadow OLDNAME.
  \texttt{caption}                   & \\%'typed dataset',
  \texttt{discard\_untyped}           & \\%bool, # Make unconverted columns inaccessible (``new'' dataset)
  \texttt{filter\_bad}                & \\%False, # Implies discard_untyped
  \texttt{numeric\_comma}             & \\%False, # floats as ``3,14''
\end{tabular}


\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build('dataset_type', ...,
  options=dict(
    column2type=dict(
      auct_start_dt='datetime:%Y-%m-%d',
      brand='json',
      item_id='number',
      comp='unicode:utf-8',
    ),
  )
\end{python}



\subsection{Typing}
This section describes all typing options in detail.

\subsubsection{Numbers}
The \emph{number} type is int or float.\\

\begin{tabular}{ll}
  \texttt{number}     & \texttt{int} or \texttt{float}\\
  \texttt{number:int} & \texttt{int}, converts floats to int.\\
\end{tabular}



\subsubsection{Float Point Numbers}
Floating point numbers may be stored as 32 or 64 bits.  In addition,
there are six parsing options that are useful in different scenarios.
The \emph{ignore} option ignores any trailing characters after the
number.  Then there are \emph{exact} that causes error if the number
does not fit, and \emph{saturate} that silently saturates a
non-fitting number.  These can also be used in combination, see table
below for all alternatives\\

\begin{tabular}{lll}
\texttt{float32} & \texttt{float64} & \emph{default}\\
\texttt{float32i} & \texttt{float64i} & \emph{ignore}, will discard trailing garbage\\
\texttt{float32e} & \texttt{float64e} & \emph{exact}, error if parsed number does not fit in type \\
\texttt{float32s} & \texttt{float64s} & \emph{saturate}, saturate to min/max if number does not fit in type \\
\texttt{float32ei} & \texttt{float64ei} & \emph{exact} + \emph{ignore} \\
\texttt{float32si} & \texttt{float64si} & \emph{saturate} + \emph{ignore} \\
\end{tabular}

\subsubsection{Integers}
Integers are stored as either 32 or 64 bits.  Parsing takes base into
account, so in addition to decimal numbers, it is also straightforward
to parse octal and hexadecimal numbers.  The \emph{ignore} option
causes parsing to ignore trailing garbage characters.\\

\begin{tabular}{lll}
  \texttt{int32\_0}   & \texttt{int64\_0}   & \emph{auto}, avoid and use a deteministic type if possible \\
  \texttt{int32\_0i}  & \texttt{int64\_0i}  & \emph{auto}, ignore trailing garbage \\
  \texttt{int32\_8}   & \texttt{int64\_8}   & \emph{octal} \\
  \texttt{int32\_8i}  & \texttt{int64\_8i}  & \emph{octal}, ignore trailing garbage \\
  \texttt{int32\_10}  & \texttt{int64\_10}  & \emph{decimal} \\
  \texttt{int32\_10i} & \texttt{int64\_10i} & \emph{decimal}, ignore trailing garbage \\
  \texttt{int32\_16}  & \texttt{int64\_16}  & \emph{hexadecimal} \\
  \texttt{int32\_16i} & \texttt{int64\_16i} & \emph{hexadecimal}, ignore trailing garbage \\
\end{tabular}



\subsection{Integers Stored as Floats}

There are also a parsing options for integers that are represented in
a floating point format in the source data.  This is useful if integer
data is stored with decimals, such as \texttt{5.0}.  In pseudocode,
the parsing basically runs \texttt{int(float(value))} for each such
value.\\

\begin{tabular}{lll}
  \texttt{floatint32e} & \texttt{floatint64e}  & \emph{exact}, error if parsed number does not fit in type\\
  \texttt{floatint32s} & \texttt{floatint64s}  & \emph{saturate}, saturate to min/max if number does not fit in type\\
  \texttt{floatint32ei}& \texttt{floatint64ei} & \emph{exact} + \emph{ignore}\\
  \texttt{floatint32si}& \texttt{floatint64si} & \emph{saturate} + \emph{ignore}\\
\end{tabular}



\subsection{Convert to Boolean}
It is common that a column holds values that are to be interpreted as
either \texttt{False} or \texttt{True}.  There are two types that is
useful in this context.  The first one is \texttt{strbool}, that works like this\\

\begin{tabular}{ll}
  \texttt{strbool} & \texttt{False} if value in (\texttt{'false', '0', 'f', 'no', 'off', 'nil', 'null', ''})\\
                   & \texttt{True} otherwise
\end{tabular}
\\
The other is \texttt{floatbool} that is True when the float has bits
set to one and zero otherwise.  There is also a \texttt{floatbooli}
that ignores trailing garbage characters.\\

\begin{tabular}{ll}
  \texttt{floatbool}  & True if float has bits set to one, zero otherwise\\
  \texttt{floatbooli} & same + \emph{ignore}\\
\end{tabular}



\subsection{Time and Date}
There are three types relating to time available, \texttt{date},
\texttt{time}, and \texttt{datetime}.  Each of these has a
corresponding version that ignores trailing garbage characters.
All time types require a format specification as described below\\

\begin{tabular}{ll}
  \texttt{date:*}      & a date with format specifier\\
  \texttt{datei:*}     & same + \emph{ignore}\\
  \texttt{time:*}      & a time with format specifier\\
  \texttt{timei:*}     & same + \emph{ignore}\\
  \texttt{datetime:*}  & a date + time with format specifier\\
  \texttt{datetimei:*} & same + \emph{ignore}\\
\end{tabular}\\

The format is standard Python time formats, see for example\\
\\
\begin{python}
   # will match for example '2017-03-22'
   auct_start_dt='date:%Y-%m-%d'  
   # will match for example '183000', i.e. half past six in the evening
   tod='time:%H%M%S'
   # will match for example '2017-03-22 18:30:15'
   timestamp='datetime:'%Y-%m-%d %H:%M:%S'
\end{python}


\subsection{Strings and Byte Sequences}
There are a number of ways to read string and byte data, depending on
how the raw input data is to be interpreted.  Here are the basic
types, and variations and options will be described below.\\

\begin{tabular}{ll}
  \texttt{bytes}      & list of bytes\\
  \texttt{bytesstrip} & list of bytes, strip characters 8-13,32 from start and end\\
  \texttt{ascii}      & list of ascii characters\\
  \texttt{asciistrip} & list of ascii characters, strip characters 8-13,32 from start and end\\
  \texttt{unicode:*}    & list of unicode characters\\
\end{tabular}\\

\noindent The following also takes argument\\

\begin{tabular}{ll}
  \texttt{unicodestrip:*}  & list of unicode characters, strip characters 8-13,32 from start and end\\
\end{tabular}\\

\noindent where the argument is one of \texttt{replace}, \texttt{ignore}, and \texttt{strict}.
\texttt{strict} will cause error, \texttt{ignore} will erase illegal characters.\\

\begin{tabular}{ll}
  \texttt{ascii:*}      & list of ascii characters\\
  \texttt{asciistrip:*} & list of ascii characters, strip characters 8-13,32 from start and end\\
\end{tabular}\\

\noindent where the argument is one of \texttt{replace}, \texttt{encode}, and \texttt{strict}.
\texttt{strict} will cause error, \texttt{ignore} will erase illegal characters,
and \texttt{encode} is reversible encoding.


\clearpage
\section{dataset\_csvexport}

The \texttt{dataset\_export} metod is used to export datasets to text
files.  It may write chains of datasets, it may write one dataset per
slice and more.


\subsection{Input Options, Datasets, and Jobids}

\noindent Options\\

\begin{tabular}{ll}
  \texttt{filename} & \\ %          = OptionString, # .csv or .gz
  \texttt{separator} & \\ %         = ',',
  \texttt{labelsonfirstline} & \\ % = True,
  \texttt{chain\_source} & \\ %       = False, # everything in source is replaced by datasetchain(self, stop=from previous)
  \texttt{quote\_fields} & \\ %      = '', # can be ' or ``
  \texttt{labels} & \\ %            = [], # empty means all labels in (first) dataset
  \texttt{sliced} & \\ %            = False, # one output file per slice, put %02d or similar in filename
\end{tabular}\\

\noindent Datasets\\

\begin{tabular}{ll}
  \texttt{datasets} & \\ %  = (['source'],) # normally just one, but you can specify several
\end{tabular}\\

\noindent jobids\\

\begin{tabular}{ll}
  \texttt{jobids} & \\ % = ('previous',)
\end{tabular}


The \texttt{dataset\_export} may write a single output file or one
file per slice.  The output format is either text or gziped text, with
separators between columns.  Filename is specified by the
\texttt{filename} option, and if \texttt{sliced} is set to
\texttt{True} (\texttt{False} is default), the filename should contain
a \texttt{\%03d} or similar for the slice number to appear in the
output files.

Any separator may be specified by the \texttt{separator} option.
Values may be quoted by setting a quote character in the
\texttt{quote\_fields} option.  Valid quotes are WHAT?


Default behaviour is to write the names of the columns on the first
line.  This is suppressed by setting \texttt{labelsonfirstline} to
\texttt{False}.  Individual columns may be selected for export using
the \texttt{labels} list, which defaults to all labels in the first
dataset in a chain.

How a chain of datasets is to be exported is controlled by the
\texttt{chain\_source} option.  Internally, this is input as \texttt{stop\_jobid}
to the iterator creating........By default, it exports everything in a
chain back to the \texttt{previous} dataset.WHAT ELSE? 















\clearpage
\section{dataset\_rehash}

Dataset rehash will create a new dataset based on its \texttt{source}
dataset.  The new dataset will be hashed on the column specified by
the \texttt{hashlabel} input option.

\subsection{Input Datasets and Options}

Datasets\\

\begin{tabular}{ll}
  \texttt{source}   & source dataset to rehash\\
  \texttt{previous} & previous dataset that will be chained to\\
\end{tabular}\\


\noindent Options\\

\begin{tabular}{ll}
  \texttt{hashlabel} & column for hashing, required.\\
  \texttt{length}    & Go back at most this many datasets. Default is -1, which goes back to previous.source\\
  \texttt{caption}   & Optional caption.  A reasonable caption is created automatically if left blank\\
  \texttt{as\_chain}  & True generates one dataset per slice, False generates one dataset.  Default \texttt{False}.\\
\end{tabular}\\

\noindent Note that columns typed as \texttt{json}, list, and set cannot be used
for hashing.


\subsection{Example Invocation}
\begin{python}
urd.build('dataset_rehash',
  datasets=dict(
    source=jid,
  ), 
  options=dict(
    hashlabel = 'start_date',
  )
)
\end{python}



\subsection{Hashing Details}
Hashing on a column means that each row will be copied to a slice that
is determined from the value of the \texttt{hashlabel} column.  In
particular, the value of the row's \texttt{hashlabel} column will be
fed through a hashing function, and the output modulo the number of
slices will be the target slice number.  In meta-language, it is
something like this
\\
\begin{python}
from gzutil import siphash
target_sliceno = siphash(cols[hashlabel])
\end{python}



\subsection{Note on Chains}
\subsubsection{1.}
Note that default operation is to rehash a complete chain of datasets
from \texttt{source} back to \texttt{previous.source}.  This is
controlled by the \texttt{length} option.

\subsubsection{2.}
Internally, \texttt{dataset\_rehash} always generates one dataset per
slice in a chain.  This is also what is returned if \texttt{as\_chain
  == True}.  Otherwise, all datasets are concatenated into one.  Thus,
there is a choice of either having the output as a chain of datasets
or as a single dataset.  The chain will execute faster, since the
concatenation step is omitted.


\clearpage
\subsection{Internal Operation}
Figure~\ref{fig:dataset_rehash} shows how rehashing happens...

Each \texttt{analysis} reads one complete slice of the source dataset
and hashes it into a new dataset.  This means that there is one new
dataset created per slice.  Each of these datasets are chained and
named using an integer counter starting at zero.  The last slice,
however, is named \texttt{default}.

If the \texttt{as\_chain} is set, \texttt{dataset\_rehash} is finished.
Otherwise, in \texttt{synthesis}, each of the datasets are read in
turn and concatenated into a single dataset.  When this is finished,
all datasets except the concatenated one is removed from disk.


\begin{figure}[h!]
  \begin{center}
    \input{dataset_rehash.pdf_t}
  \end{center}
  \caption{\texttt{dataset\_rehash} data flow}
  \label{fig:dataset_rehash}
\end{figure}


\clearpage
\section{dataset\_filter\_columns}
The method \texttt{dataset\_filter\_columns} is used to remove columns
from a dataset.  This method is typically run before methods that
operate on all columns of a dataset when only a subset of the columns
are required.  A typical example is \texttt{dataset\_rehash} that
operates on all columns of a dataset.  If a subset such as only one of
the columns is to be rehashed, they can be cut out using
\texttt{dataset\_filter\_columns}.

Note that no data is copied using this method.  It only updates soft
links.  So execution is typically significantly below a second.





\clearpage
\section{dataset\_sort}
The method \texttt{dataset\_sort} is used to sort relatively large
datasets.  One or more columns may be selected for sorting, and it
will sort one column at a time.

The method works by reading the columns to sort by, and create an
indexing column that stipulates the sorting order.  Each column is
then read in turn and sorted according to the sorting column.

The method has limited sorting capability.  Internally, it sorts one
column at a time, and it requires to hold that complete column plus an
indexing column in memory simultaneously.

\subsection{Inputs}

\noindent Options\\

\begin{tabular}{ll}
  \texttt{sort\_columns'} & \\    %       : [OptionString],
  \texttt{sort\_order} & \\        %     : OrderEnum.ascending,
  \texttt{sort\_across\_slices} & \\%     : False, # normally only sort within slices
\end{tabular}\\

\noindent Datasets\\

\begin{tabular}{ll}
  \texttt{source} & \\
  \texttt{previous} & \\
\end{tabular}

Data is sorted based on the columns specified in the list
\texttt{sort\_columns}.  Data is first sorted on the first column,
then the second, and so on.





\clearpage
\section{dataset\_datesplit, dataset\_datesplit\_discarded}

\clearpage
\section{dataset\_checksum, dataset\_checksum\_chain}


