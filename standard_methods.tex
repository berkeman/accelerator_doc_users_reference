%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The Accelerator is shipped with a set of common standard methods,
including methods to import, type, export, and hash partition data.
These methods are found in the method
directory \texttt{./standard\_methods}.  All methods
in \texttt{standard\_methods} are designed and tested to work on both
Python2 and Python3.


\section{\texttt{csvimport} -- Importing Data Files}

The \texttt{csvimport} method is used to import a text files into a
dataset.  The method can be chained, so any number of text files can
be connected in a dataset chain.  Input data is assumed to be in a
tabular format, i.e.\ it is composed of a number of rows, each having
the same number of columns separated by a separator token.  A common
format of this type is the Comma Separated Values (CSV) format, but
\texttt{csvimport} is much more flexible, as seen in the table of
options below.  For example, \texttt{csvimport} can handle any
separator character, skip or parse labels on the first line, and
supports advanced quote support.  It also deals with ``broken'' input
data in a predicted and user controlled way.


\subsection{Options}
\starttable
  \RP \texttt{filename} & \emph{mandatory} & Name of file to import.  The
  filename is mandatory and the file may either be a plain text file
  or a gzipped file.  It is also possible to specify a filename
  including a path.  If the path begins with a slash, it is absolute.
  Otherwise, the path is relative to the \texttt{source\_directory}
  configuration parameter specified in the configuration
  file, see section~\ref{sec:configfile}.  A relative path makes it
  possible to relocate files to a different directory without trigging
  job remake.\\[1ex]

 \RP \texttt{separator} & \texttt{,} & Field separator character.  Accepts a
 single \texttt{iso-8859-1} character.  Leave this empty to import
 each line of the input file into a single column.  \\[1ex]

 \RP \texttt{comment} & \texttt{''} & Lines beginning with this
 character are ignored.  Accepts a single \texttt{iso-8859-1}
 character, or the empty string for no comments. Commented lines are
 stored in the \texttt{skipped} dataset.\\[1ex]

\RP \texttt{newline} & \texttt{''} & Newline character.  Empty means
"\texttt{\textbackslash{}n}" or "\texttt{\textbackslash{}r\textbackslash{}n}".
Alternatively any single \texttt{iso-8859-1} character can be chosen.\\[1ex]

\RP \texttt{quotes} & \texttt{''} & Quote character.  Empty or \pyFalse means no
quotes, \pyTrue means both \texttt{'}, and \texttt{"}, any other
character means itself. \\[1ex]

\RP \texttt{labelsonfirstline} & \pyTrue & If set to
\mintinline{python}{True}, data on the first line of the file will
be used as column labels.  If \mintinline{python}{False}, labels must
be entered using the \texttt{label} option, see \texttt{labels}
below.\\[1ex]

\RP \texttt{labels} & \texttt{[]} & If \texttt{labelsonfirstline} (see
above) is set to \mintinline{python}{False}, labels must be provided
using this option.  For example \mintinline{python}{labels = ['foo',
'bar',]}.\\[1ex]

\stoptable
\vfill
\noindent\emph{Table continues on next page}


\clearpage
\noindent\emph{Table continues from previous page}
\starttable

\RP \texttt{rename} & \texttt{\{\}} & This option makes it possible to
change the column names read from the first line of the input file.
Renaming happens first.  It accepts a dictionary of type
\mintinline{python}/{old_name: new_name,}/.\\[1ex]

\RP \texttt{lineno\_label} & \texttt{''} & If set,
\texttt{lineno\_label} becomes a column containing line numbers.
Line numbers start at one (1), and corresponds to line numbers in the
input file.  \\[1ex]

\RP \texttt{discard} & \mintinline{python}/set()/ & Labels in the discard set will
not be stored in the dataset.\\[1ex]

\RP \texttt{allow\_bad} &\mintinline{python}{False} & By default, this is
set to \pyFalse and an error will be asserted if there are problems
parsing the input data, see section~\ref{sec:csvimport-bad}.  Setting it
to \pyTrue will put all ``bad'' lines together with the corresponding
line numbers into a separate dataset named \texttt{bad}.  It is
recommended to check the resulting datasets if enabling this
option!\\[1ex]

\RP \texttt{skip\_lines} & 0 & Skip this many lines at the start of the file.
This is useful for data files that starts with a header, for
example.  Skipped lines will be stored in the \texttt{skipped} dataset.\\[1ex]

\RP \texttt{compression} & 6 & Compression level for the \texttt{gzip} compressor.
\stoptable



\subsection{Datasets}

\starttable
  \RP \texttt{previous} & \mintinline{python}{None} & Previous dataset if creating a
  chain.\\
\stoptable



\subsection{Bad Lines}
\label{sec:csvimport-bad}
A line is flagged as ``bad'' for one of two reasons
\begin{itemize}
\item[] -- there is a problem with quoting, or
\item[] -- there is an incorrect number of separators.
\end{itemize}
for example
\begin{python}
  "a","b" "c"     # invalid assuming two or three comma separated columns
  "a","b"" ""c"   # valid assuming two comma separated columns
\end{python}


\subsection{Output}
The result of the \texttt{csvimport} is a dataset named
\texttt{default}.  Lines marked as ``bad'' will be stored in the
dataset \texttt{bad}, while skipped and commented lines will be stored
in the dataset \texttt{skipped}.  All columns will be of type
\mintinline{python}{bytes}.  Typically, the dataset from
\texttt{csvimport} is fed to a \texttt{dataset\_type} job for column
typing.


\subsection{Line Numbers}
A column with line numbers is always attached to the \texttt{bad}
and \texttt{skipped} datasets, and conditionally
using \texttt{lineno\_label} to the main dataset.  Line numbers start
at one (1), and always corresponds to the lines in the input dataset.
For example, if there are labels on the first line of the input file,
this line is number 1.  Any line number can thus only appear in one of
the main, \texttt{bad}, or \texttt{skipped} datasets.



\subsection{Limitations}
Each data value is limited to 16MB maximim.  However, this is just a
constant in the code that is by default set to a value that allows the
Accelerator to run on low memory platforms.  If you need to store,
say, \texttt{bytes} values larger than 16MB, please update this
constant to a larger value.



\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build(csvimport',
    options=dict(
        filename='inputfile.txt',
        separator='\0',
    )
)
\end{python}
this will import the file \texttt{inputfile.txt} assuming that there
are labels on the first line and the column separator is a null
character (\texttt{0x00}, \mintinline{python}|'\0'|).


%\subsection{Additional Information}
%The \texttt{csvimport} method will create a file \texttt{report.txt}
%in the job directory.  This report file contains information about
%number of rows read and the slice distribution.  In addition, there is
%a \texttt{default/dataset.txt} info file about columns as well.




\section{\texttt{csvimport\_zip} -- Importing \texttt{zip} Archives}
The \texttt{csvimport\_zip} method is a wrapper
around \texttt{csvimport} that is used to import files stored in
\texttt{zip} archives.  One or more files in a \texttt{zip} archive
can be imported by a call to this function, and each file will be
imported to a separate dataset.


\subsection{Options}

All options to \texttt{csvimport} are available to this method as
well, and the \texttt{filename} option is used to specify the name of
the \texttt{zip} file.

\starttable
\RP \texttt{inside\_filenames} & \texttt{\{\}}&
   Dictionary from filename in zipfile to dataset
   name, \mintinline{python}/{'filename in zip': 'dataset name',
   ...}/.  If left empty, all files will be imported to datasets with
   cleaned up names.  If there is only one file imported from the zip
   (whether specified explicitly or because the zip only contains one
   file) this will also end up as the default dataset.\\
\RP \texttt{chaining}          & \texttt{on}  &
  Can be one of \texttt{off}, \texttt{on}, \texttt{by\_filename},
  or \texttt{by\_dsname}.
  \begin{itemize}
  \item[]\texttt{off} -- Don't chain the imports.
  \item[]\texttt{on} -- Chain the imports in the order the files are in the zip file.
  \item[]\texttt{by\_filename} -- Chain in filename order.
  \item[]\texttt{by\_dsname} -- Chain in dataset name order. Since \texttt{inside\_filenames} is a dict this is your only way of controlling its order.
\end{itemize}\\
\RP \texttt{include\_re}       & \texttt{''}  & Regexp of files to include, matches anywhere.\\
\RP \texttt{exclude\_re}       & \texttt{''}  & Regexp of files to exclude, takes priority over \texttt{include\_re}.\\
\RP \texttt{strip\_dirs}       & \pyFalse     & Strip directories from filename (\texttt{a/b/c} $\rightarrow$ \texttt{c}.)\\
\stoptable

If you chain you will also get the last dataset as
the \texttt{default} dataset, to make it easy to find. Naming a
non-last dataset ``\texttt{default}'' is an error.

If you set \texttt{strip\_dirs} the filename (as used for both sorting
and naming datasets, but not when matching regexes) will not include
directories. The default is to include directories.

\subsection{Example invocation}

\begin{python}
    jid = urd.build("csvimport_zip",
        options=dict(
            filename="data_Q2_2019.zip",
            exclude_re=r"(__MACOSX|\.DS_Store)",
            chaining="by_filename",
            strip_dirs=True,
        ),
    )
\end{python}







\section{\texttt{dataset\_type} -- Typing Datasets}
The \texttt{dataset\_type} method will read a source dataset and
create a new dataset that is typed.  This is primarily used for typing
datasets created by \texttt{csvimport}, but the method can type any
column of type \texttt{bytes}, \texttt{ascii}, or \texttt{unicode}.

Default behaviour is to append new columns with typed data.  These
columns will have the same name as the untyped version of the data,
making the untyped data ``inaccessible'', even if it is still in the
dataset.  Using the \texttt{rename} options, typed columns could be
assigned a name that differs from the original columns, so that both
typed and untyped data are available simultaneously.  This brings
transparency to the typing process.  (But even if the untyped data is
``inaccessible'' in the typed dataset, it is still available from the
input dataset.)

In order to type the data, the input data is subject to parsing.  Some
datasets may contain data that is incorrect in the sense that it
causes parsing errors when typing.  Unparseable data can either be
replaced by a default value or removed from the dataset.  Since the
dataset type does not permit removal of rows, i.e.\ datasets can not
shrink, \texttt{dataset\_type} will in this situation create a new
dataset containing only the rows containing typeable data.








\subsection{Datasets}
\starttable
  \RP \texttt{source} & \textsl{mandatory} & Dataset to type.\\[1ex]
  \RP \texttt{previous} & \mintinline{python}{None} & Previous dataset if creating a chain.\\
\stoptable

\subsection{Options}
\starttable
  \RP \texttt{column2type} & \mintinline{python}/{}/ & A dictionary from column
  label to type, for example \mintinline{python}/{'movie': 'unicode:UTF-8',}/.\\[1ex]


  \RP \texttt{defaults} & \mintinline{python}/{}/& A \texttt{dict} from
  column name to default value, for example
  \mintinline{python}/{'COLNAME': value}/.  Method will fail if data
  is unconvertible unless \mintinline{python}/filter_bad = True/.\\[1ex]

  \RP \texttt{rename} & & A dictionary from old name to new name, for
  example \mintinline{python}/{'old': 'new'}/ The old name and data
  will be preserved, unless a new dataset is created ,
  and the column with the new name will contain the typed
  data. \\[1ex]

  \RP \texttt{caption} & \textsl{empty string} & A caption.\\[4ex]

  \RP \texttt{discard\_untyped} & \mintinline{python}{None}& Force creation of new dataset.\\[1ex]

  \RP \texttt{filter\_bad} & \pyFalse &  remove rows containing untypeable data.  Will create new dataset.\\[1ex]

  \RP \texttt{numeric\_comma} & \mintinline{python}{False}& If
  \mintinline{python}{True}, write decimal number as ``\texttt{3,14}'' instead of
  default ``\texttt{3.14}''.\\[1ex]
\stoptable
  




\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build('dataset_type',
    datasets=dict(
        source=...,
        previous=...,
    ),
    options=dict(
        column2type=dict(
            auct_start_dt='datetime:%Y-%m-%d',
            brand='json',
            item_id='number',
            comp='unicode:utf-8',
        ),
    )
)
\end{python}



\subsection{Typing}
This section describes all typing possibilities in detail.  Default
behaviour when typing numbers (i.e.\ \texttt{float}s, \texttt{int}s,
and \texttt{number}s) is that any number of whitespaces before and
after the actual number are silently discarded.



\subsubsection{Numbers}
The \texttt{number} type is integer or floating point.
\starttablenotitle
\RPnotitle   \texttt{number}    && \texttt{int} or \texttt{float} \\
\RPnotitle  \texttt{number:int} && \texttt{int}, will convert \texttt{float}s to \texttt{int}s.\\
\stoptablenotitle
\noindent Integers are enforced using \texttt{number:int}, and the type accepts
trailing decimal zeroes like \texttt{7.0}, \texttt{4.000} etc.  This
is useful when typing datafiles where numbers actually are integers
but have trailing zero decimals.


\subsubsection{Floating Point Numbers}
Floating point numbers may be stored as 32 or 64 bits.  In addition,
there are six parsing options that are useful in different scenarios.
The \emph{ignore} option ignores any trailing characters after the
number.  Then there are \emph{exact} that causes error if the number
does not fit, and \emph{saturate} that silently saturates a
non-fitting number.  These can also be used in combination, see table
below for all alternatives
\starttablenotitle
\RPnotitle \texttt{float32} & \texttt{float64} & \emph{default}\\
\RPnotitle \texttt{float32i} & \texttt{float64i} & \emph{ignore}, will discard trailing garbage\\
\RPnotitle \texttt{float32e} & \texttt{float64e} & \emph{exact}, error if parsed number does not fit in type \\
\RPnotitle \texttt{float32s} & \texttt{float64s} & \emph{saturate}, saturate to min/max if number does not fit in type \\
\RPnotitle \texttt{float32ei} & \texttt{float64ei} & \emph{exact} + \emph{ignore} \\
\RPnotitle \texttt{float32si} & \texttt{float64si} & \emph{saturate} + \emph{ignore} \\
\stoptablenotitle



\subsubsection{Integers}
Integers are stored as either 32 or 64 bits.  Parsing takes base into
account, so in addition to decimal numbers, it is also straightforward
to parse octal and hexadecimal numbers.  The \emph{ignore} option
causes parsing to ignore trailing garbage characters.
\starttablenotitle
\RPnotitle   \texttt{int32\_0}   & \texttt{int64\_0}   & \emph{auto}, avoid and use a deterministic type if possible \\
\RPnotitle   \texttt{int32\_0i}  & \texttt{int64\_0i}  & \emph{auto}, ignore trailing garbage \\
\RPnotitle   \texttt{int32\_8}   & \texttt{int64\_8}   & \emph{octal} \\
\RPnotitle   \texttt{int32\_8i}  & \texttt{int64\_8i}  & \emph{octal}, ignore trailing garbage \\
\RPnotitle   \texttt{int32\_10}  & \texttt{int64\_10}  & \emph{decimal} \\
\RPnotitle   \texttt{int32\_10i} & \texttt{int64\_10i} & \emph{decimal}, ignore trailing garbage \\
\RPnotitle   \texttt{int32\_16}  & \texttt{int64\_16}  & \emph{hexadecimal} \\
\RPnotitle   \texttt{int32\_16i} & \texttt{int64\_16i} & \emph{hexadecimal}, ignore trailing garbage \\
\stoptablenotitle




\subsubsection{Integers Stored as Floats}
There are also a parsing options for integers that are represented in
a floating point format in the source data.  This is useful if integer
data is stored with decimals, such as \texttt{5.0}.  In pseudocode,
the parsing basically runs \texttt{int(float(value))} for each such
value.
\starttablenotitle
\RPnotitle   \texttt{floatint32e} & \texttt{floatint64e}  & \emph{exact}, error if parsed number does not fit in type\\
\RPnotitle   \texttt{floatint32s} & \texttt{floatint64s}  & \emph{saturate}, saturate to min/max if number does not fit in type\\
\RPnotitle   \texttt{floatint32ei}& \texttt{floatint64ei} & \emph{exact} + \emph{ignore}\\
\RPnotitle   \texttt{floatint32si}& \texttt{floatint64si} & \emph{saturate} + \emph{ignore}\\
\stoptablenotitle



\subsubsection{Convert to Boolean}
It is common that a column holds values that are to be interpreted as
either \texttt{False} or \texttt{True}.  The following types handles
strings and floats.
\starttablenotitle
\RPnotitle  \texttt{strbool} && \mintinline{python}{False} if value in
  (\pyFalse, \texttt{0}, \texttt{f}, \texttt{no}, \texttt{off},
  \texttt{nil},\\
\RPnotitle && \texttt{null}, ````)\\ && \mintinline{python}{True} otherwise\\

\RPnotitle  \texttt{floatbool} && \mintinline{python}{True} when float has
  bits set. Is \mintinline{python}{False} otherwise.\\

\RPnotitle  \texttt{floatbooli} && same + \emph{ignore}\\
\stoptablenotitle



\subsubsection{Time and Date}
There are three types relating to time available, \texttt{date},
\texttt{time}, and \texttt{datetime}.  Each of these has a
corresponding version that ignores trailing garbage characters.
All time types require a format specification as described below\\
\starttablenotitle
\RPnotitle  \texttt{date:*}      && a date with format specifier\\
\RPnotitle    \texttt{datei:*}     && same + \emph{ignore}\\
\RPnotitle    \texttt{time:*}      && a time with format specifier\\
\RPnotitle    \texttt{timei:*}     && same + \emph{ignore}\\
\RPnotitle    \texttt{datetime:*}  && a date + time with format specifier\\
\RPnotitle    \texttt{datetimei:*} && same + \emph{ignore}\\
\stoptablenotitle
\noindent The format is standard Python time formats, like shown in these examples
\begin{python}
# will match for example '2017-03-22'
auct_start_dt='date:%Y-%m-%d'  
# will match for example '183000', i.e. half past six in the evening
tod='time:%H%M%S'
# will match for example '2017-03-22 18:30:15'
timestamp='datetime:'%Y-%m-%d %H:%M:%S'
\end{python}


\subsubsection{Strings and Byte Sequences}
There are a number of ways to read string and byte data, depending on
how the raw input data is to be interpreted.  The basic types are
shown first, and the more advanced variations and options will be
described below.
\starttablenotitle
\RPnotitle  \texttt{bytes}      && list of bytes\\
\RPnotitle    \texttt{bytesstrip} && list of bytes, strip characters 8-13, 32 from start and end\\
\RPnotitle    \texttt{ascii}      && list of ASCII characters\\
\RPnotitle    \texttt{asciistrip} && list of ASCII characters, strip characters 8-13, 32 from start and end\\
\stoptablenotitle

\noindent When typing to unicode and ASCII, there are several ways to handle
individual unparsable characters.  For unicode, there are two types,
\starttablenotitle
\RPnotitle  \texttt{unicode:*}  && list of unicode characters\\
\RPnotitle    \texttt{unicodestrip:*} && list of unicode characters, strip
  characters 8-13, 32 from start and end\\
\stoptablenotitle
\noindent The asterisk represents options that take the form
\begin{python}
"codec" #or
"codec/errors"
\end{python}
\texttt{unicode:codec/errors} will read bytes encoded in
\texttt{codec} and write "unicode" (which is stored as utf-8, but
that's invisible to the Python side).  \texttt{codec} is often
\texttt{utf-8}, but could be for example \texttt{utf-8},
\texttt{ascii}, \texttt{iso-8859-1}, \texttt{iso-8859-15},
\texttt{cp437}, or \texttt{windows-1252} etc.  See the Python
documentation
\begin{center}
  \texttt{https://docs.python.org/2/library/codecs.html\#standard-encodings}
\end{center}
for more information.  The \texttt{errors} part is optional, and
can be one of\\[1ex]

\begin{tabular}{p{2cm}p{10cm}}
  \texttt{strict} &The default, an error marks this row as bad\\[1ex]
  \texttt{ignore} & All unparsable bytes are discarded.\\[1ex]
  \texttt{replace} & All unparsable bytes are replaced by the unicode
  replacement character ("\texttt{\textbackslash ufffd}").\\[1ex]
\end{tabular}\\[1ex]

\noindent Using \texttt{strict} will cause errors if unparsable.  For
example, typing the string "\texttt{ab\textbackslash xffc}" will give
an error (\texttt{strict}), "\texttt{abc}" (ignore), or
"\texttt{ab\textbackslash ufffdc}" (replace).  \texttt{strip} will
happen before \texttt{ignore}.



ASCII is similar, there are two types
\starttablenotitle
\RPnotitle \texttt{ascii:*}      && list of ASCII characters\\
\RPnotitle \texttt{asciistrip:*} && list of ASCII characters, strip characters 8-13,32 from start and end\\
\stoptablenotitle

\noindent where the argument is one of\\[1ex]

\begin{tabular}{p{2cm}p{10cm}}
  \texttt{strict} & The default, an error marks this row as bad\\[1ex]
  \texttt{ignore} & All unparsable bytes are discarded\\[1ex]
  \texttt{replace} & All unparsable bytes are replaced by an octal
  escapes "\texttt{\textbackslash ooo}"\\[1ex]
  \texttt{encode} & Like replace except "\texttt{\textbackslash}" is also replaced by
  "\texttt{\textbackslash 134}" (for full reversibility).\\[1ex]
\end{tabular}\\[1ex]
\noindent Using \texttt{strict} will cause errors if unparsable.
\texttt{strip} will happen before \texttt{ignore}.



\section{\texttt{csvexport} -- Exporting Text Files}

The \texttt{dataset\_export} method is used to export datasets to
column based text files (CSV, Comma Separated Values).  It can export
plain files and gzip-compressed files, export a chain of datasets,
export one output file per slice, and more.  Read the Options section
for full details.

\subsection*{Options}
\starttable

  \RP \texttt{filename} & \textsl{mandatory} & Name of output file.
  File will by default be stored in the job's job directory.  The
  filename has to end with ``\texttt{.csv}'' for plain text files, and
  ``\texttt{.gz}'' for gzipped output.\\[1ex]
                                                                                                                                %          .gz
  \RP \texttt{separator} & ',' & Column separator.\\[1ex]
  
  \RP \texttt{labelsonfirstline} & \mintinline{python}{True} & If
  \mintinline{python}/True/, write column names on first row.\\[1ex]
  
  \RP \texttt{chain\_source} & \mintinline{python}{False} & If
  \mintinline{python}/True/, read a dataset chain from
  \mintinline{python}/datasets.source/ back to
  \mintinline{python}/jobids.previous/\\[1ex]

  \RP \texttt{quote\_fields} & \textsl{empty string} & Export quoted fields.  Must be empty
  (no quote character, default), ``\texttt{'}``, or ``\texttt{"}``.\\[1ex]
  
  \RP \texttt{labels} & \mintinline{python}/[]/ & Specify which labels to
  export.  An empty list corresponds to all labels in dataset.\\[1ex]
  
  \RP \texttt{sliced} & \mintinline{python}/False/ & Each slice is
  exported in a separate file when \mintinline{python}/True/.  If so,
  use \mintinline{python}/"%02d"/ or similar in filename as
  placeholder for the slice number.\\[1ex]
\stoptable

\subsection*{Datasets}
\starttable
  \RP \mintinline{python}{[source,]} & \textsl{mandatory} & A single
  dataset or a \textsl{list} of datasets.\\[1ex]
\stoptable


\subsection*{Jobids}
\starttable
  \texttt{previous} & \mintinline{python}/None/ & Jobid to
  previous \texttt{csvexport} if chained.\\[1ex]
\stoptable




\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build(csvexport',
    datasets=dict(
        source='test-3/foo',
    ),
    options=dict(
        filename='output.txt.gz',
        separator=' ',
        quote_fields="\'",
        ),
    )
)
\end{python}












\section{\texttt{dataset\_rehash} -- Hash Partition a Dataset}
\label{sec:dataset_rehash}

The \texttt{dataset\_rehash} method will create a new dataset based on
its \texttt{source} dataset.  The new dataset will be hashed on a
column specified in the options.

\subsection*{Options}
\starttable
  \RP \texttt{hashlabel} & \textsl{mandatory} & column for hashing,
  required.  Note that columns typed as \texttt{list}, \texttt{set},
  or \texttt{json} cannot be used for hashing.\\[1ex]

  \RP \texttt{length} & \mintinline{python}/-1/ & Go back at most this
  many datasets in a chain.  Default is -1, which goes back to
  previous.source if it exists, or to the first dataset in the chain
  otherwise.\\[1ex]

  \RP \texttt{caption} & & Optional caption.  A
  reasonable caption is created automatically if left blank\\[1ex]

  \texttt{as\_chain} & \pyFalse & True generates one dataset per slice, False
  generates one dataset.  Default \texttt{False}.\\
\stoptable


\subsection*{Datasets}

\starttable
  \RP \texttt{source} & \textsl{mandatory} & Source dataset to rehash\\[1ex]
  \RP \texttt{previous} & \mintinline{python}/None/ & Previous dataset to chain  to.\\[1ex]
\stoptable




\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build('dataset_rehash',
          datasets=dict(source=jid,), 
          options=dict(hashlabel='start_date',))
\end{python}



\subsection{Hashing Details}
This method will create a new dataset based on all the data in the
source dataset.  The difference between input and output is in which
slices the rows will be stored.  For each row, the target slice is
determined based on the output value of a hashing function applied to
a certain column (the \texttt{hashlabel}) of that row.  To illustrate
the operation, the code is similar to
\begin{python}
from accelerator.gzutil import siphash24

target_sliceno = siphash24(cols[hashlabel]) % params.slices
\end{python}

\subsection{Notes on Chains}

\begin{enumerate}
  \item[1.]  The default operation is to rehash a complete chain of
    datasets from \texttt{source} back to \texttt{previous.source}.
    This is controlled by the \texttt{length} option.

  \item[2.]  Internally, \texttt{dataset\_rehash} always generates one
    dataset per slice in a chain.  This is also what is returned if
    \mintinline{python}/as_chain == True/.  Otherwise, all datasets
    will be concatenated into one.  Thus, there is a choice of either
    having the output as a chain of datasets -- or as a single dataset.
    The chain will execute faster, since the concatenation step is
    omitted.
\end{enumerate}

%% \clearpage
%% \subsection{Internal Operation}
%% Figure~\ref{fig:dataset_rehash} shows how rehashing happens...

%% Each \texttt{analysis} reads one complete slice of the source dataset
%% and hashes it into a new dataset.  This means that there is one new
%% dataset created per slice.  Each of these datasets are chained and
%% named using an integer counter starting at zero.  The last slice,
%% however, is named \texttt{default}.

%% If the \texttt{as\_chain} is set, \texttt{dataset\_rehash} is finished.
%% Otherwise, in \texttt{synthesis}, each of the datasets are read in
%% turn and concatenated into a single dataset.  When this is finished,
%% all datasets except the concatenated one is removed from disk.


%% \begin{figure}[h!]
%%   \begin{center}
%%     \input{dataset_rehash.pdf_t}
%%   \end{center}
%%   \caption{\texttt{dataset\_rehash} data flow}
%%   \label{fig:dataset_rehash}
%% \end{figure}



\section{\texttt{dataset\_filter\_columns} -- Removing Columns from a Dataset}

The \texttt{dataset\_rehash} method removes columns from a dataset.
It is typically run before applying methods that operate on all
columns of a dataset and only a subset of the columns are required.  A
typical example is \texttt{dataset\_rehash} that operates on all
columns of a dataset.  If not all columns are needed, time and storage
can be saved by removing columns using this method prior to applying
\texttt{dataset\_rehash}.

Note that this method only updates soft links, and no data is actually
copied.  So execution time is typically a fraction of a second and no
redundant data is written to disk.

\subsection*{Options}
\starttable
  \RP \texttt{columns} & \mintinline{python}/[]/ & A list of columns to
  keep.\\[1ex]
\stoptable






\section{\texttt{dataset\_sort} -- Sorting a Dataset}
The method \texttt{dataset\_sort} is used to sort relatively large
datasets.  One or more columns may be selected for sorting, and it
will sort one column at a time.  The sorting algorithm is stable,
meaning that things with equal sorting keys will keep their order.



\subsection*{Options}

\starttable
\RP \texttt{sort\_columns} & \textsl{mandatory} & A column or a list of
  columns.  If a list is specified, sorting will be carried out from left
  to right.\\[1ex]

  \RP \texttt{sort\_order} & \texttt{ascending} & Could be reversed by
  specifying \texttt{descending}\\[1ex]
  
  \RP \texttt{sort\_across\_slices} & \mintinline{python}/False/& If
  \mintinline{python}/False/, only sort within slices.  Otherwise sort
  across slices.\\[1ex]
\stoptable


\subsection*{Datasets}
\starttable
\RP \texttt{source} & \textsl{mandatory}& A dataset to sort.\\[1ex]
\RP \texttt{previous} & \mintinline{python}/None/ & A previous dataset to chain to.\\[1ex]
\stoptable


\subsection{Sorting \pyNone and \mintinline{python}|NaN| values}
The special values \mintinline{python}|None| and \mintinline{python}|NaN| follow these rules
\begin{itemize}
\item \mintinline{python}|NaN| will sort same as \mintinline{python}|+Inf|, i.e.\ \textsl{last}.
\item \pyNone sorts as \mintinline{python}|-Inf|, i.e.\ \textsl{first} in \texttt{float} columns.  Intermingled \pyNone and \mintinline{python}|-Inf| will keep their original order due to the stable sorting algorithm.
\item \pyNone sorts \textsl{last} in \texttt{date}, \texttt{time}, and \texttt{datetime} columns.
\item For all other types, \pyNone sorts \textsl{first}.
\end{itemize}



\subsection*{Spreading of Left Over Values}
If the number of rows in a dataset is not even divisible by the number
of slices, some slices will have one more row than others.  Instead of
putting this data in, say, the first slices, \texttt{dataset\_sort}
attempts to even out any bias by selecting the slices that get the
additional data row in a pseudo-random manner.  In order to have the
sorting stable, selection of slices is based on the first values of
the sorting column.  It is not perfect, if the data is already sorted
the first slices will be picked, but it is stable, which is the most
important thing.





\subsection{A Practical Limitation}
Internally, the method works by reading the columns to sort by, and
create an indexing column that stipulates the sorting order.  Each
column is then read in turn and sorted according to the sorting
column.

Therefore, the method has limited sorting capability.  Internally, it
sorts one column at a time, and it needs to hold that complete column
plus an indexing column in memory simultaneously.  Still, a standard
computer can sort very large datasets without trouble.






\section{dataset\_checksum, dataset\_checksum\_chain}

The \texttt{dataset\_checksum} method is used to create a single
checksum from a dataset based on one or more columns.  The chained
version returns a single checksum from a dataset chain.  It is mainly
intended as a debugging aid, enabling comparison of datasets across
machines, even if they have different slicing.

If \mintinline{python}|options.sort=False|, hashing will depend on the
actual row order of the dataset.  If, on the other
hand, \mintinline{python}|options.sort=True|, hashing will
be \textsl{slice invariant} and \textsl{row order invariant}, meaning
that the methods only look at the contents of the dataset(s).

Chain limits will affect the checksum of a chain, so if checksumming
two chains containing the same data, but with different number of
chained datasets, their checksums will differ.

Note that sorting uses about 64 bytes per row, upper limiting the size
of hashable datasets.  This corresponds to about 1GB of RAM per 20
million lines or so.


\subsection*{Options}

\starttable
\RP \texttt{columns} & \texttt{set()} & A set of columns to base the checksum on.  Leave blank for all columns\\[1ex]

\RP \texttt{sort} & \texttt{True} & Sort dataset before hashing, see text. \\[1ex]

\RP \texttt{chain\_length} & \texttt{-1} & Number of datasets in chain to hash. \\[1ex]
\stoptable


\subsection*{Datasets}
\starttable
\RP \texttt{source} & \textsl{mandatory}& A dataset to sort.\\[1ex]
\RP \texttt{stop} & chained version only& Stop hashing at this dataset.\\[1ex]
\stoptable

