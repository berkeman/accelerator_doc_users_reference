%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\texttt{csvimport} -- Importing Data Files}

This method is used to import a text file in tabular format (CSV,
Comma Separated Values) into a dataset.  A wide range of data formats
is supported, and the method reads plain text files as well as
\texttt{gzip} compressed files.  A number of options are available to
customise importing to specific cases.


\subsection{Options}

%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
\starttable
  \RP \texttt{filename} & \emph{mandatory} & Name of file to import.  The
  filename is mandatory and the file may either be a plain text file
  or a gzipped file.  It is also possible to specify a filename
  including a path.  If the path begins with a slash, it is absolute.
  Otherwise, the path is relative to the \texttt{source\_directory}
  configuration parameter specified in the configuration
  file~\ref{void:configuration_file}.  A relative path makes it
  possible to relocate files to a different directory without trigging
  job remake.\\
  
  \RP \texttt{separator} & ``,'' & Field separator.  Any character, except
  ``\verb$\0$'', ``\verb$\n$'', and ``\verb$\r$'' will work.\\

  
  \RP \texttt{labelsonfirstline} & \texttt{True} & If set to
  \mintinline{python}{True}, data on the first line of the file will be used as
  column labels.  If \mintinline{python}{False}, labels must be entered using the
  \texttt{label} option, see \texttt{labels} below.\\

  \RP \texttt{labels} & \mintinline{python}{[]} & If \texttt{labelsonfirstline} (see
  above) is set to \mintinline{python}{False}, labels must be provided using this
  option.  For example \mintinline{python}{labels = ['foo', 'bar',]}.\\

  \RP \texttt{hashlabel} & \mintinline{python}{None} & If not specified, the dataset
  will be created ``round-robin'', so that rows in the input file will
  be separated evenly into the dataset slices.  If a valid label is
  specified, each input data row will be allocated to a slice
  depending on the output of a hash function applied to the columns
  data.  \textsl{Note that typing of a dataset typically changes how
    the data is represented, which most likely voids hashing.  Use
    this option only for datasets that will not be typed.  Otherwise,
    use \texttt{dataset\_rehash} after typing.}\\

  \RP \texttt{quote\_support} & \mintinline{python}{False} & If set to \mintinline{python}{True},
  it is possible to read CSV files with quoted values, such as
  \mintinline{python}{'foo'} and \mintinline{python}{"bar"}.\\

  \RP \texttt{rename} & \mintinline{python}/{}/ & This option makes it possible to
  change the column names read from the first line of the input file.
  Renaming happens first.  It accepts a dictionary of type
  \mintinline{python}/{old_name: new_name,}/.\\

  \RP \texttt{discard} & \mintinline{python}/set()/ & labels in the discard set will
  not be stored in the dataset.\\
  
  \RP \texttt{allow\_bad} & \mintinline{python}{False} & Relaxes the input file
  strictness if set to \mintinline{python}{True}.  If set to \mintinline{python}{False}, which
  is the default, \texttt{csvimport} will assert an error if there are
  format errors in the input data.  Setting it to \mintinline{python}{True} makes
  importing silently ignoring any format errors.  It is recommended to
  check the resulting dataset if this option is enabled.\\
%\end{tabular}
\stoptable

\subsection{Datasets}

%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
\starttable
  \RP \texttt{previous} & \mintinline{python}{None} & Previous dataset if creating a
  chain.\\
\stoptable
  %\end{tabular}

\subsection{Output}
The result of the \texttt{csvimport} is a dataset named
\texttt{default}.  All columns will be of type \mintinline{python}{bytes}.

%% \subsection{Extra}
%% The method will create a file \texttt{report.txt}, containing
%% information about number of rows read and slice distribution, in the
%% job directory.  In addition, there is a \texttt{default/dataset.txt}
%% info file about columns as well.






\clearpage
\section{\texttt{dataset\_type} -- Typing Datasets}
This method will read a source dataset and create a new dataset that
is typed.  This is useful for typing datasets created by
\texttt{csvimport}.

Default behaviour is to append new columns with typed data.  These
columns will have the same name as the untyped version of the data,
making the untyped data ``inaccessible'', even if it is still in the
dataset.  Using the \texttt{rename} options, typed columns could be
assigned a name that differs from the original columns, so that both
typed and untyped data is available simultaneously.  This brings
transparency to the typing process.

Some datasets may have contain data that is incorrect in the sense
that it causes parsing errors when typing.  Unparseable data could
either be replaced by a default value or removed from the dataset.
Removing rows from a dataset is not possible.  Instead, the
\texttt{dataset\_type} method will create a new dataset containing
only rows with typeable data.








\subsection{Datasets}
\starttable
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
  \RP \texttt{source} & \textsl{mandatory} & Dataset to type.\\
  \RP \texttt{previous} & \mintinline{python}{None} & Previous dataset if creating a chain.\\
%\end{tabular}
\stoptable

\subsection{Options}
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
\starttable
  \RP \texttt{source} & \textsl{mandatory} & Dataset to type.\\[1ex]

  \RP \texttt{column2type} & \mintinline{python}/{}/ & A dictionary from column
  label to type, for example \mintinline{python}/{'movie': 'unicode:UTF-8',}/.\\[1ex]


  \RP \texttt{defaults} & \mintinline{python}/{}/& A \texttt{dict} from
  column name to default value, for example
  \mintinline{python}/{'COLNAME': value}/.  Method will fail if data
  is unconvertible unless \mintinline{python}/filter_bad = True/.\\[1ex]

  \RP \texttt{rename} & & A dictionary from old name to new name, for
  example \mintinline{python}/{'old': 'new'}/ The old name and data
  will be preserved, unless a new dataset is created ,
  and the column with the new name will contain the typed
  data. \\[1ex]

  \RP \texttt{caption} & \textsl{empty string} & A caption.\\[1ex]

  \RP \texttt{discard\_untyped} & \mintinline{python}{None}& Force creation of new dataset.\\[1ex]

  \RP \texttt{filter\_bad} & \pyFalse &  remove rows containing untypeable data.  Will create new dataset.\\[1ex]

  \RP \texttt{numeric\_comma} & \mintinline{python}{False}& If
  \mintinline{python}{True}, write decimal number as ``\texttt{3,14}'' instead of
  default ``\texttt{3.14}''.\\[1ex]
%\end{tabular}
\stoptable
  



\clearpage
\subsection{Example Invocation}
An example invocation is the following

\begin{python}
urd.build('dataset_type', ...,
  options=dict(
    column2type=dict(
      auct_start_dt='datetime:%Y-%m-%d',
      brand='json',
      item_id='number',
      comp='unicode:utf-8',
    ),
  )
\end{python}



\subsection{Typing}
This section describes all typing options in detail.

\subsubsection{Numbers}
The \emph{number} type is int or float.
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{9cm}}
  \texttt{number}     && \texttt{int} or \texttt{float} \\
  \texttt{number:int} && \texttt{int}, converts floats to int.\\
\end{tabular}
\end{leftbar}


\subsubsection{Floating Point Numbers}
Floating point numbers may be stored as 32 or 64 bits.  In addition,
there are six parsing options that are useful in different scenarios.
The \emph{ignore} option ignores any trailing characters after the
number.  Then there are \emph{exact} that causes error if the number
does not fit, and \emph{saturate} that silently saturates a
non-fitting number.  These can also be used in combination, see table
below for all alternatives
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{9cm}}
\texttt{float32} & \texttt{float64} & \emph{default}\\
\texttt{float32i} & \texttt{float64i} & \emph{ignore}, will discard trailing garbage\\
\texttt{float32e} & \texttt{float64e} & \emph{exact}, error if parsed number does not fit in type \\
\texttt{float32s} & \texttt{float64s} & \emph{saturate}, saturate to min/max if number does not fit in type \\
\texttt{float32ei} & \texttt{float64ei} & \emph{exact} + \emph{ignore} \\
\texttt{float32si} & \texttt{float64si} & \emph{saturate} + \emph{ignore} \\
\end{tabular}
\end{leftbar}

\subsubsection{Integers}
Integers are stored as either 32 or 64 bits.  Parsing takes base into
account, so in addition to decimal numbers, it is also straightforward
to parse octal and hexadecimal numbers.  The \emph{ignore} option
causes parsing to ignore trailing garbage characters.
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{9cm}}
  \texttt{int32\_0}   & \texttt{int64\_0}   & \emph{auto}, avoid and use a deteministic type if possible \\
  \texttt{int32\_0i}  & \texttt{int64\_0i}  & \emph{auto}, ignore trailing garbage \\
  \texttt{int32\_8}   & \texttt{int64\_8}   & \emph{octal} \\
  \texttt{int32\_8i}  & \texttt{int64\_8i}  & \emph{octal}, ignore trailing garbage \\
  \texttt{int32\_10}  & \texttt{int64\_10}  & \emph{decimal} \\
  \texttt{int32\_10i} & \texttt{int64\_10i} & \emph{decimal}, ignore trailing garbage \\
  \texttt{int32\_16}  & \texttt{int64\_16}  & \emph{hexadecimal} \\
  \texttt{int32\_16i} & \texttt{int64\_16i} & \emph{hexadecimal}, ignore trailing garbage \\
\end{tabular}
\end{leftbar}



\subsubsection{Integers Stored as Floats}
There are also a parsing options for integers that are represented in
a floating point format in the source data.  This is useful if integer
data is stored with decimals, such as \texttt{5.0}.  In pseudocode,
the parsing basically runs \texttt{int(float(value))} for each such
value.
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{9cm}}
  \texttt{floatint32e} & \texttt{floatint64e}  & \emph{exact}, error if parsed number does not fit in type\\
  \texttt{floatint32s} & \texttt{floatint64s}  & \emph{saturate}, saturate to min/max if number does not fit in type\\
  \texttt{floatint32ei}& \texttt{floatint64ei} & \emph{exact} + \emph{ignore}\\
  \texttt{floatint32si}& \texttt{floatint64si} & \emph{saturate} + \emph{ignore}\\
\end{tabular}
\end{leftbar}



\subsubsection{Convert to Boolean}
It is common that a column holds values that are to be interpreted as
either \texttt{False} or \texttt{True}.  The following types handles
strings and floats.
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{8cm}}
  \texttt{strbool} && \mintinline{python}{False} if value in
  (\pyFalse, \texttt{0}, \texttt{f}, \texttt{no}, \texttt{off},
  \texttt{nil}, \texttt{null}, ````)\\ && \mintinline{python}{True}
  otherwise\\

  \texttt{floatbool} && \mintinline{python}{True} when float has
  bits set. Is \mintinline{python}{False} otherwise.\\

  \texttt{floatbooli} && same + \emph{ignore}\\
\end{tabular}
\end{leftbar}



\subsubsection{Time and Date}
There are three types relating to time available, \texttt{date},
\texttt{time}, and \texttt{datetime}.  Each of these has a
corresponding version that ignores trailing garbage characters.
All time types require a format specification as described below\\
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{8cm}}
  \texttt{date:*}      && a date with format specifier\\
  \texttt{datei:*}     && same + \emph{ignore}\\
  \texttt{time:*}      && a time with format specifier\\
  \texttt{timei:*}     && same + \emph{ignore}\\
  \texttt{datetime:*}  && a date + time with format specifier\\
  \texttt{datetimei:*} && same + \emph{ignore}\\
\end{tabular}
\end{leftbar}
\noindent The format is standard Python time formats, like shown in these examples
\begin{python}
# will match for example '2017-03-22'
auct_start_dt='date:%Y-%m-%d'  
# will match for example '183000', i.e. half past six in the evening
tod='time:%H%M%S'
# will match for example '2017-03-22 18:30:15'
timestamp='datetime:'%Y-%m-%d %H:%M:%S'
\end{python}


\subsubsection{Strings and Byte Sequences}
There are a number of ways to read string and byte data, depending on
how the raw input data is to be interpreted.  The basic types are
shown first, and the more advanced variations and options will be
described below.
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{8cm}}
  \texttt{bytes}      && list of bytes\\
  \texttt{bytesstrip} && list of bytes, strip characters 8-13,32 from start and end\\
  \texttt{ascii}      && list of ascii characters\\
  \texttt{asciistrip} && list of ascii characters, strip characters 8-13,32 from start and end\\
\end{tabular}
\end{leftbar}

When typing to unicode and ascii, there are several ways to handle
individual unparsable characters.  For unicode, there are two types,
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{8cm}}
  \texttt{unicode:*}  && list of unicode characters\\
  \texttt{unicodestrip:*} && list of unicode characters, strip
  characters 8-13,32 from start and end\\
\end{tabular}
\end{leftbar}
The asterisk represents options that take the form
\begin{python}
"codec" #or
"codec/errors"
\end{python}
\texttt{unicode:codec/errors} will read bytes encoded in
\texttt{codec} and write "unicode" (which is stored as utf-8, but
that's invisible to the Python side).  \texttt{codec} is often
\texttt{utf-8}, but see Python documentation on \texttt{bytes.decode}
for more information.  The \texttt{errors} part is optional, and can
be one of
\begin{snugshade}
\begin{tabular}{p{2cm}p{10cm}}
  \texttt{strict} &The default, an error marks this row as bad\\[1ex]
  \texttt{ignore} & All unparsable bytes are discarded.\\[1ex]
  \texttt{replace} & All unparsable bytes are replaced by the unicode
  replacement character ("\texttt{\textbackslash ufffd}").\\[1ex]
\end{tabular}
\end{snugshade}
\noindent Using \texttt{strict} will cause errors if unparsable.
Example.  Typing the string "\texttt{ab\textbackslash xffc}" will give
an error (\texttt{strict}), "\texttt{abc}" (ignore), or
"\texttt{ab\textbackslash ufffdc}" (replace).



Ascii is similar, there are two types
\begin{leftbar}
\begin{tabular}{p{2cm}p{2cm}p{8cm}}
  \texttt{ascii:*}      && list of ascii characters\\
  \texttt{asciistrip:*} && list of ascii characters, strip characters 8-13,32 from start and end\\
\end{tabular}
\end{leftbar}
where the argument is one of
\begin{snugshade}
\begin{tabular}{p{2cm}p{10cm}}
  \texttt{strict} & The default, an error marks this row as bad\\[1ex]
  \texttt{ignore} & All unparsable bytes are discarded\\[1ex]
  \texttt{replace} & All unparsable bytes are replaced by an octal
  escapes "\texttt{\textbackslash ooo}"\\[1ex]
  \texttt{encode} & Like replace except "\texttt{\textbackslash}" is also replaced by
  "\texttt{\textbackslash 134}" (for full reversability).\\[1ex]
\end{tabular}
\end{snugshade}
\noindent Using \texttt{strict} will cause errors if unparsable.




\clearpage
\section{\texttt{csvexport} -- Exporting Text Files}

The \texttt{dataset\_export} metod is used to export datasets to
column based text files (CSV, Comma Separated Values).  It can export
plain files and gzip-compressed files, export a chain of datasets,
export one output file per slice, and more.  Read the Options section
for full details.

\subsection*{Options}
\starttable
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}\hline
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]

  \RP \texttt{filename} & \textsl{mandatory} & Name of output file.
  File will by default be stored in the job's job directory.  The
  filename has to end with ``\texttt{.csv}'' for plain text files, and
  ``\texttt{.gz}'' for gzipped output.\\[1ex]
                                                                                                                                %          .gz
  \RP \texttt{separator} & ',' & Column separator.\\[1ex]
  
  \RP \texttt{labelsonfirstline} & \mintinline{python}{True} & If
  \mintinline{python}/True/, write column names on first row.\\[1ex]
  
  \RP \texttt{chain\_source} & \mintinline{python}{False} & If
  \mintinline{python}/True/, read a dataset chain from
  \mintinline{python}/datasets.source/ back to
  \mintinline{python}/jobids.previous/\\[1ex]

  \RP \texttt{quote\_fields} & \textsl{empty string} & Export quoted fields.  Must be empty
  (no quote character, default), ``\texttt{'}``, or ``\texttt{"}``.\\[1ex]
  
  \RP \texttt{labels} & \mintinline{python}/[]/ & Specify which labels to
  export.  An empty list corresponds to all labels in dataset.\\[1ex]
  
  \RP \texttt{sliced} & \mintinline{python}/False/ & Each slice is
  exported in a separate file when \mintinline{python}/True/.  If so,
  use \mintinline{python}/"%02d"/ or similar in filename as
  placeholder for the slice number.\\[1ex]
%\end{tabular}
\stoptable

\subsection*{Datasets}
\starttable
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}\hline
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
  \RP \mintinline{python}{[source,]} & \textsl{mandatory} & A single
  dataset or a \textsl{list} of datasets.\\[1ex]
\stoptable
%\end{tabular}


\subsection*{Jobids}
\starttable
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}\hline
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
  \texttt{previous} & \mintinline{python}/None/ & Jobid to
  previous \texttt{csvexport} if chained.\\[1ex]
%\end{tabular}
\stoptable
















\clearpage
\section{\texttt{dataset\_rehash} -- Hash Partition a Dataset}

Dataset rehash will create a new dataset based on its \texttt{source}
dataset.  The new dataset will be hashed on a column specified in the
options.

\subsection*{Options}
%\begin{snugshade}
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{8.5cm}}
%\\
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]\hline\\[1ex]
\starttable
  \RP \texttt{hashlabel} & \textsl{mandatory} & column for hashing,
  required.  Note that columns typed as \texttt{list}, \texttt{set},
  or \texttt{json} cannot be used for hashing.\\

  \RP \texttt{length} & \mintinline{python}/-1/ & Go back at most this
  many datasets in a chain.  Default is -1, which goes back to
  previous.source if it exists, or to the first dataset in the chain
  otherwise.\\

  \RP \texttt{caption} & & Optional caption.  A
  reasonable caption is created automatically if left blank\\

  \texttt{as\_chain} & \pyFalse & True generates one dataset per slice, False
  generates one dataset.  Default \texttt{False}.\\
%\\\end{tabular}
%\end{snugshade}
\stoptable


\subsection*{Datasets}

\starttable
%\begin{snugshade}
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{8.5cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]\hline\\[1ex]
  \RP \texttt{source} & \textsl{mandatory} & Source dataset to rehash\\[1ex]
  \RP \texttt{previous} & \mintinline{python}/None/ & Previous dataset to chain  to.\\[1ex]
%\end{tabular}
%\end{snugshade}
\stoptable




%% \subsection{Example Invocation}
%% \begin{python}
%% urd.build('dataset_rehash',
%%   datasets=dict(
%%     source=jid,
%%   ), 
%%   options=dict(
%%     hashlabel = 'start_date',
%%   )
%% )
%% \end{python}



\subsection{Hashing Details}
This method will create a new dataset based on all the data in the
source dataset.  The difference between input and output is in which
slices the rows will be stored.  For each row, the target slice is
determined based on the output value of a hashing function applied to
a certain column (the \texttt{hashlabel}) of that row.  In code, the
operation is similar to
\begin{python}
from gzutil import siphash
target_sliceno = siphash(cols[hashlabel]) % params.slices
\end{python}

\subsection{Notes on Chains}

\begin{enumerate}
  \item[1.]  The default operation is to rehash a complete chain of
    datasets from \texttt{source} back to \texttt{previous.source}.
    This is controlled by the \texttt{length} option.

  \item[2.]  Internally, \texttt{dataset\_rehash} always generates one
    dataset per slice in a chain.  This is also what is returned if
    \mintinline{python}/as\_chain == True/.  Otherwise, all datasets
    will be concatenated into one.  Thus, there is a choice of either
    having the output as a chain of datasets or as a single dataset.
    The chain will execute faster, since the concatenation step is
    omitted.
\end{enumerate}

%% \clearpage
%% \subsection{Internal Operation}
%% Figure~\ref{fig:dataset_rehash} shows how rehashing happens...

%% Each \texttt{analysis} reads one complete slice of the source dataset
%% and hashes it into a new dataset.  This means that there is one new
%% dataset created per slice.  Each of these datasets are chained and
%% named using an integer counter starting at zero.  The last slice,
%% however, is named \texttt{default}.

%% If the \texttt{as\_chain} is set, \texttt{dataset\_rehash} is finished.
%% Otherwise, in \texttt{synthesis}, each of the datasets are read in
%% turn and concatenated into a single dataset.  When this is finished,
%% all datasets except the concatenated one is removed from disk.


%% \begin{figure}[h!]
%%   \begin{center}
%%     \input{dataset_rehash.pdf_t}
%%   \end{center}
%%   \caption{\texttt{dataset\_rehash} data flow}
%%   \label{fig:dataset_rehash}
%% \end{figure}


\clearpage
\section{\texttt{dataset\_filter\_columns} -- Removing Columns from a Dataset}

This method removes columns from a dataset.  It is typically run
before applying methods that operate on all columns of a dataset when
only a subset of the columns are required.  A typical example is
\texttt{dataset\_rehash} that operates on all columns of a dataset.
If not all columns are needed, time can be saved by removing columns
using this method prior to applying \texttt{dataset\_rehash}.

Note that this method only updates soft links, and no data is actually
copied.  So execution is typically significantly below a second and no
redundant data is written to disk.

\subsection*{Options}
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}\hline
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
\starttable
  \RP \texttt{columns} & \mintinline{python}/[]/ & A list of columns to
  keep.\\[1ex]
%\end{tabular}
\stoptable





\clearpage
\section{\texttt{dataset\_sort} -- Sorting a Dataset}
The method \texttt{dataset\_sort} is used to sort relatively large
datasets.  One or more columns may be selected for sorting, and it
will sort one column at a time.

\subsection*{Options}

%\fbox{
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
%  \hline\\[1ex]
\starttable
\RP \texttt{sort\_columns'} & \textsl{mandatory} & A column or a list of
  columns.  If a list is specified, sorting will be carried out left
  to right.\\[1ex]

  \RP \texttt{sort\_order} & \texttt{ascending} & Could be reversed by
  specifying \texttt{descending}\\[1ex]
  
  \RP \texttt{sort\_across\_slices} & \mintinline{python}/False/& If
  \mintinline{python}/False/, only sort within slices.  Otherwise sort
  across slices.\\[1ex]
%\end{tabular}
%}
\stoptable


\subsection*{Datasets}
%\fbox{
%\begin{tabular}{ p{3.2cm} p{1.8cm} p{10cm}}
%  \textsl{name} & \textsl{default} & \textsl{description}\\[2ex]
%  \hline\\[1ex]
\starttable
\RP \texttt{source} & \textsl{mandatory}& A dataset to sort.\\[1ex]
\RP \texttt{previous} & \mintinline{python}/None/ & A previous dataset to chain to.\\[1ex]
\stoptable
%\end{tabular}
%}

\subsection{A Practical Limitation}
Internally, the method works by reading the columns to sort by, and
create an indexing column that stipulates the sorting order.  Each
column is then read in turn and sorted according to the sorting
column.

Thus, the method has limited sorting capability.  Internally, it sorts
one column at a time, and it needs to hold that complete column plus
an indexing column in memory simultaneously.






\clearpage
\section{dataset\_datesplit, dataset\_datesplit\_discarded}

\clearpage
\section{dataset\_checksum, dataset\_checksum\_chain}


