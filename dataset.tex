\section{Introduction}

The dataset is the prefered way to store large amounts of data.  The
dataset is the container for fast and simple access to data.  Data in
a dataset are stored as a matrix in rows and columns.  Using the
dataset, data is simple to access and at a very high performance.

Datasets are created in methods, and any job may contain zero, one, or
more datasets.  The most obvious use of a dataset is the cvsimport
method that creates a dataset from an input file.

A job may contain more than one dataset.  This allows for efficient
storage and access of data in some common practical situations.  For
example, a filtering job may split the input dataset in two or more
output datasets that can be accessed independently.

For performance reasons, datasets are typically split into several
slices, where each data line exists in exactly one of the slices.  The
actual slicing may be carried out in any fashion, like round robin, or
even random, but an interesting approach is to slice according to the
hash value of a certain column.  Slicing according to a hashed column
ensures that all lines with a certain hash column value ends up in the
same slice.

This chapter covers ...
chaining, where datasets grows in number of lines, and column
appending, where datasets grow in number of columns.



\newpage
\section{Dataset access}

In a running job, datasets are represented by objects from the Dataset
class.  These objects are immediately available from the
datasets-tuple.  For example

\begin{python}
datasets = ('tlog',)

def synthesis(datasets):
  print(datasets.tlog.filename)
\end{python}
\\
Most common dataset operations are available as member functions
to the dataset class.

A dataset may also be instantiated using jobids. like this

\begin{python}
from dataset import Dataset
d = Dataset('foo-0_0')
\end{python}
\\
In this case, d will be an object based on the default dataset
residing at jobid foo-0\_0.  If there are more datasets in a jobid, or
the dataset is stored by a different name, it may be accessed like
this

\begin{python}
d = Dataset('foo-0_0/foo')
# or
d = Dataset(('foo-0_0', 'foo'))
\end{python}
\\
where the latter is more general in that it uses parameters for both
jobid and name.



\newpage
\section{Dataset properties}

The dataset class has a number of member functions and attributes that
is intended to make it simple to work with datasets.



\subsubsection{Column names}

Consider the
following example, which shows how to find the available columns in
the dataset

\begin{python}
datasets = ('source',)

def synthesis():
  print(datasets.source.columns.keys())
  # ['GTIN', 'date', 'locale', 'subsource']
\end{python}



\subsubsection{Column properties}

For each column, the name, type, and if applicable, the min and max
values are accessible like this

\begin{python}
# each key, i.e. column, has a number of properties, of which the
# most important ones are shown below
print(datasets.source.columns['locale'].type)
# ascii
print(datasets.source.columns['locale'].name)
# locale
print(datasets.source.columns['locale'].min)
# 3
print(datasets.source.columns['locale'].max)
# 9
\end{python}

\subsubsection{Lines per slice}

It may be interesting to see how many lines there are per slice in a
dataset.  This information is available as a list, for example

\begin{python}
print(datasets.source.lines)
# [5771, 6939, 6212, 6312, 6702, 6341, 5988, 6195,
#  6741, 6587, 6518, 5840, 6327, 5933, 6745, 6673,
#  6536, 6405, 6259, 6455, 6036, 6088, 6937, 6245,
#  6418, 6437, 6360, 6106, 6878]
\end{python}
\\
From an efficiency perspective, the variance of the numbers in this
list should be low, so that all slices will contain about the same
amount of data.

\subsubsection{Dataset shape (i.e. number of columns and total number
  of lines}

The shape if the dataset, i.e. the number of columns times the total
number of lines, is available from the shape attribute

\begin{python}
print(datasets.source.shape)
# (4, 184984)
\end{python}
\\
the second number is exactly the sum of the number of lines for each
slice from above.



\subsubsection{hashlabel}
If the dataset is hashed on a particular column, this column is stored
in the hashlabel attribute

\begin{python}
print(datasets.source.hashlabel)
# GTIN
\end{python}



\subsubsection{Filename, caption, hashlabel}

The dataset may have a filename associated to it.  This makes sense in
situations for example where the dataset is created from an input
datafile.  The filename is accessable like this

\begin{python}
print(datasets.source.filename)
# /data/incoming/raw_repository_5391.gz
\end{python}
\\
Furthermore, it is possible to set a caption when creating a dataset.
The caption is entirely user-defined, an accessible like this

\begin{python}
print(datasets.source.caption)
# flattening
\end{python}



\subsubsection{Chains}

When a dataset is created, it is possible to input an optional
parameter previous which is a link to another dataset.  This has
effect on the dataset iterators, which may continue to iterate over
dataset boundaries when one dataset is exhausted and continue to the
next.

This will be described in another section in more detail.  The
previous dataset is available as an attribute

\begin{python}
print(datasets.source.previous)
# neu4-4893_0/default
\end{python}



\newpage
\section{Column Typing}
The dataset columns are typed.  The following types are available

\begin{table}[h!]
 \begin{tabular}{ll}
  \hline
    number    &  float or int\\
    float64   &  64 bit double float\\
    float32   &  32 bit float\\
    int64     &  64 bit signed integer\\
    int32     &  32 bit integer\\
    bool      &  True or False\\
    date      &  date\\
    time      &  time\\
    datetime  &  complete date and time object\\
    bytes     &  raw input, avoid \\
    ascii     &  ascii is faster in python2, otherwise unicode\\
    unicode   &  use for strings\\
    parsed:number   & int, float or string parsing to int or float \\
    parsed:float64  &  and so on...\\
    parsed:float32  &  \\
    parsed:int64    &  \\
    parsed:sint32   &  \\
    json            &  a datastructure that is jsonable\\
    parsed:json     &  string containing parseable json\\
  \hline
 \end{tabular}
 \caption{Available dataset column types.}
\end{table}
\textbf{To be completed.}


\newpage
\section{Create New Dataset}
Datasets are either created in prepare + analysis, or in just
synthesis.

\subsection{Create in prepare + analysis}
A simple example writing three columns to the default dataset is
presented next.  Note that the example creates a dataset chain,
linking the currently created dataset to the dataset that is input
with the name previous.

\begin{python}
from dataset import DatasetWriter
datasets = ('previous',)

def prepare():
  dw = DatasetWriter(
    hashlabel = 'X',
    previous = datasets.previous,
  )
  dw.add('X', number)
  dw.add('Y', number)
  dw.add('Z', number)
  return dw

def analysis(sliceno, prepare_res):
  dw = prepare_res
  ...
  for x, y, z in data:
    dw.write(x, y, z)
\end{python}
\\
Note that the order of the variables in the dw.write function call is
the same as the order of the add calls in prepare\footnote{in case
  write is called with a dict, the order is unknown, but then names
  are looked up using the dict keys.}.

DatasetWriter takes a number of optional arguments such as caption and
filename.  The argument ``name'' specifies the name of the dataset,
which is set to be ``default'' when unassigned.  Several datasets can
be created in the same method using more than one datasetwriter
instance with different ``name''s.

There is some flexibility in the way the write function may be called

\begin{python}
  dw.write_dict({column: value})
  dw.write_list([value, value, ...])
  dw.write(value, value, ...)
  # or even
  dw.writers[name].write(value)  # return True if hashed to correct slice
\end{python}



\subsection{Creating Hashed datasets}

If hashlabel is set, one can use dw.hashcheck(value) to check if value
belongs to the slice.  It is also possible to just call the writer
since it will discard anything not belonging to the correct slice.



\subsection{Create in synthesis}

There are two options if the dataset is to be created in synthesis.
One in to set the slice number first

\begin{python}
  dw.set_slice(sliceno)
\end{python}
\\
while the other is to use one of these functions

\begin{python}
  dw.get_split_write_dict()({column: value})
  dw.get_split_write_list()([value, value, ...])
  dw.get_split_write()(value, value, ...)
\end{python}
\\
that will assign the data to the correct slice automatically.

\subsection{Placeholder:  Creating datasets more manually}


\newpage
\section{Appending new columns to an existing Dataset}

With minimal overhead, the dataset supports adding new columns to an
existing dataset.  This is implemented by storing the new column data
together with a pointer to the dataset

Appending new columns work exactly the same way as creating a dataset,
with the exception that a link to a dataset that is to be appended to
is input to the writer constructor.  The following example appends one
column to an existing dataset while maintaining the chain.  Note that
appending does only apply to one dataset, and not to the complete
chain.

\begin{python}
datasets = ("source", "previous",)

def prepare():
  dw = dataset.DatasetWriter(
    parent=datasets.source,
    previous=datasets.previous,
    caption="flattening_attempt_1"
  )
  dw.add(name, type)
  return dw

def analysis(sliceno, prepare_res):
  dw = prepare_res
  ...
  dw.write(value)
\end{python}
\\
Note that an error is issued if the total number of appended lines
does not match the number of lines in the parent dataset.
