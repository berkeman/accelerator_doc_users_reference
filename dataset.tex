%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{chap:datasets}

The Dataset class provides fast and simple access to data.  It is the
prefered way to store data using the Accelerator.  Datasets are
created by methods, and are therefore located inside job directories.
There can be any number of Datasets in a job.  Datasets are
lightweight -- adding new columns to a dataset, or appending datasets
to eachother are instantaneous operations.

The most obvious way to generate a dataset is using the
\texttt{cvsimport} method that creates a dataset from an input file.
But much more advanced use is possible since a job may contain more
than one Dataset.  Being able to create several Datasets at once
allows for efficient storage and access of data in some common
practical situations.  For example, a filtering job may split the
input Dataset into two or more output Datasets that can be accessed
independently.

For performance reasons, datasets are typically split into several
slices, where each data row exists in exactly one of the slices.  The
actual slicing may be carried out in different ways, like round robin,
or even random, but an interesting approach is to slice according to
the hash value of a certain column.  Slicing according to a hashed
column ensures that all rows with a certain column value always ends
up in the same slice.



\section{Dataset Internals}

On a high level, the dataset stores a \textsl{matrix} of rows and
columns.  Each column is represented by a column name,
or \emph{label}, and all columns have the same number of rows.
Columns are typed, and there is a wide range of types available.
Typing will be introduced in section~\ref{sec:dataset-typing}.

The dataset is further split into disjoint slices, where each slice
holds a unique subset of the dataset's rows.  Slicing makes simple but
efficient parallel processing possible.  See Figure~\ref{fig:slices}.
The number of slices is set initally by the user, and all workdirs
that are used together in a project must use the same number of
slices.

On a low level, there is one file stored on disk for each slice and
column.  A job that needs to read only a subset of the total number of
columns may open and read from the relevant files only.

A technical note: If the number of slices is large and files are
small, there will be a signinficant overhead from disk \texttt{seek()}
if using rotating disks.  The Accelerator mitigates this by using
single files with offset-indexing when appropriate.

\begin{figure}[b!]
  \begin{center}
     \input{figures/dataset_concept.pdftex_t}
     \caption{A ``movie rating'' dataset composed of four columns
              sliced into three slices.}
     \label{fig:slices}
  \end{center}
\end{figure}


\section{Chaining}
When a dataset is created, it is optional to input a link to another
dataset using the parameter \texttt{previous}.  This is called
\emph{chaining}.  Chaining provides a lightweight way to append rows
to datasets, simply by linking datasets together.  A typical use case
is the import of log files.  A new dataset is created from each new
log file, and each dataset chains to the previous.  Reading the full
chain will access all log rows.  This has effect on the dataset
\emph{iterators} (see chapter~\ref{chap:iterators}), which may continue iterating
over the next dataset in the chain when the current dataset is
exhausted.


\section{Slicing and Hashing}
\label{sec:slicing_and_hashing}

Datasets are by default sliced into a number of slices specified by
the Accelerator's configuration file~\ref{sec:configfile}.  Slicing means that the
rows of data in the dataset are distributed into different sets,
called slices.  Typically, there is one file on disk for each slice
and column.  The main reason for doing this is performance.  All files
could be read in parallel.

Datasets can be sliced in a number of different ways.  A simple method
is to use round-robin, which cycles through the slices when writing.
Round-robin will balance the number of rows per slice as equal as
possible, which is a good thing in many scenarios.  In mathematical
terms, round robin would be
\begin{equation}
  \textrm{row}~n \longrightarrow \textrm{slice}(n~\textrm{mod}~N)
\end{equation}

Another way is to slice by looking at the values of a fixed single
column and put all rows with equal values in the same column.  This
way, data will be sliced ``by content'', and the number of rows per
slice may vary significantly.  In the context of the Accelerator, this
is called \emph{hashing}, and a dataset can be hashed on any single
column.  Written as an equation, it will look like this
\begin{equation}
\textrm{row}~n \longrightarrow \textrm{slice}(\textrm{hash}(\textrm{word})~\textrm{mod}~N)
\end{equation}

The advantage of this method is that data in each slice becomes
independent in many application, which is ideal for parallel
processing of the dataset.  The hash function used by the Accelerator
is a well-known function called \texttt{siphash24} that is available
from the \texttt{gzutil} library
\begin{python}
from gzutil import siphash24
\end{python}



\section{Dataset as Input Parameter}
Datasets may be input to a method using the \texttt{datasets} input
parameter list.  In a running job, the items in this list are object
of the \texttt{Dataset} class.  This class has a number of member
functions, for example
\begin{python}
datasets = ('source',)

def synthesis():
  print(datasets.source.shape)
\end{python}
will print the number of rows and columns of the \texttt{source}
Dataset.


\section{Datasets from Jobids}
Although a not so common operation, a Dataset object can be
instantiated from a jobid, like this
\begin{python}
from dataset import Dataset
...
d = Dataset('foo-0')
\end{python}
In this case, \texttt{d} will be an object based on the default
dataset residing at jobid \texttt{foo-0}.  If the dataset is stored by
a different name, it may be accessed like this
\begin{python}
d = Dataset('foo-0/bar')
# or
d = Dataset(('foo-0', 'bar'))
\end{python}


\section{Change to Another Dataset in a Job}
If a job contains more than one dataset, it is possible to switch from
one to another easily.  Assume for example that the input dataset in
the following example is stored in a job that has two datasets
named \texttt{first} and \texttt{second}.
\begin{python}
from dataset import Dataset

datasets=('source',)

def synthesis():
    first = datasets.source, 'first')
    second = datasets.source, 'second')
\end{python}


                      



\clearpage
\section{Dataset Properties}
The Dataset class has a number of member functions and attributes that
is intended to make it simple to work with.  These functions will be
described in the next sections.


\subsection*{Column Names}
All columns in a dataset may be aquired using the \texttt{columns}
property, like this
\begin{python}
datasets = ('source',)

def synthesis():
  print(datasets.source.columns.keys())
  # may print something like
  # ['GTIN', 'date', 'locale', 'subsource']
\end{python}
The \texttt{columns} attribute is actually a dictionary from column
name to properties, as will be shown in the next section.


\subsection*{Column Properties}
For each column, the name, type, and if applicable, the minimum and
maximum values are accessible like this
\begin{python}
print(datasets.source.columns['locale'].type)
# number

print(datasets.source.columns['locale'].name)
# locale

print(datasets.source.columns['locale'].min)
# 3

print(datasets.source.columns['locale'].max)
# 107
\end{python}
Creation of the \texttt{max} and \texttt{min} values is a simple
operation that is done in linear time when the dataset is created.
Maximum and minimum values are used for example when iterating over
chains of sorted datasets, to quickly decide if a dataset is outside
range and can be skipped in its entirety, see
section~\ref{sec:iterate_sloppy_range}.


\subsection*{Rows per Slice}

It may be interesting to see how many rows there are per slice in a
dataset.  This information is available as a list, for example
\begin{python}
print(datasets.source.lines)
# [5771, 6939, 6212, 6312, 6702, 6341, 5988, 6195,
#  6741, 6587, 6518, 5840, 6327, 5933, 6745, 6673,
#  6536, 6405, 6259, 6455, 6036, 6088, 6937, 6245,
#  6418, 6437, 6360, 6106, 6878]
\end{python}
The first item in the list is the number of rows in slice 0, and so
fourth.  The total number of rows in the Dataset is the sum of these
numbers.


\subsection*{Dataset Shape}
The shape of the dataset, i.e.\ the number of rows and columns, is
available from the shape attribute
\begin{python}
print(datasets.source.shape)
# (4, 184984)
\end{python}
The second number is exactly the sum of the number of lines for each
slice from above.


\subsection*{Hashlabel}
If the dataset is hashed on a particular column, the name of this
column is stored in the hashlabel attribute
\begin{python}
print(datasets.source.hashlabel)
# GTIN
\end{python}



\subsection*{Filename and Caption}
The dataset may have a filename associated to it.  This makes sense in
situations for example where the dataset is created from an input data
file using \texttt{csvimport} or similar.  The filename is accessable
using the \texttt{filename} attribute:
\begin{python}
print(datasets.source.filename)
# /data/incoming/raw_repository_5391.gz
\end{python}
Furthermore, it is possible to set a caption at dataset creation time.
The caption is entirely user-defined and has no function in the
Accelerator.  The caption is accessible like this
\begin{python}
print(datasets.source.caption)
# rehash_of_raw_data
\end{python}



\subsection*{Chains}
The previous dataset in a dataset chain is found in the
\texttt{previous} attribute:
\begin{python}
print(datasets.source.previous)
# import-4893/default
\end{python}



\newpage
\section{Column Typing}
The dataset columns are typed.  This means, for example, that if a
column's type is \texttt{date}, each value read from the column will
be in Python's \texttt{date} format, ready for processing.  The same
goes for all types, including \texttt{json}, which may return rather
complex datatypes.

All available types are shown in the following table.  More details
follow in the next sections.



\begin{snugshade}
%\begin{table}[h!]
 \begin{tabular}{p{3cm} p{8cm}}
\\
   \textbf{typename}   & \textbf{explanation} \\[2ex]\hline\\
   \texttt{number}     &  float or int\\
   \texttt{number:int} &  int\\[1ex]
   
   \texttt{float64}   &  64 bit (double) float\\
   \texttt{float32}   &  32 bit float\\
   \texttt{int64}     &  64 bit signed integer\\
   \texttt{int32}     &  32 bit integer\\[1ex]
   
   \texttt{bool}      &  True or False\\[1ex]
   
   \texttt{date}      &  date\\
   \texttt{time}      &  time\\
   \texttt{datetime}  &  complete date and time object\\[1ex]
   
   \texttt{bytes}     &  raw input, avoid \\
   \texttt{ascii}     &  ascii is faster in python2, otherwise use unicode\\
   \texttt{unicode}   &  use for strings\\[1ex]
   
   \texttt{json}            &  a datastructure that is jsonable\\[1ex]
   
   \texttt{parsed:number}   & int, float or string parsing into \texttt{number} \\
   \texttt{parsed:float64}  & int, float or string parsing into \texttt{float64} \\
   \texttt{parsed:float32}  & int, float or string parsing into \texttt{float32} \\
   \texttt{parsed:int64}    & int, float or string parsing into \texttt{int64} \\
   \texttt{parsed:int32}    & int, float or string parsing into \texttt{int32} \\
   \texttt{parsed:json}     &  string containing parseable json\\[1ex]
   
 \end{tabular}
% \caption{Available dataset column types.}
% \label{tab:types}
%\end{table}
\end{snugshade}

\subsection{Arbitrary precision numbers:  \texttt{number}}
The type \texttt{number} is integer when possible and float otherwise.
it can handle very large numbers, up to $\pm (2^{1007}-1)$.  Integers
are enforced using \texttt{number:int}, and it accepts trailing
decimal zeroes like \texttt{7.0}, \texttt{4.000} etc.  This is useful
when typing datafiles where numbers actually are integers but have
trailing zero decimals.

The \texttt{number} type occupies a minimum of nine bytes on disk,
where eight is for the number itself and the additional byte is a
marker.


\subsection{Standard Fixed Size Numbers}
The common \texttt{int} and \texttt{float} types in 32 and 64 bit
versions are available for use when the range of the data is known.


\subsection{Booleans}
The \texttt{bool} type is used to store logical
\mintinline{python}/True/ or \mintinline{python}/False/ values only.


\subsection{Types Relating to Time}
The \texttt{date}, \texttt{time}, and \texttt{datetime} are compatible
with Python's corresponding classes, where \texttt{datetime} is the
combination of \texttt{date} and \texttt{time}.  A column that is
typed to any of these may directly take advantage of the high level
time related methods, like for example
\begin{python}
for ts in datasets.source.iterate(sliceno, 'timestamp'):
    print(ts.strftime('%Y-%m-%d')
\end{python}


\subsection{String Types}
There are three string types, \texttt{bytes}, \texttt{ascii}, and
\texttt{unicode}.  The \texttt{bytes} type is what is assigned to
columns by \texttt{csvimport}.  For columns with text,
\texttt{unicode} is the prefered choice, and it executes faster in
Python~3.


\subsection{JSON Type}
It is possible to store more complex data structures using the JSON
format.  The \texttt{JSON} type accept a JSON-able datastructure as
input.

\subsection{\texttt{parsed} Types}
In addition, there are a few types prefixed with \texttt{parsed:} that
allow for a more flexible assignment of values.  For example,
the \texttt{parsed:number} type accepts both \texttt{int}s
and \texttt{float}s, as well as strings that are parseable to a
number, such as \texttt{'3.14'}.



\clearpage
\section{Create a New Dataset}
Datasets are created by methods using the \texttt{DatasetWriter}
class.  The most common scenario is to set up the new dataset in
\prepare, and write data to it in parallel in \analysis, but is is
also possible to write a dataset in an entirely serial fashion in
\synthesis.  When a dataset-creating method terminates, it will create
and store all required meta-information, such as min/max values, for
the created dataset(s) automatically.

The most common arguments to \texttt{DatasetWriter} are
\begin{snugshade}
  \begin{tabular}{p{3cm} p{8cm}}
    \\
    \textbf{argument} & \textbf{description}\\[1ex]\hline\\[1ex]
    \texttt{filename}  & if there is a filename associated, store it here\\
    \texttt{caption}   & additional caption\\
    \texttt{hashlabel} & name of column specifying hash for slicing\\
    \texttt{previous}  & previous Dataset, for chaining\\
    \texttt{name}      & default set to \texttt{default}\\
    \texttt{parent}    & parent Dataset when adding columns\\
  \end{tabular}
\end{snugshade}
%\noindent See section~\ref{} for a complete list of arguments and
%advanced dataset use.




\subsection{Create in \prepare + \analysis}
\label{sec:create_dataset_in_analysis}

The following example will use \texttt{DatasetWriter} to create a
Dataset with three columns.  The name of the dataset will be
\texttt{firstset}.  If the name is omitted, the the name
\texttt{default} will be used instead.  The writer will be initialised
in \prepare, and data will be written to the Dataset in \analysis.
Note that the example creates a dataset \emph{chain}, linking the
dataset under creation to the dataset named \texttt{previous} from the
input parameters.
\texttt{X}.
\begin{python}
from dataset import DatasetWriter
datasets = ('previous',)

def prepare():
    dw = DatasetWriter(
        previous = datasets.previous,
        name = 'firstset'
    )
    dw.add('X', 'number')
    dw.add('Y', 'unicode')
    dw.add('Z', 'time')
    return dw

def analysis(sliceno, prepare_res):
    dw = prepare_res
    ...
    for x, y, z in some_data:
        dw.write(x, y, z)
\end{python}
The order of the variables in the dw.write function call is the same
as the order of the add calls in prepare.  There are a few alternative
ways of writing data, as shown here
\begin{python}
dw.write_dict({column: value})
dw.write_list([value, value, ...])
dw.write(value, value, ...)
\end{python}
Several Datasets can be created simultaneously using multiple writers
with different names.



\clearpage
\subsection{Create in synthesis}

There are two possible ways to create a Dataset in \synthesis.  One is
to first set a slice number
\begin{python}
dw.set_slice(sliceno)
\end{python}
before writing data into that slice.  The other is to use one of
the \texttt{split\_write} functions
\begin{python}
dw.get_split_write_dict()({column: value})
dw.get_split_write_list()([value, value, ...])
dw.get_split_write()(value, value, ...)
\end{python}
These writers will write round-robin if the dataset is not hashed, and
to the ``right'' slice if the dataset is hashed.



\subsection{Creating Hashed Datasets}
Creating a hashed dataset is accomplished by setting
the \texttt{hashlabel} argument of \texttt{DatasetWriter}.  It is up
to the dataset generating method to make sure that each row is written
to the correct slice, according to the hash function value of the
hashlabel column.  A row should go into slice $n$ if and only if
\begin{python}
from gzutil import siphash24
assert siphash24(hashcol) % options.slices == n
\end{python}
Otherwise an exception will occur.  It is possible to override this
behavior by calling
\begin{python}
dw.enable_hash_discard()
\end{python}
first in each slice or after each \texttt{set\_slice()}.  Then, writes
that belongs to another slice are silently ignored.



\subsection{More Advanced Dataset Creation}
Currently out-of-scope of this manual.  Please see the file
\texttt{dataset.py} for full information.


\clearpage
\section{Appending New Columns to an Existing Dataset}

With minimal overhead, existing datasets could be extended with new
columns.  Internally, this is implemented by storing the new column
data together with a pointer to the ``parent'' dataset.

Appending new columns works the same way as when creating a dataset,
with the exception that a link to a dataset that is to be appended to
is input to the writer constructor.  The following example appends one
column to an existing dataset while maintaining the chain.  Note that
appending a column does only apply to one single dataset, and not to
the complete chain of datasets, if present.
\begin{python}
from dataset import DatasetWriter

datasets = ('source', 'previous',)

def prepare():
    dw = DatasetWriter(
        parent=datasets.source,
        previous=datasets.previous,
        caption='with the new colum'
    )
    dw.add('newcolname', 'unicode')
    return dw

def analysis(sliceno, prepare_res):
    dw = prepare_res
    ...
    dw.write(value)
\end{python}
The \texttt{DatasetWriter} will automatically check that the number of
appended rows does match the number of rows in the parent dataset.
Otherwise, an error will be issued and execution will terminate.

It is strongly recommended that new columns are added
in \texttt{analysis}, and \textbf{not} in \synthesis.  This is because
reading data is one full slice at a time, while writing data is
round-robin per row, and unless being very careful, new column values
will not be added to the correct rows.
