%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2021 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{chap:datasets}

The \textsl{dataset} is the preferred way to store row-column data
using the Accelerator.  The dataset provides fast parallel streaming
access to data, and strict typing with many supported types.
Furthermore, datasets are lightweight -- adding new columns to a
dataset, or appending datasets to eachother are instantaneous
operations.  Using datasets in a program is simple, since all
operations are avai through the \texttt{Dataset} class.

Datasets are created by methods, and are therefore located inside job
directories.  There can be any number of datasets in a job.  The most
obvious way to generate a dataset is using the
\texttt{cvsimport} method that creates a dataset from an input file.
But the fact that a job can create and hold many datasets opens up for
a lot of possibilities.  The \texttt{csvimport} method, for example,
creates a dataset from the input data, and it can be instructed to put
unparsable lines of input data into a second ``bad'' dataset.

For performance reasons, datasets are split into several slices, and
each data row exists in exactly one of the slices.  The actual slicing
may be carried out in different ways, like round robin, or randomly,
but an interesting approach is to slice according to the hash value of
a certain column.  Slicing according to a hashed column ensures that
all rows with a certain column value always ends up in the same slice.
Hash-based slicing often makes completely parallel processing of a
dataset possible, since related data is not spread over different
slices, and thus no merge or ``reduce'' stage is required.



\section{Dataset Internals}

On a high level, the dataset stores a \textsl{matrix} of rows and
columns.  Each column is represented by a column name,
or \emph{label}, and all columns have the same number of rows.
Columns are typed, and there is a wide range of types available.
Typing will be introduced in section~\ref{sec:dataset_typing}.

The dataset is further split into disjoint slices, where each slice
holds a unique subset of the dataset's rows.  Slicing makes simple but
efficient parallel processing possible.  See Figure~\ref{fig:slices}.
The number of slices is set initially by the user in the Accelerator's
configuration file, and all workdirs that are used together in a
project must use the same number of slices.

On a low level, there is one file stored on disk for each slice and
column.  A job that needs to read only a subset of the total number of
columns may open and read from the relevant files only.

A technical note: If the number of slices is large and files are
small, there will be a significant overhead from disk \texttt{seek()},
especially if using rotating disks.  The Accelerator mitigates this by
changing the storage model to using single files with offset-indexing
when appropriate.

\begin{figure}[b!]
  \begin{center}
     \input{figures/dataset_concept.pdftex_t}
     \caption{A ``movie rating'' dataset composed of four columns
              sliced into three slices.}
     \label{fig:slices}
  \end{center}
\end{figure}


\section{Slicing and Hashing}
\label{sec:slicing_and_hashing}

As shown in the previous section and in figure~\ref{fig:slices},
datasets are \textsl{sliced} into disjoint sets,
called \textsl{slices}, that can be accessed independently.
Typically, in a dataset, there is one file on disk for each slice
\textsl{and} column.  The main reason for doing this is performance.
All files could be read in parallel, and only files relevant to the
task at hand are read.

How to slice a dataset is not unique.  There are many ways to carry
out slicing.  Perhaps the most simple way is to use round-robin, which
cycles through the slices when writing, one data item at a time.
Round-robin will balance the number of rows per slice as equal as
possible, which is a good thing in many scenarios.  In
semi-mathematical terms, round robin would be
\begin{equation*}
  n \longrightarrow n~\textrm{mod}~N
\end{equation*}
Meaning that input data row $n$ is stored in slice $n~\textrm{mod}~N$.

Another way is to slice by looking at the values of a fixed single
column and put all rows with equal ``property'' in the same column.
This way, data will be sliced ``by content'', and the number of rows
per slice may vary significantly.  In the context of the Accelerator,
the ``property'' is a hash function, and the process is
called \emph{hash partitioning}. A dataset can be hashed on any single
column, as long as its contents is hashable.  Written as an equation,
it will look like this
\begin{equation*}
  n \longrightarrow \textrm{hash}(\textrm{data})~\textrm{mod}~N
\end{equation*}
where ``data'' is the value of row $n$ in the hashing column.

In many practical applications, data may be sliced using a hashing
function so that data in each slice becomes \textsl{independent} for
the application at hand.  Independent data means that processing of
the dataset can be carried out in a completely parallel fashion.

The hash function used by the Accelerator is a well-known function
called siphash-2-4 that is available from the
Accelerator's \texttt{gzutil} library
\begin{python}
from accelerator.gzutil import siphash24

y = siphash24(x)
\end{python}
This function is normally only used ``under the hood'', there should
be no need to call it explicitly.



\section{Chaining}
When a dataset is created, it is optional to attach a link to another
dataset using the parameter \texttt{previous}.  This is called
\emph{chaining}.  Chaining provides a lightweight way to append rows
to datasets, simply by linking datasets together.  A typical use case
is the import of log files.  A new dataset is created from each new
log file, and each dataset chains to the previous.  Reading the full
chain will access all log rows.  As will be discussed in detail later,
this relates to dataset \emph{iterators} (see
chapter~\ref{chap:iterators}), that may continue iterating over the
next dataset in the chain when the current dataset is exhausted.  Here
is a an example of how \texttt{csvimport} jobs can be chained
\begin{python}
job = urd.build('csvimport', filename='file1.txt')
job = urd.build('csvimport', filename='file2.txt', previous=job)
\end{python}
In order to maintain high speed when processing long chains, the
Accelerator caches chain metadata every 64th dataset.  This reduces
seek times significantly on rotating disks.



\section{Dataset as Job Input Parameter}
Datasets may be input to a method using the \texttt{datasets} input
parameter list.  In a running job, the items in this list are object
of the \texttt{Dataset} class.  See the following example
\begin{python}
datasets = ('source',)
def synthesis():
    print(datasets.source.columns)
\end{python}
Note the comma used to indicate that this is in fact a
Python \texttt{tuple} of dataset(s).

It is also possible to input a list or set of datasets, like this
\begin{python}
datasets = (['source',])
def synthesis():
    for ds in datasets.source:
        print(ds.columns)
\end{python}



\section{Datasets from Job Objects}
Information about a job instance's datasets is provided using
the \texttt{job.dataset()} function and the \texttt{job.datasets}
attribute.  To find all datasets in a job, use \texttt{job.datasets}
like in this example
\begin{python}
def main(urd):
    job = urd.build('create_datasets')
    for ds in job.datasets:
        print(ds.name)
\end{python}
To work on a specific dataset, just ask for it using its name as input
parameter to \texttt{job.dataset()},
\begin{python}
    ds_first = job.dataset('first')
    ds_default = job.dataset()
\end{python}
Without an input parameter, the default dataset is returned.  An error
is issued if the dataset does not exist.

                      

\section{Dataset Properties}
The \texttt{Dataset} class has a number of member functions and
attributes that is intended to make it simple to work with.  These
functions will be described in the next sections.  But first a note on
naming datasets.


\subsection{Dataset Name}
The name of a dataset is accessible using the \texttt{.name}
attribute, like this
\begin{python}
    print(ds_first.name)
\end{python}
The Accelerator is designed to handle various string encodings with
ease, and in most situations the naming rules are very liberal.  The
dataset name, however, should preferably be \textsl{limited to ASCII
characters}, since a directory with the name of the dataset will be
stored on disk.  The Accelerator cannot guarantee that the file system
in use handles any ``special'' characters.  Newline is for example not
allowed.


\subsection{Column Names}
All columns in a dataset may be acquired using the \texttt{.columns}
property, like this
\begin{python}
datasets = ('source',)

def synthesis():
    print(datasets.source.columns.keys())
    # may print something like
    # ['GTIN', 'date', 'locale', 'subsource']
\end{python}
The \texttt{.columns} attribute is actually a dictionary from column
name to properties, as will be shown in the next section.

Not all column names are valid, see
section~\ref{sec:valid-column-names} for more information.

\subsection{Column Properties}
For each column, the name, type, and if applicable, the minimum and
maximum values are accessible like this
\begin{python}
print(datasets.source.columns['locale'].type)
# number

print(datasets.source.columns['locale'].name)
# locale

print(datasets.source.columns['locale'].min)
# 3

print(datasets.source.columns['locale'].max)
# 107
\end{python}
Creation of the \texttt{max} and \texttt{min} values is a simple
operation that is done in linear time when the dataset is created.
Maximum and minimum values are used for example when iterating over
chains of sorted datasets, to quickly decide if a dataset is outside
range and can be skipped in its entirety, see
section~\ref{sec:iterate_sloppy_range}.


\subsection{Rows per Slice}

It may be interesting to see how many rows there are per slice in a
dataset.  This information is available as a list, for example
\begin{python}
print(datasets.source.lines)
# [5771, 6939, 6212, 6312, 6702, 6341, 5988, 6195,
#  6741, 6587, 6518, 5840, 6327, 5933, 6745, 6673,
#  6536, 6405, 6259, 6455, 6036, 6088, 6937, 6245,
#  6418, 6437, 6360, 6106, 6878]
\end{python}
The first item in the list is the number of rows in slice 0, and so
fourth.  The total number of rows in the dataset is the sum of these
numbers.


\subsection{Dataset Shape}
The shape of the dataset, i.e.\ the number of rows and columns, is
available from the shape attribute
\begin{python}
print(datasets.source.shape)
# (4, 184984)
\end{python}
The second number is exactly the sum of the number of lines for each
slice from above.


\subsection{Hashlabel}
If the dataset is hash partitioned on a particular column, the name of
this column is stored in the hashlabel attribute
\begin{python}
print(datasets.source.hashlabel)
# GTIN
\end{python}



\subsection{Filename and Caption}
The dataset may have a filename associated to it.  This makes sense in
situations for example where the dataset is created from an input data
file using \texttt{csvimport} or similar.  The filename is accessible
using the \texttt{filename} attribute:
\begin{python}
print(datasets.source.filename)
# /data/incoming/raw_repository_5391.gz
\end{python}
Furthermore, it is possible to set a caption at dataset creation time.
The caption is entirely user-defined and has no function in the
Accelerator.  The caption is accessible like this
\begin{python}
print(datasets.source.caption)
# rehash_of_raw_data
\end{python}



\section{Operations on Chains}
The \texttt{.chain} function is used to operate on dataset chains.  It
takes a dataset as input, some options that will be discussed next,
and returns a \texttt{DatasetChain} object.
\begin{python}
# return a chain object for dataset "ds"
chain = ds.chain()
\end{python}
The \texttt{chain()}-function takes the following optional arguments

\starttable
\texttt{length} & \texttt{-1} & Number of datasets to include, default is \texttt{-1}, meaning all datasets in chain.  Do not confuse with \texttt{stop\_ds}.\\
\texttt{reverse} & \pyFalse & Return the datasets in reverse order.  Default \pyFalse.\\
\texttt{stop\_ds} & \pyNone & Return datasets \textsl{from} \texttt{stop\_ds} to current dataset.  Do not mix with \texttt{length}.\\
\stoptable
\noindent The returned value, \texttt{chain} in the previous example, is of
type \texttt{DatasetChain}, which supports the following operations.
\starttabletwo
\mintinline{python}|min(<column>)| & Minimum value of column, or \pyNone if column does not exist or is not sortable.\\
\mintinline{python}|max(<column>)| & Return minimum value of column, or \pyNone if column does not exist or is not sortable.\\
\mintinline{python}|lines(sliceno=None)| & Default is number of lines in chain.  With option \texttt{sliceno=x}, number of lines in slice \texttt{x}.\\
\mintinline{python}|column_count(<column>)| & Number of datasets in chain containing column \texttt{<column>}\\
\mintinline{python}|column_counts()| & Counter of occurances per column per dataset in chain.\\
\mintinline{python}|with_column(<column>)| & A \texttt{DatasetChain} object containing only those datasets that has column \texttt{<column>}.\\
\mintinline{python}|iterate(...)| & Same arguments as \texttt{Dataset.iterate()}.  Will iterate over the whole chain.\\
\stoptabletwo





\section{Column Data Types}
\label{sec:dataset_typing}

Dataset columns are typed.  This means, for example, that if a
column's type is \texttt{date}, each value read from the column will
be in Python's \texttt{date} format, ready for processing.  The same
goes for all supported types, including \texttt{json}
and \texttt{pickle}, that may return rather complex datatypes.

By default, a typed column does not allow the storage of \pyNone
values.  This can be changed by setting the \texttt{none\_support}
Boolean when creating the column, see
section~\ref{sec:datasetwriter_add}.

All available types are shown in the following table.  More details
follow in the next sections.



\starttabletwo
\RPtwo   \texttt{number}     &  float or int\\[1ex]
   
\RPtwo   \texttt{float64}   &  64 bit (double) float\\
\RPtwo   \texttt{float32}   &  32 bit float\\[1ex]

\RPtwo   \texttt{int64}     &  64 bit signed integer\\
\RPtwo   \texttt{int32}     &  32 bit integer\\[1ex]

\RPtwo   \texttt{bits64}     &  64 bit bitmask\\
\RPtwo   \texttt{bits32}     &  32 bit bitmask\\[1ex]

\RPtwo   \texttt{complex64}     &  64 bit complex number\\
\RPtwo   \texttt{complex32}     &  32 bit complex number\\[1ex]

\RPtwo   \texttt{bool}      &  True or False\\[1ex]
   
\RPtwo   \texttt{date}      &  date\\
\RPtwo   \texttt{time}      &  time\\
\RPtwo   \texttt{datetime}  &  complete date and time object\\[1ex]
   
\RPtwo   \texttt{bytes}     &  raw data \\
\RPtwo   \texttt{ascii}     &  ascii is faster in python2, otherwise use unicode\\
\RPtwo   \texttt{unicode}   &  use for strings\\[1ex]
   
\RPtwo   \texttt{json}            &  a datastructure that is jsonable\\
\RPtwo   \texttt{pickle}          &  a datastructure that is pickle-able!\\[1ex]
   
\RPtwo   \texttt{parsed:number}   & int, float or string parsing into \texttt{number} \\
\RPtwo   \texttt{parsed:float64}  & int, float or string parsing into \texttt{float64} \\
\RPtwo   \texttt{parsed:float32}  & int, float or string parsing into \texttt{float32} \\
\RPtwo   \texttt{parsed:complex64}  & int, float or string parsing into \texttt{complex64} \\
\RPtwo   \texttt{parsed:complex32}  & int, float or string parsing into \texttt{complex32} \\
\RPtwo   \texttt{parsed:int64}    & int, float or string parsing into \texttt{int64} \\
\RPtwo   \texttt{parsed:int32}    & int, float or string parsing into \texttt{int32} \\
\RPtwo   \texttt{parsed:json}     &  string containing parseable json\\[1ex]
\stoptabletwo

\subsection{Arbitrary precision numbers:  \texttt{number}}
The type \texttt{number} is integer when possible and float otherwise.
it can handle very large numbers, up to $\pm (2^{1007}-1)$.
The \texttt{number} type occupies a minimum of nine bytes on disk,
where eight is for the number itself and the additional byte is a
marker.



\subsection{Standard Fixed Size Numbers}
The common \texttt{int} and \texttt{float} types in 32 and 64 bit
versions are available for use when the range of the data is known.


\subsection{Booleans}
The \texttt{bool} type is used to store logical
\mintinline{python}/True/ or \mintinline{python}/False/ values only.


\subsection{Types Relating to Time}
The \texttt{date}, \texttt{time}, and \texttt{datetime} are compatible
with Python's corresponding classes, where \texttt{datetime} is the
combination of \texttt{date} and \texttt{time}.  A column that is
typed to any of these may directly take advantage of the high level
time related methods, like for example
\begin{python}
for ts in datasets.source.iterate(sliceno, 'timestamp'):
    print(ts.strftime('%Y-%m-%d')
\end{python}


\subsection{String Types}
There is a \texttt{unicode} type for strings.  On Python2, the
\texttt{ascii} type could be used as well.  The \texttt{unicode}
type executes faster on Python3.


\subsection{Raw Data}

The \texttt{bytes} type is used to store raw data, such as binary
image files.  The upper storage limit for a value typed
as \texttt{bytes} is almost 2GB ($2^{31}-1$ bytes).
The \texttt{csvimport} standard method uses this type for all data in
its output dataset.


\subsection{Bitmasks}
The bitmask types, \texttt{bits32} and \texttt{bits64}, are stored as
32 or 64 bits of data in a dataset, and is represented by unsigned
integers in the Python code.


\subsection{Complex Numbers}
There are two complex number types, \texttt{complex32}
and \texttt{complex64}.


\subsection{JSON}
The JSON type makes it possible to store and load more complex data
structures in a dataset.  Anything that is JSONable works as input.
Conversion between JSON and Python data types is done by the writers
and iterators, so the user can just work on the data and never has to
see the actual JSON, for example
\begin{python}
    dw = job.datasetwriter(...)
    ...
    a = dict(x=3, y=dict(z=5, w=[1,2,3]))
    dw.write(a)
\end{python}



\subsection{Pickle}
The Python Pickle type is the most flexible type of all.  It supports
any pickle-able object, and encoding and decoding is done on-the-fly
under the hood.  This means that \textsl{it is possible to write almost
anything into a dataset column}!



\subsection{\texttt{parsed} Types}
In addition, there are a few types prefixed with \texttt{parsed:} that
allow for a more flexible assignment of values.  For example,
the \texttt{parsed:number} type accepts both \texttt{int}s
and \texttt{float}s, as well as strings that are parseable to a
number, such as \texttt{'3.14'}.



\subsection{\pyNone-Handling}
The value \pyNone is valid input for all types that support \pyNone,
i.e.\ all types except the bitmask-types.  For example, valid values
for a \texttt{bool} type column are \{\pyTrue, \pyFalse\}
without \texttt{none\_support},
and \{\pyTrue, \pyFalse, \mintinline{python}|None|\}
with \texttt{none\_support}.  See section~\ref{sec:datasetwriter_add}
for more information on \texttt{none\_support}.



\section{Creating a New Dataset}
\label{sec:datasetwriter}
Datasets are created by methods using the \texttt{DatasetWriter}
class.  An instance of this class is available in a running method
as \texttt{job.datasetwriter} like this
\begin{python}
def prepare(job):
    dw = job.datasetwriter()
\end{python}

The most common scenario is to set up the new dataset in
\prepare, and write data to it in parallel in \analysis, but is is
also possible to write a dataset in an entirely serial fashion in
\synthesis.  When a dataset-creating method terminates, it will create
and store all required meta-information, such as min/max values, for
the created dataset(s) automatically.

The most common arguments to \texttt{DatasetWriter} are
\starttabletwo
\RPtwo    \texttt{filename}  & if there is a filename associated, store it here\\
\RPtwo    \texttt{caption}   & additional caption\\
\RPtwo    \texttt{hashlabel} & name of column to hash by when slicing\\
\RPtwo    \texttt{previous}  & previous Dataset, for chaining\\
\RPtwo    \texttt{name}      & dataset name, default set to \texttt{default}\\
\RPtwo    \texttt{parent}    & parent Dataset when adding columns\\
\stoptabletwo




\subsection{Create in \prepare + \analysis}
\label{sec:create_dataset_in_analysis}
The following example will use \texttt{DatasetWriter} to create a
dataset with three columns of different types.  The name of the
dataset will be \texttt{firstset}.  The writer will be initialised
in \prepare, and data will be written to the dataset in \analysis.
Note that the example actually creates a dataset \emph{chain}, since
it links the dataset under creation to the dataset
named \texttt{previous} from the input parameters.
\begin{python}
datasets = ('previous',)

def prepare(job):
    dw = job.datasetwriter(
        previous = datasets.previous,
        name = 'firstset'
    )
    dw.add('X', 'number')
    dw.add('Y', 'unicode')
    dw.add('Z', 'time')
    return dw

def analysis(sliceno, prepare_res):
    dw = prepare_res
    ...
    for x, y, z in some_data:
        dw.write(x, y, z)
\end{python}
The function \texttt{dw.write()} is used to write data to the dataset.
The order of the variables in the \texttt{.write()} function call is
the same as the order of the \texttt{.add()} calls in prepare.  There
are a few alternative ways of writing data, as shown here
\begin{python}
    # write a dict with keys corresponding to column names
    dw.write_dict({column: value})

    # write a list with items in same order as dw.add() calls
    dw.write_list([value, value, ...])

    # one parameter for each .add() call, in same order
    dw.write(value, value, ...)
\end{python}
Several datasets can be created simultaneously using multiple writers
with different names.



\subsection{Create in \synthesis}
\label{sec:create_in_synthesis}
Since a dataset is sliced in multiple disjoint sets, and \synthesis is
run only once, data has to be sliced during writing somehow.  There
are two possible ways to do this.  One is to first set a slice number
\begin{python}
    dw.set_slice(sliceno)
\end{python}
before writing data into that slice.  Note that this can only be done
once per slice.  The other is to use one of the \texttt{split\_write}
functions
\begin{python}
    # use a dict-writer
    writer = dw.get_split_write_dict()
    writer({column: value})

    # use a list-writer
    writer = dw.get_split_write_list()
    writer([value, value, ...])

    # use a parameterised writer
    writer = dw.get_split_write()
    writer(value, value, ...)
\end{python}
These writers will write round-robin if the dataset is not hashed, and
to the ``matching'' slice if the dataset is hash partitioned.



\subsection{Creating a Dataset Chain in one Job}
It is possible to create a chain of datasets by inserting a writer
object as previous to another writer, like this.
\begin{python}
def prepare(job):
    dw1 = job.datasetwriter(name='ds1')
    dw2 = job.datasetwriter(name='ds2', previous=dw1)
\end{python}



\subsection{Completing Dataset Creation}
Normally, there is no need to tell the \texttt{DatasetWriter} that the
last line of data is written.  This is handled automatically when the
method exits.  In some situations, such as when a dataset is to be
used by a \texttt{subjob} launched from the creating method, it is
necessary to manually tell the writer that the dataset is complete.
This is done by calling \texttt{finish()} as shown below
\begin{python}
dw.finish()
\end{python}
The \texttt{finish()}-call returns a dataset object, so the just
finished dataset could be put in use immediately, like this
\begin{python}
ds = dw.finish()
it = ds.iterate(None, 'user')
\end{python}



\subsection{Datasets Created by Subjobs}
If a dataset is created in a subjob, it is not visible from the build
scripts.  This is solved by linking dataset meta-information to the
job calling the subjob, using the \texttt{.link\_to\_here()} function.
This is explained in detail in section~\ref{sec:subjobs}, but the
basic idea is as follows
\begin{python}
from accelerator import subjobs
def synthesis():
    job = subjobs.build('dataset_creator')
    ds = job.dataset('ds1')
    ds.link_to_here('first_dataset')
\end{python}
Here, the subjob creates a dataset named \texttt{ds1}.  This dataset
resides in the subjob's job directory, but a link to it from
the \textsl{current} job is created using the \texttt{link\_to\_here}
function, which also allows the dataset to be renamed.  The current
job will now claim that it contains a dataset \texttt{first\_dataset}.

In the same call, the subjob's dataset's previous can also be
overridden:
\begin{python}
    ds.link_to_here('first_dataset', override_previous=anotherds)
\end{python}
This might sound complicated, but is simply means that the current job
can export a set of datasets created by its subjobs, and that it can
manipulate these datasets so that they may appear to be chained in any
way the user finds fit.



\subsection{Creating Hash Partitioned Datasets}
A hash partitioned dataset is created by setting
the \texttt{hashlabel} argument to \texttt{job.datasetwriter}.  For a
hash partitioned dataset, only data fulfilling the hashing requirement
for a slice may be written to that slice, and an exception will be
rised if the data to be written does not belong to the current slice.

A simple way to filter the data to be written is to call
\begin{python}
    dw.enable_hash_discard()
\end{python}
first in each slice or after each \texttt{.set\_slice()}.  Then,
writes that belongs to another slice are silently ignored, while
``correct'' data gets written as expected.

It is possible to check before writing if the data is to be put into
the current slice using the \texttt{dw.hashcheck()} function, like
this
\begin{python}
...
if dw.hashcheck(hashcoldata):
    # compute bulkdata here
    bulkdata = expensive_function(...)
    # and write to dataset
    dw.write(hashcoldata, bulkdata)
\end{python}
This is beneficial if it is expensive to compute the data to be
stored.  In the example above using \texttt{hashcheck()}, data is only
computed if it is to be stored in the slice.

In general, the hash function for a particular column is available like this
\begin{python}
dw.writers[colname].hash
\end{python}
This function can be used to manually check if the data belongs to a
slice.  For more details and alternatives, please see the
documentation in the source file \texttt{dataset.py}.



\subsection{Column Name Restrictions}
\label{sec:valid-column-names}

Column names must be valid Python identifiers.  Invalid characters are
replaced by the underline(\texttt{\_}) character.  The underline
character is also used to make column names unique when necessary.
The table below shows some examples.
\starttablenoheader
\RP \textbf{input} & \textbf{converted} & \textbf{comment}\\\midrule
\RP \texttt{"-"}      &  \texttt{"\_"}       & Converting to valid python identifier.\\
\RP \texttt{"a b"}    &  \texttt{"a\_b"}     & Converting to valid python identifier.\\
\RP \texttt{"42"}     &  \texttt{"\_42"}     & Converting to valid python identifier.\\
\RP \texttt{"print"}  &  \texttt{"print\_"}  & \texttt{print} is a keyword (in py2).\\
\RP \texttt{"print@"} & \texttt{"print\_\_"} & \texttt{print\_} was just taken.\\
\RP \texttt{"None"}   &  \texttt{"None\_"}   & \texttt{None} is a keyword (in py3).\\
\stoptable





\subsection{More Advanced Dataset Creation}
Currently out-of-scope of this manual.  Please see the source file
\texttt{dataset.py} for full information.



\section{Appending New Columns to an Existing Dataset}
\label{sec:appending_new_columns}

With minimal overhead, existing datasets could be extended with new
columns.  Internally, this is implemented by storing the new column
data together with a pointer to the original, ``parent'', dataset.

Appending new columns works the same way as when creating a dataset,
with the exception that a link to a dataset that is to be appended to
is input to the writer constructor.  Columns can be appended either
in \texttt{analysis} or \texttt{synthesis}, as shown in the two
following sections.  Note that appending a column does only apply to
one single dataset, and not to the complete chain of datasets, if
present.  See the blog at \texttt{exax.org} for several examples of
how to append columns to a dataset chain.


\subsection{Appending New Columns in Analysis}

The following example appends one column to an existing
dataset \texttt{source}, while chaining to the
dataset \texttt{previous}.
\begin{python}
datasets = ('source', 'previous',)

def prepare(job):
    dw = job.datasetwriter(
        parent=datasets.source,
        previous=datasets.previous,
        caption='with the new column'
    )
    dw.add('newcolname', 'unicode')
    return dw

def analysis(sliceno, prepare_res):
    dw = prepare_res
    for data in datasets.source.iterate(sliceno, ... )
        ...
        dw.write(value)
\end{python}
The \texttt{DatasetWriter} will automatically check that the number of
appended rows does match the number of rows in the parent dataset.
Otherwise, an error will be issued and execution will terminate.
Typically new columns are derived from existing ones, so this is
usually not a problem.

\subsection{Appending New Columns in Synthesis}

A straightforward way to append columns to a dataset in synthesis is
using the \texttt{set\_slice()} function, as shown in the example
below.
\begin{python}
def synthesis(job):

    dw = job.datasetwriter(parent=datasets.source)
    dw.add('newcolumn', 'json')
    
    for sliceno in range(job.params.slices):
        dw.set_slice(sliceno)
        for data in datasets.source.iterate(sliceno, ...):
            ...
            dw.write(x)
\end{python}
Note that the \texttt{for}-loop over all slices is
controlling \textsl{both} the reading iterator \textsl{and} the
dataset writer.  Note also that \texttt{.set\_slice()} can only be
called once per slice.



\section{Standard Methods on Datasets}
Figure~\ref{fig:ds_ops} shows an example of how the standard
methods \texttt{csvimport}, \texttt{csvexport}, \texttt{dataset\_sort}, \texttt{dataset\_hashpart},
and \texttt{dataset\_unroundrobin} can be used to change the storage
of a dataset.

In the upper part of the figure, a file \texttt{data.csv} in a column
storage format is imported using the \texttt{csvimport} method.  This
method reads one line at a time and writes to the a new dataset's
slices in a round robin fashion.

In many applications, this round robin reorder does not matter, but in
some cases, such as when the input data is a time series, it is
critical to keep the order of the data intact.  In those situations,
the \texttt{dataset\_unroundrobin} is be used to ``transpose'' the
dataset.  One can think of is as if the method reads data row by row,
but writes it slice by slice.  If the \texttt{csvexport} method is
applied to an ``unroundrobined'' dataset, the output CSV file will be
in the same order as the original input file.

The \texttt{dataset\_unroundrobin} method talkes a
parameter \texttt{trigger\_column} which is used to make sure that a
slice switch does not appear as long as the data in that column is a
constant value.  If the data is sorted, this ensures that all rows
with a particular value is not spread along several slices.  In
figure~\ref{fig:ds_ops}, the transition from slice~1 to slice~2
appears while the method is writing the value ``10''.  If
\texttt{trigger\_column} is enabled, switching is delayed until all ``10''s
are written.

The \texttt{dataset\_hashpart} method is used to partition the data
into disjoint slices based on the content of a single column.
Depending on the distribution of the data, the generated dataset will
be more or less balanced.

Finally, the \texttt{dataset\_sort} method is used to sort data.  It
can sort data independently per slice, or sort all the data in a
dataset across all slices.  In the latter case, there is
a \texttt{trigger\_column} option similar to the one
in \texttt{dataset\_unroundrobin} that ensures that slice switches
appear only when the value of a particular column is changing.  This
is the reason for the first column in the bottom right dataset in
figure~\ref{fig:ds_ops} to be so long.  Slice switching should happen
while writing the ``10'' value, but since the \texttt{trigger\_column}
is set, switching is delayed until all ``10''s are written.

\begin{figure}
  \begin{center}
     \input{figures/dataset_standard_ops.pdftex_t}
     \caption{An example of how a dataset (with three slices) is modified by some common standard methods.}
     \label{fig:ds_ops}
  \end{center}
\end{figure}
