This section covers the Accelerator's most important Classes.

\section{The \texttt{Job} and \texttt{CurrentJob} Classes}
The \texttt{Job} class is used to represent and operate on existing
jobs.  An object of this class is returned from job \texttt{build()}
calls as well as when retreiving jobs from Urd or a \texttt{JobList}
object.  The \texttt{CurrentJob} class is an extension that provides
mechanisms for operations performed while a job is executing, such as
saving files to the job's jobdir.  The classes are derived from
the \texttt{str} class, and objects of these classes decay to
(unicode) strings when pickled.

The following attributes are available on both the \texttt{Job}
and \texttt{CurrentJob} classes:
\starttabletwo
\texttt{method} & The job method.  This can be overriden by \texttt{name=} if job instance is output from Urd or a \texttt{build()} call.\\
\texttt{path} & The filesystem directory where the job is stored.\\
\texttt{workdir} & The workdir name (the part before \texttt{-number} in the jobid).\\
\texttt{number} & The job number as an \texttt{int}.\\
\texttt{filename()} & Return absolute path to a file in a job.\\
\texttt{withfile()} & A \texttt{JobWithFile} with this job.\\
\texttt{params} & Return a dict corresponding to the file \texttt{setup.json} for this job.\\
\texttt{post} & Return a dict corresponding to \texttt{post.json} for this job.\\
\texttt{open()} & Similar to standard \texttt{open}, use to open files\\
\texttt{load()} & Load a pickle file from the job's directory.\\
\texttt{json\_load()} & Load a json file from the job's directory.\\
\texttt{dataset()} & Return a named dataset from the job.\\
\texttt{datasets} & List of datasets in this job.\\
\texttt{output()} & Return what the job printed to \texttt{stdout} and \texttt{stderr}.\\
\stoptabletwo
\noindent In addition, the \texttt{CurrentJob} class has these
attributes too:
\starttable
\texttt{datasetwriter()} && to get a DatasetWriter object.  See documentation for \texttt{Dataset.DatasetWriter()}, section~\ref{}.\\
\texttt{open()}&& With extra temp argument.\\
\texttt{save()} && to store a pickle file\\
\texttt{json\_save()} && to store a json file\\
\stoptable
\noindent Detailed description of the functions, where neccessary, follows.

@@@@@@@@@@ Intro till sliced files var???

\subsection{\texttt{Job.filename()}}
\begin{leftbar}
Return the absolute (full path) filename to a file stored in the job.
If the file is sliced, a particular slice file can be retrieved using
the \texttt{sliceno} parameter.
\starttable
\texttt{filename} & \textsl{Mandatory} & Name of file in job directory.\\
\texttt{sliceno}  & \pyNone & Set to current slice number if sliced, otherwise \pyNone.\\
\stoptable
\end{leftbar}


\subsection{\texttt{Job.open()}}
\begin{leftbar}
This is a wrapper around the standard \texttt{open} function with some
extra features.  Note that
\begin{itemize}
\item[--]  \texttt{Job.open()} can only read files, not write
them, and therefore ``\texttt{r}'' flag must be set.
\item[--]  \texttt{CurrentJob.open()} can both read and write.
\item[--]  \texttt{CurrentJob.open()} must be used as a context manager,
like this
\begin{python}
with job.open(...) as fh:
    ....
\end{python}
\item[--]  \texttt{CurrentJob.open()} can use the \texttt{temp} flag to
modify the persistence of written files.
\end{itemize}
\starttable
\texttt{filename} & \textsl{Mandatory} & Name of file.\\
\texttt{mode} & \texttt{r} & Open file in this mode, see Python's \texttt{open()}\\
\texttt{sliceno} & \pyNone & Read or write sliced files.\\
\texttt{encoding} & \pyNone & Same as Python's \texttt{open()}\\
\texttt{errors} & \pyNone & Same as Python's \texttt{open()}\\
\texttt{temp} & \pyNone & Control file persistence.  See text.\\
\stoptable
The \texttt{temp} argument is used to control the persistence of files
written using \texttt{.open()}.  This is useful mainly for debug
purposes, and explained in section~\ref{sec:debugflag}.  Sliced files
are described in section~\ref{sec:slicedfiles}.
\end{leftbar}


\subsection{\texttt{Job.withfile()}}
\begin{leftbar}
The \texttt{.withfile()} is used to highlight a specific file in a job
and feed it to another job \texttt{build()}.  The file could be
sliced.
\starttable
\texttt{filename} & \textsl{Mandatory} & Name of file.\\
\texttt{sliced} & \pyFalse & Boolean indicating if the file is sliced or not.\\
\texttt{extra} & \pyNone & Any additional information to the job to be built.\\
\stoptable
\end{leftbar}


\subsection{\texttt{Job.load()}}
\begin{leftbar}
Load a file from a job in Python's pickle format.
\starttable
\texttt{filename} & \texttt{result.pickle} & \hspace{2ex}Name of file.\\
\texttt{sliceno} & \pyNone & \\
\texttt{encoding} & \texttt{bytes} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Job.json\_load()}}
\begin{leftbar}
Load a file from a job, in JSON format.
\starttable
\texttt{filename} & \texttt{result.json} & Name of file.\\
\texttt{sliceno} & \pyNone & \\
%\texttt{unicode\_as\_utf8bytes} & \texttt{PY2} & \\
\stoptable
%The \texttt{unicode\_as\_utf8bytes} flag is active in Python2 in order
%to let strings decode as bytes.  Override this for unicode.  In
%Python3, strings decode as unicode.
\end{leftbar}


\subsection{\texttt{Job.dataset()}}
\begin{leftbar}
Get a dataset instance from a job.
\starttable
\texttt{name} & \texttt{default} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Job.output()}}
\begin{leftbar}
Get everything a job has printed to \texttt{stdout}
and \texttt{stderr} in a string variable.
\starttable
\texttt{what} & \pyNone & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Job.save()}}
\begin{leftbar}
For \texttt{CurrentJob} instances only.  Save data into the current
job's directory in Python's pickle format.
\starttable
\texttt{obj} & \textsl{Mandatory} & \\
\texttt{filename} & \texttt{result.pickle} & \\
\texttt{sliceno} & \pyNone & \\
\texttt{temp} & \pyNone & \\
\stoptable
The \texttt{temp} argument is used to control the persistence of files
written using \texttt{.save()}.  This is useful mainly for debug
purposes, and explained in section~\ref{sec:debugflag}.
\end{leftbar}


\subsection{\texttt{Job.json\_save()}}
\begin{leftbar}
For \texttt{CurrentJob} instances only.  Save data into the current
job's directory in JSON format.
\starttable
\texttt{obj} & \textsl{Mandatory} & \\
\texttt{filename} & \texttt{result.json} & \\
\texttt{sliceno} & \pyNone & \\
\texttt{sort\_keys} & \pyTrue & \\
\texttt{temp} & \pyNone & \\
\stoptable
The \texttt{temp} argument is used to control the persistence of files
written using \texttt{.json\_save()}.  This is useful mainly for debug
purposes, and explained in section~\ref{sec:debugflag}.
\end{leftbar}


\subsection{Sliced Files}
\label{sec:slicedfiles}
\begin{leftbar}
A \textsl{sliced} file is actually a set of files used to store data
independently in each \analysis process using a common name.  The
functions that operate on files, such as for example \texttt{.open()}
and \texttt{.load()}, can switch to sliced files using
the \texttt{sliceno} parameter.  From a user's perspective, they
always appear to work on single files.  For example
\begin{python}
def analysis(sliceno, job):
    data = ...
    job.date(data, "mydata", sliceno=sliceno, temp=False)
\end{python}
will create a set of files \texttt{mydata.\%d}, where \texttt{\%d} is
replaced by the slice number.  In this way, data can be passed ``in
parallel'' between different jobs.
\end{leftbar}


\subsection{File Persistence}
\label{sec:debugflag}
\begin{leftbar}
@@@@@@@@@@@@@@ FÃ¥r inte detta att funka, file verkar aldrig sparas vid
DEBUG eller DEBUGTEMP.  Via --flags=debug

The \texttt{temp} argument controls persistence of files stored
using \texttt{.open()}, \texttt{.save()}, or \texttt{.json\_save()}.
By default it is being set to \pyFalse, which implies that the stored
file is \textsl{not} temporary.  But setting it to \pyTrue, like in
the following
\begin{python}
job.save(data, filename, temp=True)
\end{python}
will cause the stored file to be deleted upon job completion.  The
argument takes two additional values, \texttt{DEBUG} and
\texttt{DEBUGTEMP}, working like this
\begin{snugshade}
\begin{center}
\begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}ll}
  \texttt{temp}      & ``normal'' mode     & debug mode  \\\hline
  \pyFalse           & stored              & stored\\
  \pyTrue            & stored and removed  & stored and removed\\
  \texttt{DEBUG}     &                     & stored\\
  \texttt{DEBUGTEMP}\hspace{4ex} & stored and removed  & stored\\
\end{tabular*}
\end{center}
\end{snugshade}
\noindent Debug mode is active if the Accelerator is started with the
\texttt{--debug} flag.  For example
\begin{python}
from accelerator import Temp
def analysis(sliceno, job):
    data = ...
    # save only if --debug
    job.save(data, filename, sliceno=sliceno, temp=Temp.DEBUG)
    # save always, but remove unless --debug
    job.save(data, filename, sliceno=sliceno, temp=Temp.DEBUGTEMP)
\end{python}
\end{leftbar}





\section{The \texttt{JobWithFile} Class}
The \texttt{JobWithFile} class is used to create a job input parameter
from a file stored in a job.
\starttabletwo
\texttt{resolve()} & Return filename. \\
\texttt{load()} & load file contents. \\
\texttt{json\_load()} & load JSON file contents.\\
\stoptabletwo
All three functions take the argument \texttt{sliceno}, which default
is set to \pyNone, indicating that it is actually a single file on
disk.  If \texttt{sliceno} is set, it is assumed that the file is
sliced, see section~\ref{sec:slicedfiles}, and the function will look
up that slice of the file only.



\section{The \texttt{JobList} Class}
Objects of the \texttt{JobList} class are returned by member functions
to the \texttt{Urd} class.  They are used to group sessions of jobs
together.
\starttabletwo
\texttt{find()} & Return a new \texttt{JobList} with only jobs with that method or name in it.\\
\texttt{get()} & Return the latest \texttt{Job} with that method or name.\\
\texttt{[<method>]} & Same as \texttt{.get} but error if no job with that method or name is in the list.\\
\texttt{as\_tuples} &  The \texttt{JobList} represented as \texttt{(method, jid)} \texttt{tuple}s.\\
\texttt{pretty} & Return a prettified string version of the \texttt{JobList}.\\
\texttt{profile} & Total execution time and execution time per method.\\
\texttt{print\_profile()} & Print profiling information to \texttt{stdout}.\\
\stoptabletwo
\noindent Detailed description of the functions, where neccessary, follows.


\subsection{\texttt{Joblist.find()}}
\begin{leftbar}
\starttable
\texttt{method} & \textsl{Mandatory} & Method or name to find.\\
\stoptable
Return a new \texttt{Joblist} will all jobs in the
current \texttt{JobList} matching the \texttt{method} argument.  The
matching part is either the unique name of the method's source code,
or the name optionally given at build time using the \texttt{name=}
argument.
\end{leftbar}


\subsection{\texttt{JobList.get()}}
\begin{leftbar}
\starttable
\texttt{method} & \textsl{Mandatory} & Method or name to find.\\
\texttt{default} & \pyNone & Return the latest matching job.\\
\stoptable
Return the latest job that matches the \texttt{method} argument.  The
matching part is either the unique name of the method's source code,
or the name optionally given at build time using the \texttt{name=}
argument.  If no matches are found, it will return
the \texttt{default} argument.
\end{leftbar}


\subsection{\texttt{JobList.print\_profile()}}
\begin{leftbar}
Print total execution time for the \texttt{Joblist}, and,
conditionally, execution time for each job in the list,
to \texttt{stdout}.
\starttable
\texttt{verbose} & \pyTrue & In addition to total time, print execution time for each method in list.\\
\stoptable
\end{leftbar}

\section{The \texttt{Dataset} Class}
The \texttt{Dataset} class is used to operate on small or large
datasets stored on disk.  It decays to a (unicode) string when
pickled.

\starttabletwo
\texttt{columns} & A \texttt{dict} from column to properties, such as type, min, and max values.\\
\texttt{previous} & The dataset's previous dataset, if it exists, \pyNone otherwise.\\
\texttt{parent} & The dataset's parent dataset, if it exists, \pyNone otherwise.\\
\texttt{filename} & The dataset's filename, if it exists.  (\texttt{csvimport} sets this.)\\
\texttt{hashlabel} & Column used for hash partitioning, or \pyNone.\\
\texttt{caption} & The dataset's caption.\\
\texttt{lines} & A \texttt{list} with number of lines per slice.\\
\texttt{shape} & A tuple containing number of columns and number of lines in dataset.\\
\texttt{link\_to\_here()} & Used to associate a subjob's dataset with the current job, see section~\ref{sec:linktohere}.\\
\texttt{merge()} & Merge this dataset with another dataset, see section~\ref{sec:datasetmerge}.\\
\texttt{column\_filename()} & @@@@@@@@@@@@@@@@@@\\
\texttt{chain()} & A \texttt{DatasetChain} object, see section~\ref{sec:dschain}\\
\texttt{iterate\_chain()} & Iterator over chains, see section~\ref{sec:iterator}.\\
\texttt{iterate()} & Iterator over dataset see section~\ref{sec:iterator}.\\
\texttt{iterate\_list()} & Iterator over a list of datasets, see section~\ref{sec:iterator}.\\
\texttt{new()} & @@@@@@@@@@@@@@@@@@@\\
\texttt{append()} & @@@@@@@@@@@@@\\
\stoptabletwo
\noindent Detailed description of the functions, where neccessary, follows.


\subsection{\texttt{Dataset.link\_to\_here()}}
\begin{leftbar}
Use this to expose a subjob as a dataset in your job, like in this
example:
\begin{python}
def synthesis():
    job = build('ex')
    job.dataset().link_to_here(name='new')
\end{python}
The current job will now appear to have a dataset named \texttt{new},
that is actually a link to the subjob's \texttt{default} dataset.  It
is possible to filter which columns should be visible in the link
using \texttt{column\_filter}.  For chaining purposes, it is possible
for the link to expose a parent dataset of choice, set using
the \texttt{override\_previous} parameter.
\starttable
\texttt{name} & \texttt{default} & The new name of the dataset.\\
\texttt{column\_filter} & \pyNone & Iterable of columns to include, or \pyNone to get all.\\
\texttt{override\_previous} & \texttt{\_no\_override} & Set this to the new previous.\\
\stoptable
\end{leftbar}


\subsection{\texttt{Dataset.merge()}}
\begin{leftbar}
Merge this and other dataset. Columns from the other dataset take
priority.  If datasets do not have a common ancestor you get an error
unless \texttt{allow\_unrelated} is set. The new dataset always has
the previous specified here (even if \pyNone).  Returns the new
dataset.
\starttable
\texttt{other} & \textsl{Mandatory} & Merge with this dataset.\\
\texttt{name} & ``\texttt{default}''& Name of new dataset\\
\texttt{previous} & \pyNone& The new dataset's \texttt{previous} dataset.\\
\texttt{allow\_unrelated} & \pyFalse& Set this if the datasets do not share a common ancestor.\\
\stoptable
\end{leftbar}



\subsection{\texttt{Dataset.column\_filename()}}
@@@@@@@@@@@@@@@@@


\subsection{\texttt{Dataset.chain()}}
\begin{leftbar}
This function will return a \texttt{DatasetChain} object, see
section~\ref{sec:datasetchain}.
\starttable
\texttt{length} & \texttt{-1} & Number of datasets in chain.  The default value of \texttt{-1} will include all datasets in chain.\\
\texttt{reverse} & \pyFalse & Reverse order of chain.\\
\texttt{stop\_ds} & \pyNone & If set, chain will start at the dataset after \texttt{stop\_ds}.\\
\stoptable
\end{leftbar}


\subsection{\texttt{Dataset.new()}}
@@@@@@@@@@@@@@@@


\subsection{\texttt{Dataset.append()}}
@@@@@@@@@@@@@@@@@@@



\section{The \texttt{DatasetChain} Class}
These are lists of datasets returned from Dataset.chain.
They exist to provide some convenience methods on chains.
\starttabletwo
\texttt{min()} & Min value for a specified column over the whole chain.\\
\texttt{max()} & Max value for a specified column over the whole chain.\\
\texttt{lines()} & Number of rows in this chain, optionally for a specific slice. \\
\texttt{column\_counts()} & The number of datasets each column appears in.\\
\texttt{column\_count()} & Number of datasets in this chain that contain a specified column.\\
\texttt{with\_column()} & Return a new chain without any datasets that don't contain a specified column.\\
\stoptabletwo
\noindent Detailed description of the functions, where neccessary, follows.


\subsection{\texttt{DatasetChain.min()}, \texttt{DatasetChain.max()}}
\begin{leftbar}
\starttable
\texttt{column} & \textsl{Mandatory} & Min/max value of column, see text.\\
\stoptable
Minimum or maximum value for column over the whole chain.  Will be
\pyNone if no dataset in the chain contains \texttt{column}, if all datasets are
empty or if \texttt{column} has a type without min/max tracking.
\end{leftbar}


\subsection{\texttt{DatasetChain.lines()}}
\begin{leftbar}
Number of rows in this chain, optionally for a specific slice.
\starttable
\texttt{sliceno} & \pyNone & If set, return number of lines in speficied slice.\\
\stoptable
\end{leftbar}


\subsection{\texttt{DatasetChain.column\_counts()}}
\begin{leftbar}
Return a Python \texttt{Counter},\mintinline{python}|{colname:
occurances}|, holding the number of datasets each column appears in.
Takes no options.
\end{leftbar}


\subsection{\texttt{DatasetChain.column\_count()}}
\begin{leftbar}
Number of datasets in this chain that contain a specified column.
\starttable
\texttt{column} & \textsl{Mandatory} & A column name.\\
\stoptable
\end{leftbar}


\subsection{\texttt{DatasetChain.with\_column()}}
\begin{leftbar}
Return a new \texttt{DatasetChain} with all datasets in this chain
containing a speficied column.
\starttable
\texttt{column} & \textsl{Mandatory} & A column name.\\
\stoptable
\end{leftbar}



\section{The \texttt{DatasetWriter} Class}
\starttabletwo
\texttt{add()} & Add a new column to the dataset under creation.\\
\texttt{hashcheck()} & Check if value belongs in current slice.\\
\texttt{set\_slice()} & Set which slice that will receive the next write.\\
\texttt{column\_filename()} & @@@@@@@@@@@@@@\\
\texttt{enable\_hash\_discard()} & Make the write functions silently discard data that does not hash to the current slice. \\
\texttt{get\_split\_write()} & Get a writer object, see section~\ref{}.\\
\texttt{get\_split\_write\_list()} & Get a writer object, see section~\ref{}.\\
\texttt{get\_split\_write\_dict()} & Get a writer object, see section~\ref{}.\\
\texttt{close()} & @@@@@@@@@@@@@@@@@@@@@@@@@@@@@\\
\texttt{discard()} & Discard the dataset under creation@@@@@@@@@@@@@@22\\
\texttt{set\_lines()} & @@@@@@@@@@@@@@@@@\\
\texttt{set\_minmax()} & @@@@@@@@@@@@@@2\\
\texttt{finish()} & Call this if dataset is to be used before creating job finishes, e.g.\ if the dataset under creation is input to a subjob.\\
\stoptabletwo
\noindent Detailed description of the functions, where neccessary, follows.


\subsection{\texttt{DatasetWriter.add()}}
Add a new column to a dataset in creation.
\begin{leftbar}
\starttable
\texttt{colname} & \textsl{Mandatory} & Name of new column.\\
\texttt{coltype} & \textsl{Mandatory} & Type of new column.\\
\stoptable
All dataset types are described in section~\ref{}.
\end{leftbar}


\subsection{\texttt{DatasetWriter.hashcheck()}}
Check if a value belongs to the current slice.
\begin{leftbar}
\starttable
\texttt{value} & \textsl{Mandatory} & Some data/\\
\stoptable
Return \pyTrue if \texttt{value} belongs to the current
slice, \pyFalse otherwise.
\end{leftbar}


\subsection{\texttt{DatasetWriter.set\_slice()}}
Specify which slice that will receive the next write(s).  Use this if
writing data in \prepare or \synthesis.
\begin{leftbar}
\starttable
\texttt{sliceno} & \textsl{Mandatory} & Slice number to use for writing.\\
\stoptable
\end{leftbar}


\subsection{\texttt{DatasetWriter.column\_filename()}}
@@@@@@@@@@@@@@@@@@@
\begin{leftbar}
\starttable
\texttt{} & \texttt{} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{DatasetWriter.enable\_hash\_discard()}}
@@@@@@@@@@@@@@@@@@@
\begin{leftbar}
Takes no options.  Set this in each slice or after
each \texttt{set\_slice()} to make the writer discard values that do
not belong to the current slice.
\end{leftbar}


\subsection{\texttt{DatasetWriter.set\_lines()}}
@@@@@@@@@@@@@@@222
\begin{leftbar}
\starttable
\texttt{} & \texttt{} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{DatasetWriter.set\_minmax()}}
@@@@@@@@@@@@@@@@
\begin{leftbar}
\starttable
\texttt{} & \texttt{} & \\
\stoptable
\end{leftbar}


\begin{verbatim}
    Create in prepare, use in analysis. Or do the whole thing in
        synthesis.

    You can pass these through prepare_res, or get them by trying to
        create a new writer in analysis (don't specify any arguments except
            an optional name).

    There are three writing functions with different arguments:

    dw.write_dict({column: value})
        dw.write_list([value, value, ...])
            dw.write(value, value, ...)

    Values are in the same order as you add()ed the columns (which is in
        sorted order if you passed a dict). The dw.write() function names the
            arguments from the columns too.

    If you set hashlabel you can use dw.hashcheck(v) to check if v
        belongs in this slice. You can also call enable_hash_discard
            (in each slice, or after each set_slice), then the writer will
                discard anything that does not belong in this slice.

    If you are not in analysis and you wish to use the functions above
        you need to call dw.set_slice(sliceno) first.

    If you do not, you can instead get one of the splitting writer
        functions, that select which slice to use based on hashlabel, or
            round robin if there is no hashlabel.

    dw.get_split_write_dict()({column: value})
        dw.get_split_write_list()([value, value, ...])
            dw.get_split_write()(value, value, ...)

    These should of course be assigned to a local name for performance.

    It is permitted (but probably useless) to mix different write or
        split functions, but you can only use either write functions or
            split functions.

    You can also use dw.writers[colname] to get a typed_writer and use
        it as you please. The one belonging to the hashlabel will be
            filtering, and returns True if this is the right slice.

    If you need to handle everything yourself, set meta_only=True and
        use dw.column_filename(colname) to find the right files to write to.
            In this case you also need to call dw.set_lines(sliceno, count)
                before finishing. You should also call
                    dw.set_minmax(sliceno, {colname: (min, max)}) if you can.
\end{verbatim}










\section{The \texttt{Urd} Class}
\starttabletwo
\texttt{get()} & Get an Urd item from a specified list and timetamp.\\
\texttt{latest()} & Get the latest Urd item for a specified list.\\
\texttt{first()} & Get the first Urd item for a specified list.\\
\texttt{peek()} & Get an Urd item from a specified list and timetamp without recording.\\
\texttt{peek\_latest()} & Get the latest Urd item for a specified list without recording.\\
\texttt{peek\_first()} & Get the first Urd item for a specified list without recording.\\
\texttt{since()} & Get all timestamps later than a specified timestamp for a specified list.\\
\texttt{list()} & List all Urd lists\\
\texttt{begin()} & Start a new Urd session.\\
\texttt{abort()} & Abort a running Urd session.\\
\texttt{finish()} & Finish a running Urd session and store its contents.\\
\texttt{truncate()} & Discard all Urd items later than a specified timestamp for a specified list.\\
\texttt{set\_workdir()} & Set the target workdir.\\
\texttt{build()} & Build a job.\\
\texttt{build\_chained()} & Build a job with chaining.\\
\texttt{warn()} & @@@@@@@@@@@@@@@\\
\stoptabletwo
\noindent Detailed description of the functions, where neccessary, follows.


\subsection{\texttt{Urd.get()}}
Get an Urd item with specified list and timestamp.  The operation is
recorded in the current Urd session.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\texttt{timestamp} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.latest()}}
Get the latest job in a specified Urd list.  The operation is recorded
in the current Urd session.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.first()}}
Get the first job in a specified Urd list.  The operation is recorded
in the current Urd session.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.peek()}}
Same as \texttt{.get()}, but without recording the dependency.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\texttt{timestamp} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.peek\_latest()}}
Same as \texttt{.latest()}, but without recording the dependency.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.peek\_first()}}
Same as \texttt{.first()}, but without recording the dependency.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.since()}}
Return a list of all timestamps more recent than the
input \texttt{timestamp} for a specified Urd list.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\texttt{timestamp} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.list()}}
Return a list of all available Urd lists.
\begin{leftbar}
\end{leftbar}


\subsection{\texttt{Urd.begin()}}
Start a new Urd session.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\texttt{timestamp} & \textsl{Mandatory} & \\
\texttt{caption} & \pyNone & \\
\texttt{update} & \pyFalse & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.abort()}}
Abort the current Urd session, discard its contents.
\begin{leftbar}
\end{leftbar}


\subsection{\texttt{Urd.finish()}}
Finish the current Urd session and store it in the Urd database.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\texttt{timestamp} & \textsl{Mandatory} & \\
\texttt{caption} & \pyNone & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.truncate()}}
Discard everything later than \texttt{timestamp} for the specified Urd
list.
\begin{leftbar}
\starttable
\texttt{path} & \textsl{Mandatory} & \\
\texttt{timestamp} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.set\_workdir()}}
Set target workdir.  It can be set to any workdir present in the
Accelerator's configuration file.
\begin{leftbar}
\starttable
\texttt{workdir} & \textsl{Mandatory} & \\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.build()}}
Build a job.  If an Urd session is running, the job and its
dependencies will be recorded.
\begin{leftbar}
\starttable
\texttt{method} & \textsl{Mandatory} & Method to build.\\
\texttt{options} & \texttt{\{\}} & Input options.\\
\texttt{datasets} & \texttt{\{\}} & Input datasets.\\
\texttt{jobs} & \texttt{\{\}} & Input jobs.\\
\texttt{name} & \pyNone & Record job using this name instead of method name.\\
\texttt{caption} & \pyNone & Optional caption\\
\texttt{why\_build} & \pyFalse & @@@@@@@@@@@@@@@@@@@@\\
\texttt{workdir} & \pyNone & Store job in this workdir.\\
\stoptable
\end{leftbar}


\subsection{\texttt{Urd.build\_chained()}}
Build a chained job. @@@@@@@@@@@@@@@@
\begin{leftbar}
Same options as \texttt{build}
\end{leftbar}


\subsection{\texttt{Urd.warn()}}
\begin{leftbar}
@@@@@@@@@@@@@@@
\starttable
\texttt{} & \texttt{} & \\
\stoptable
\end{leftbar}




\section{Generate Progress Messages:  the \texttt{status} Module}

The status module is used by the Accelerator to report processing
state.  It is also used by various functions to report iterator and
file access progress.  Status messages are presented in
the \texttt{run} shell by pressing \texttt{C-t}, i.e.\
the \texttt{Ctrl} and \texttt{t} keys simultaneously.

The status module can be used to write progress and status messages
for any function.  Here is an example of how to use the status module
\begin{python}
from accelerator.status import status
...
def analysis(sliceno):
    msg = "reached line %d already!"
    with status(msg % (0,) as update:
        for ix, data in enumerate(datasets.source.iterate(sliceno, 'data')):
            if ix % 1000000 == 0:
                update(msg % (ix,))
\end{python}
In the example above, the status message will be updated once every
million iteration.  By pressing \texttt{C-t} during its execution, the
user will get a message telling how many lines the iterator has
reached.





%% \section{Share Data Between Jobs:  the \texttt{blob} Module}
%% @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@22222

%% The simplest way to share reasonable amounts of data between jobs is
%% by using the \texttt{blob} module.  This module is a
%% convenience-wrapper around the Python \texttt{pickle} module.

%% Note that the Accelerator will set the ``current work directory'' to
%% the current job directory when building a method, so all files created
%% by a job will be stored in the current job directory, unless the
%% filename contains a path pointing elsewhere.

%% \subsection*{Storing/Loading a  Single File}
%% Data is saved in this way
%% \begin{python}
%% from accelerator import blob
%% def synthesis():
%%     data = ...  # some data created here
%%     blob.save(data, filename)
%% \end{python}
%% The data is loaded like this
%% \begin{python}
%% from accelerator import blob
%% def synthesis()
%%     data = blob.load(jobid, filename)
%% \end{python}
%% The \texttt{jobid} in \texttt{blob.load()} is not mandatory.  It
%% defaults to the current workdir unless specified.



%% \subsection{Storing/Loading a Sliced File}
%% It is also possible to use the \texttt{blob} module in \analysis.
%% From a user's perspective it will look like a single file is being
%% handled, but there is actually one file per slice.  This is how to do
%% it
%% \begin{python}
%% def analysis(sliceno):
%%     # save data in slices like this
%%     blob.save(data, filename, sliceno=sliceno)
%%     # load like this
%%     data = blob.load(filename, sliceno=sliceno)
%% \end{python}

%% Data can be passed ``in parallel'' between different jobs using this
%% feature.



%% \subsection{Default Value}

%% The value of the \texttt{default} parameter is returned if trying to
%% load a file that does not exist, for example
%% \begin{python}
%% x = blob.load('thisfiledoesnotexist', default=dict())
%% \end{python}
%% will set \texttt{x} to an empty dict if loading fails.



%% \subsection{Save Files for Debugging}
%% \label{sec:debugflag}
%% The \texttt{temp} argument controls persistence of the stored files.
%% By default it is being set to \pyFalse, which implies that the stored
%% file is \textsl{not} temporary.  But setting it to \pyTrue, like in
%% the following
%% \begin{python}
%%     blob.save(data, filename, temp=True)
%% \end{python}
%% will cause the stored file to be deleted upon job completion.  The
%% argument takes two additional values, \texttt{DEBUG} and
%% \texttt{DEBUGTEMP}, working like this
%% \vspace{3ex}

%% \begin{center}
%% \begin{tabular*}{\textwidth}{l@{\extracolsep{\fill}}ll}
%%   \texttt{temp=}  & ``normal'' mode     & debug mode  \\\hline
%%   \pyFalse           & stored              & \\
%%   \pyTrue            & stored and removed  & stored and removed\\
%%   \texttt{DEBUG}     &                     & stored\\
%%   \texttt{DEBUGTEMP}\hspace{4ex} & stored and removed  & stored\\
%% \end{tabular*}
%% \end{center}
%% Debug mode is active if the Accelerator is started with the
%% \texttt{--debug} flag.

%% \noindent Example
%% \begin{python}
%% from accelerator.extras import Temp
%% def analysis(sliceno):
%%   # save only if --debug
%%   blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUG)
%%   # save always, but remove unless --debug
%%   blob.save(data, filename, sliceno=sliceno, temp=Temp.DEBUGTEMP)
%% \end{python}



%% \section{Find the Full Path to a File in Another Job}
%% Accessing a file stored in another job from within a method or build
%% script is simple, and the functionality is implemented
%% in \texttt{resolve\_jobid\_filename()}.  The function takes two
%% arguments, a \textsl{jobid} and a \textsl{filename}.  See the example
%% below
%% \begin{python}
%% from accelerator.extras import resolve_jobid_filename

%% jobids = ('oldjob',)

%% def synthesis():
%%     filename = resolve_jobid_filename(jobids.oldjob, 'nameoffile')
%% \end{python}
%% Note that this function works in a build script as well.



\section{Symlinking}
\label{sec:symlinking}
Creating a symlink, for example from the \texttt{result\_directory} to
current workdir, may be implemented in a simple and safe way like this
\begin{python}
from accelerator.extras import symlink @@@@@@@@@@@@@@@@@@@@@@@@

def synthesis(job):

    # create file and write it to jobid
    ...
    with open(filename, 'wb') as fh:
        fh.write(...)

    # create a symlink to filename in result_directory
    symlink(filename, job.result_directory)
\end{python}
The \texttt{extras.symlink} function will write a soft link
to \texttt{filename} in \texttt{job.result\_directory}, overwriting it if
it already exists.



%% \section{job\_post}
%% The \texttt{job\_post} function returns a job's post execution
%% information as a Python \texttt{dict}
%% \begin{python}
%% from accelerator.extras import job_post
%% postinfo = job_post(jobid)
%% \end{python}
%% The post data contains mainly profiling information.



%% \section{json\_encode}
%% \begin{python}
%% from accelerator.extras import json_encode
%% json_encode(variable, sort_keys=True, as_str=False)
%% \end{python}

%% \starttabletwo
%% \RPtwo \texttt{variable} & variable to be serialised.  \texttt{set}s and
%% \texttt{tuple}s will be converted to \texttt{list}s.\\

%% \RPtwo \texttt{sort\_keys} & Sort keys if \pyTrue.\\

%% \RPtwo \texttt{as\_str} & return a \texttt{str} if \pyTrue, \texttt{bytes}
%% otherwise.\\
%% \stoptabletwo



%% \section{json\_decode}
%% \begin{python}
%% from accelerator.extras import json_decode
%% x = json_decode(s)
%% \end{python}
%% Return a datastructure defined by the string \texttt{s}.




%% \section{json\_save}
%% \begin{python}
%% from accelerator.extras import json_save
%% json_save(variable,
%%     filename='result',
%%     jobid=None,
%%     sliceno=None,
%%     sort_keys=True,
%%     _encoder=json_encode,
%%     temp=False
%% )
%% \end{python}




%% \section{json\_load}
%% \begin{python}
%% from accelerator.extras import json_load
%% x = json_load(
%%     filename='result',
%%     jobid='',
%%     sliceno=None,
%%     default=None,
%%     unicode_as_utf8bytes=PY2
%% )
%% \end{python}
%% The \texttt{unicode\_as\_utf8bytes} flag is active in Python2 in order
%% to let strings decode as bytes.  Override this for unicode.  In
%% Python3, strings decode as unicode.





\section{DotDict}
\begin{verbatim}
    """Like a dict, but with d.foo as well as d['foo'].
    d.foo returns '' for unset values by default, but you can specify
    _attr_default and _item_default constructors (or None to get errors).
    Normally you should specify _default to set them both to the same thing.
    The normal dict.f (get, items, ...) still return the functions.
\end{verbatim}



\section{gzutil}
\begin{python}
with gzutil.GzUnicodeLines(filename, strip_bom=True) as fh:
\end{python}



\section{profile\_jobs}
Returns a float corresponding to the total execution time (in seconds)
of an input list of jobids.
\begin{python}
from accelerator.automata_common import profile_jobs
...
print(profile_jobs(urd.joblist))
\end{python}
