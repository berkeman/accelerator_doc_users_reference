\section{Dataset Iterators}

Iterators are used to stream dataset data one line at a time into a
method.  Data may be streamed from a single dataset, or from a chain
of datasets, one dataset at a time.


\subsection{Basic Iteration}

The iterator iterates over specified data columns.  The output will be
one line at a time, formatted as a tuple.

\subsubsection*{A Simple Parallel Iterator Invocation}
Read variables X and Y in all slices in parallel

\begin{python}
def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate_chain(sliceno, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
\\
data is iterated over all rows and all datasets in the chain, running
in increasing jobid direction.


\subsubsection*{Iterate over all data in synthesis}
It is possible to iterate over all slices in a single loop by
specifying sliceno=None as follows

\begin{python}
def synthesis():
    for user, item in datasets.source.iterate_chain(None, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
\\
slices will be iterated one at at time in increasing order.


\subsection{Special cases, iterating over all or a singe column}
If column names are omitted, the iterator will produce a tuple of data
including all columns.  %%%% NB: column order???

\begin{python}
for items in iterate_datasets(sliceno, None, jobids):
    print(items)  # is a tuple of all columns
\end{python}
\\
Also, if only one column is specified, it is possible to have the
iterator produce a scalar directly

\begin{python}
# alternative 1, use lists/tuples always
for user in iterate_datasets(sliceno, ('USER',), jobids):
    userset.add(user[0])  # user is a tuple

# alternative 2, target is tuple
for user, in iterate_datasets(sliceno, ('USER',), jobids):
    userset.add(user)

# alternative 3, input is string, not list
for user in iterate_datasets(sliceno, 'USER', jobids):
    userset.add(user)
\end{python}
\\
Both styles are supported by filters and translators.




\subsection{Halting Iteration}

Iteration over a dataset chain will continue until all data is
exhausted.  Sometimes, one wish to limit the number of iterations.
There are several mechanisms for that, and they may be combined in the
same expression.  If so, iteration will be over the shortest range of
the conditions.

\subsubsection*{Halting using length}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    length = options.length):
\end{python}
\\
This will iterate for options.length number of datasets.  Note that a
length of -1 iterates without bounds and is the default.


\subsubsection*{Halting using stop\_jobid}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    stop_jobid = datasets.stopjob):
\end{python}

A more advanced, but very useful, method is to stop at a dataset that
is input to another job

\subsubsection*{Halting using another job's input parameters}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    stop_jobid = {jobids.preprocess: 'source',}):
\end{python}
\\
this will iterate until reaching end or the dataset
job\_params(jobids.preprocess).datasets.source.

It is also possible to iterate over a specified data range.  This
works for datasets that are sorted on the column of choice.

\subsubsection*{Iterating over a data range}
\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    range={timestamp, ('2016-01-01', '2016-01-31'),}):
\end{python}
\\
this will limit the iterator to exactly the range of lines that
fulfill the range condition.  It is relatively costly to filter each
line, and there is a speed advantage by specifying sloppy\_range,
which will iterate over all datasets that contain part of the range.

\subsection{Iterating in the reverse direction}

It is possible to iterate the dataset chain in the backwards direction
by specifying reverse=True, but note that iteration is always in the
forward direction within each dataset.

\begin{python}
for user, item in datasets.source.iterate_chain(
    sliceno,
    columns=('user', 'item',),
    reverse=True):
\end{python}

@@@ HASHLABEL FOR AVOIDING MISTAKES!!!

\subsection{iterate\_list}
PLACEHOLDER
%\begin{python}
%  # def iterate_list(sliceno, columns, jobids, hashlabel=None, pre_callback=None, post_callback=None, filters=None, translators=None):
%  from dataset import Dataset
%  for item in Dataset.iterate_list(None, None, datasets.source):
%    print(item)
%\end{python}


\subsection{Translators}

Translators transform data values. A translator is either a callable or
a \texttt{dict}.  Translators are similar to \texttt{filters}, and
always executed before filtering.

The idea behind translators and filters is that they provide a way to
modify code behavior by supplying functions as iterator options.  In
this way, it is possible to write re-useable simple methods that still
could be altered significantly in behavior.



\subsubsection*{Callable Translator}

A translator function is called with a tuple and is expected to return
a tuple of the same length.  Using translator functions it is possible
to mix different columns with each other before sending them to the
iterator output.  Here is an example

\begin{python}
def merger(user, item):
    return "%s:%s" % (user, item), None
for merge, _ in iterate_datasetchain(None, ('user', 'item',), tip_jobid=jobid,
                                     translator=merger):
    ...
\end{python}
\\
The purpose of this translator is to convert each
\texttt{(user, item)} tuple to a string \texttt{user:item}.  This is
the first output of the translator and iterator and is stored in the
\texttt{merge} variable.  The second output variable is not used in
this application, but a variable still has to be assigned.



\subsubsection*{Translator dict}

One or more columns may be translated independently using a translator
dictionary, specified as \texttt{\{name:\ translation\}}.  A
translation may be either a \texttt{dict} or a callable.  Examples of
both kinds are shown below.

First an example illustrating the use of a translation \texttt{dict}.
Here, integers are translated into more comprehensible strings.

\begin{python}
mapper = {2: 'HUMANLIKE', 4: 'LABRADOR', 5: 'STARFISH',}
for animal in iterate_datasetchain(None, 'NUM_LEGS', tip_jobid=jobid,
                                     translator={'NUM_LEGS': mapper,}):
    ...
\end{python}
\\
Items missing in a translation-\texttt{dict} yield \texttt{None}.

Second, an example of a translation callable.  In this example, each
user string is output from the iterator reads backwards.

\begin{python}
def reverse(x):
    return x[::-1]
for resu, item in iterate_datasetchain(None, ('USER', 'ITEM',), tip_jobid=jobid,
                                       translator={'USER': reverse,}):
    ...
\end{python}



%\newpage
\subsection{Filters}

Filters are run \emph{after} translators.

Filters decide which rows to pass to the iterator output.  A filter is
either a callable or a \texttt{dict}.


\subsubsection*{Callable Filter}

Callable filters receive the iterator tuple as input.  The output must
be \texttt{True} for the tuple to be output from the iterator,
otherwise the iterator continues reading the next line.

The following two examples, of which the first uses a callable filter,
are functionally equivalent

\begin{python}
# Ex1.  filter users that are bearded and longhaired
filter=lambda line: line[1] and line[2]
for user, beard, hair in iterate_datasetchain(None, ('user', 'bearded', 'longhaired'),
                                       tip_jobid=jobid, filters=filter):
    ...

# Ex2.  filter users that are bearded and longhaired
for user, beard, hair in iterate_datasetchain(None, ('user', 'bearded', 'longhaired'),
                                       tip_jobid=jobid):
    if beard and hair:
        ...
\end{python}




\subsubsection*{Filter Dict}

It is possible to filter on one or more columns independently using a
\texttt{dict}.  All filters must be \texttt{True} for a line to be
output from the iterator.  Here are two examples.  The first example
will remove all lines except the ones with valid users.  The second
example will only keep lines with movie items.  (The fact that book is
false is redundant --- missing keys will result in discarded lines.)

\begin{python}
# keep valid users only
validusers={'user1', 'user2', 'user3'}
filters={'user': validusers.__contains__}

# keep valid users with movie items
validitems={'movie': True, 'book': False}
filters={'user': validusers.__contains__, 'item': validitems.get}
\end{python}




\subsubsection*{Filter by Column Values}


Column values could also be used directly, i.e.\ the values get
evaluated by Python.  For example, assume there is a column
\texttt{hasbeard} with values being boolean integers, i.e.\ \texttt{1}
or \texttt{0}.  Bearded users may be collected by
\\
\begin{python}
for user, beard in iterate_datasetchain(None, ('user', 'bearded',), tip_jobid=jobid,
                                       filters={'bearded': None}):
    bearded.add(user)
\end{python}
\\
This may seem strange at first, but it works because the key
for the \texttt{bearded} column exists, and the value is
\texttt{None}, and not a callable nor a \texttt{dict}.




%You can also pass a single name (a str) as names_list, in which case
%you don't get a tuple back (just the
%values). Tuple-filters/translators also get just the value in this
%case (column versions are unaffected).

    


\subsection{Callback}
\label{sec:callback}

It is also possible to inject callback functions, either before or
after each dataset is loaded.  If \texttt{sliceno=None},
i.e.\ iteration runs over all slices of all datasets, it is even
possible to have callback between slice change.

The example below will print dataset \texttt{jobid} for each dataset
prior to iterating over it.
\\
\begin{python}
# pre_callback once per dataset
def prefun(jobid):
    print(jobid)
for user, item in iterate_datasetchain(sliceno, ('user', 'item',), tip_jobid=jobid,
                                       pre_callback=prefun):
    ...
\end{python}
\\
Next is an example of an iterator running over all slices.
The callback function is executed before each new slice is iterated.
Note the difference between this example and the previous.  The
callback function in this example takes two arguments, while the
previous takes only one.

\begin{python}
# callback once per slice
def prefun(jobid, sliceno):
    print(sliceno, jobid)
for user, item in iterate_datasetchain(None, ('user', 'item',), tip_jobid=jobid,
                                       pre_callback=prefun):
    ...
\end{python}
\\
Post-callback is defined similarly.



\subsubsection*{Skipping Datasets and Slices from Callbacks}
It is possible to skip iterations by raising exceptions, as follows
\\
\begin{python}

# Raise this in pre_callback to skip iterating the coming slice
# (if your callback doesn't want sliceno, this is the same as SkipJob)
raise SkipSlice

# Raise this in pre_callback to skip iterating the coing job
raise SkipJob

# Raise this to quit iterating, but with the side effect that
# post_callback will not run.
# @@@ TO BE DEFINED
raise StopIteration
\end{python}
