%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                          %
% Copyright (c) 2018 eBay Inc.                                             %
% Modifications copyright (c) 2019-2020 Anders Berkeman                    %
%                                                                          %
% Licensed under the Apache License, Version 2.0 (the "License");          %
% you may not use this file except in compliance with the License.         %
% You may obtain a copy of the License at                                  %
%                                                                          %
%  http://www.apache.org/licenses/LICENSE-2.0                              %
%                                                                          %
% Unless required by applicable law or agreed to in writing, software      %
% distributed under the License is distributed on an "AS IS" BASIS,        %
% WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. %
% See the License for the specific language governing permissions and      %
% limitations under the License.                                           %
%                                                                          %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\label{chap:iterators}

The basic idea of the Accelerator's datasets is to make it easy to
create parallel programs that can read and write large amounts of data
at a very high speed.  High speed data read access is implemented as a
set of special Python \textsl{iterators}.  Each iterator yields
one \texttt{tuple} at a time containing elements from one or more
specified data columns, one row at a time.  In case of iterating over
a single column, the output may optionally be a scalar instead
of \texttt{tuple} for cleaner code and more efficient computing.



\section{The Three Iterators}
Technically, iterators are members of the \texttt{Dataset} class.
Iterators can be parallel, in \analysis, or sequential, in \prepare
or \synthesis.  There are three iterators available:
\begin{itemize}
\item [] \texttt{iterate()}, for single dataset iteration,
\item [] \texttt{iterate\_chain()} for iterating over dataset chains, and
\item [] \texttt{iterate\_list()} for iterating over a specified list of datasets.
\end{itemize}
And each of them will be discussed later in this chapter.  For
completeness, it should also be mentioned that
the \texttt{DatasetChain} class also has an \texttt{.iterate()}
function, and it works similar to \texttt{iterate\_chain()}.

In many common use cases it is sufficient to provide only two
arguments to the iterator: \texttt{sliceno}, which is mandatory,
and \texttt{columns}.  These, and all other arguments are presented in
detail shortly. A typical use of an iterator looks like this
\begin{python}
datasets = ('source',)

def analysis(sliceno):
    for m, u in dataset.source.iterate(sliceno, ('movie', 'user',)):
        # do something with m and u here...
\end{python}
Python's constructors can be used to create objects from iterators
like in the following example, where the purpose is to compose
a \texttt{dict}.
\begin{python}
n2d = dict(dataset.source.iterate(sliceno, ('name', 'date',)))
\end{python}
or this example
\begin{python}
from collections import Counter
...
    c = Counter(dataset.source.iterate(sliceno, 'user')))
\end{python}
that will create a counter of how many times each \texttt{user} is
present in the dataset.


\newpage
\subsection{Iterator Arguments}

All three iterators share these arguments
\starttable
  \RP \texttt{sliceno} & \textsl{mandatory} & Slice number (an
  integer) to iterate over, \mintinline{python}/None/ to iterate over
  all slices sequentially, or \texttt{roundrobin} to take one value
  per slice in a round robin fashion. \\[1ex]

  \RP \texttt{columns} & \pyNone & Tuple of column labels or a single
  name if iterating over one column.  \pyNone selects all columns in
  alphabetical order.\\[1ex]

  \RP \texttt{hashlabel} & \pyNone & Name of hash column.  If the code
  relies on a dataset being hashed on a particular column, set this to
  make the iterator \texttt{assert} that this is actually the case.
  Execution will terminate if the hashlabel is incorrect.\\[1ex]

  \RP \texttt{rehash} & \pyFalse & Setting this
  to \mintinline{python}/True/ will hash partition the dataset
  on-the-fly based on the \texttt{hashlabel} column.  (Rehashing
  on-the-fly is slower, so ideally datasets should be rehashed using
  the \texttt{dataset\_rehash}
  method~\ref{sec:dataset_hash_partition}.)\\[1ex]
  
%  \RP \texttt{translators} & \pyNone & Translators transform data
%  values.  Explained in section~\ref{sec:translators}\\[1ex]
  
%  \RP \texttt{filters} & \pyNone & Filters decide which rows to
%  include.  Explained in section~\ref{sec:filters}\\[1ex]

  \RP \texttt{status\_reporting} & \pyTrue & Give status when
  pressing \texttt{C-t}.  Unless manually \texttt{zip}ing several
  iterators together, this should be set to
  default \mintinline{python}/True/.  See \texttt{dataset.py} source
  code for full information.\\
\stoptable

\noindent In addition, \texttt{iterate\_chain} takes these arguments too
\starttable
  \RP \texttt{length}&$-1$& Number of datasets in a chain to iterate
  over.  Default is $-1$, which corresponds to all datasets in a
  chain.\\[1ex]
  
  \RP \texttt{range}& \pyNone& Filter rows based on a column's value
  being within a range, see
  section~\ref{sec:iterate_sloppy_range}\\[1ex]

  \RP \texttt{sloppy\_range}& \pyFalse & Used with \texttt{range}, but
  will iterate over full datasets for those datasets i a chain that
  have values within range, see
  section~\ref{sec:iterate_sloppy_range}.\\[1ex]
  
  \RP \texttt{reverse}& \pyFalse & Iterate chain backwards.  Default
  is to iterate forward, i.e.\ from oldest to newest dataset.\\[1ex]

  \RP \texttt{stop\_ds}& \pyNone & Iterate back to this dataset.
  Actually, setting this will iterate from the dataset
  \textsl{following} \texttt{stop\_ds} to the newest dataset in the
  chain.\\[1ex]

  \RP \texttt{pre\_callback}& \pyNone & A function that will be called
  before iterating each dataset.\\[1ex]

  \RP \texttt{post\_callback}& \pyNone & A function that will be
  called after iterating each dataset.\\
\stoptable

\noindent and \texttt{iterate\_list()} takes a \texttt{datasets} parameter
\starttable
  \RP \texttt{datasets} &\pyNone& List of datasets to iterate over.\\
\stoptable



\section{Basic Iteration}
Basic use include iterating in parallel or serial over one dataset or
a chain of datasets.


\subsection{Parallel Iterator Invocation}
For parallel iteration in \analysis, the iterator needs to know the
number of the current slice.  This information can be fed to
the \analysis function in the \texttt{sliceno} variable. The following
is an example of iteration that happens independently in each slice.
\begin{python}
from collections import defaultdict
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate(sliceno, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
The program creates dictionaries mapping \texttt{user}s to sets of
\texttt{item}s for the \texttt{source} dataset.  Assuming that
the dataset is hash partitioned (see ~\ref{sec:slicing_and_hashing}),
this operation is entirely parallel and there is no need to merge all
the results from the analysis processes afterwards, since the
different slices do not share any keys with each other.



\subsection{Sequential Iterator Invocation}
Setting the \texttt{sliceno} parameter to
\pyNone will cause the iterator to run through all slices of
the dataset, one slice at a time, like in this example
\begin{python}
def synthesis():
    h = defaultdict(set)
    for user, item in datasets.source.iterate(None, columns=('user', 'item',)):
        h[user].add(item)
\end{python}
Dataset slices will be iterated in increasing order.



\subsection{Special Case, Round Robin Iteration}
By default, the iterators stream slices of data.  This is almost
always exactly what is needed.  But sometimes, for example when the
order of rows imported by \texttt{csvimport} matters, there is a need
for a row-wise iteration order.  For maximum performance,
the \texttt{csvimport} method writes datasets in a round robin
fashion, so iterating over a csvimported dataset does not return the
lines in the same order as they were written.

By setting the first parameter of any of the iterator functions to
``\texttt{roundrobin}'', the iterator will internally fetch all slice
iterators and return one value at a time from each iterator in a round
robin fashion.  The resulting output is then in the same order as in
the file imported by \texttt{csvimport}.  In a dataset chain, round
robin will happen \textsl{per dataset}.  There is a performance
penalty associated with this functionality, but it is handy for
time-series-like data.



\subsection{Special Cases, Iterating Over All or a Singe Column}
It is possible to iterate over all columns in a dataset by specifying
an empty list of column names, like this
\begin{python}
for items in dataset.source.iterate(sliceno, None):
    print(items)  # is a tuple of all columns
\end{python}
The iterator will output a \texttt{tuple} populated with all column
values for each row.  The columns will be in sorted column name order.

If iterating over a single column, it makes little sense to keep the
output values in a one-dimensional tuple.  A scalar is cleaner and
more efficient.  Here are the two different ways to iterate over a
single column
\begin{python}
# alternative 1, use lists/tuples
for user in datasets.source.iterate(sliceno, ('USER',)):
    userset.add(user[0])  # user is a tuple

# alternative 2, specify column as string, not list
for user in datasets.source.iterate(sliceno, 'USER',):
    userset.add(user)     # user is a scalar!
\end{python}
%Both styles are supported by filters and translators introduced later
%in this chapter.



\subsection{Iterate Over Chains}
The \texttt{iterate\_chain()} iterator is used to iterate over one or
more datasets in a chain, starting at the ``oldest'' dataset.  The
following example will iterate over the last three datasets in the
chain, oldest dataset first.
\begin{python}
datasets = ('source',)

def analysis(sliceno):
    h = defaultdict(set)
    for user, item in datasets.source.iterate_chain(
                           sliceno, columns=('user', 'item',), length=3):
        h[user].add(item)
\end{python}
Using \texttt{iterate\_chain()} without explicitly specifying
\texttt{length} will default to a \texttt{length} of $-1$, which
corresponds to all datasets in the chain.

%% Here is an interesting example of a method that will iterate over all
%% chained \texttt{source} datasets that are new since the last
%% invocation of the method.
%% \begin{python}
%% datasets = ('source',)
%% jobs = ('previous',)

%% def analysis(sliceno):
%%     h = defaultdict(set)
%%     for user, item in datasets.source.iterate_chain(
%%                            sliceno, columns=('user', 'item',),
%%                            stop_ds=jobs.previous.source,):
%%         h[user].add(item)
%% \end{python}






%% \begin{figure}[t!]
%%   \begin{center}
%%     \hspace{2.5cm}\input{figures/dsprocchain.pdftex_t}
%%     \caption{Example of import and processing jobs.  Blue text and
%%     arrows relate to the \textsl{current job}, \texttt{proc-1}.}
%%     \label{fig:dsprocchain}
%%   \end{center}
%% \end{figure}



%% \subsection{An Example}
%% Consider the case where new files are added to a project in a
%% continuous fashion.  These files are imported and chained, so that
%% each new imported dataset links to the previously imported dataset,
%% and so on.  This is illustrated in the left part of
%% figure~\ref{fig:dsprocchain}.

%% Once in a while, but not necessarily at the same rate as new files are
%% added, some processing of the data is performed.  This processing
%% could for example be \textsl{updating} a machine learning model with
%% the newly added data imported since the last model update.

%% The processing job should then iterate over a part of the the dataset
%% chain only, from the first dataset after the last processing job up to
%% the most recent imported dataset.  Again, see
%% figure~\ref{fig:dsprocchain}.  The right part of the image illustrates
%% two builds of the processing method.  The first operates on the
%% dataset \texttt{imp-0}, and the last on \texttt{imp-1}
%% and \texttt{imp-2}.  The input parameters to the first processing
%% job, \texttt{proc-0}, are
%% \begin{itemize}
%% \item[] \mintinline{python}|jobs.previous = None|
%% \item[] \mintinline{python}|datasets.source = 'imp-0'|
%% \end{itemize}
%% and the input parameters for the second processing
%% job, \texttt{proc-1}, are
%% \begin{itemize}
%% \item[] \mintinline{python}|jobs.previous = 'proc-0'|
%% \item[] \mintinline{python}|datasets.source = 'imp-2'|
%% \end{itemize}

%% The processing job should iterate on the \texttt{datasets.source}
%% dataset, and set the \texttt{stop\_id=} parameter to
%% the \texttt{previous} job's \texttt{source} dataset, and the code may
%% look something like this
%% \begin{python}
%% datasets = ('source',)
%% jobs = ('previous',)

%% def analysis(sliceno):
%%     for ... in datasets.source.iterate_chain(
%%             ...
%%             stop_ds={jobs.previous: 'source',}):
%%     ...
%% \end{python}











\section{Halting Iteration}

Iteration over a dataset chain will continue until all datasets are
exhausted or a stop criteria is fulfilled.  There are several
mechanisms for stopping, and they may be combined in a single iterator
call.  If so, iteration will be over the shortest range of the
conditions.

\subsection{Halting Using \texttt{length}}
\begin{python}
for user, item in datasets.source.iterate_chain(
                       sliceno, ('user', 'item',),
                       length = options.length):
\end{python}
This will iterate for the last \texttt{options.length} number of
datasets.  Note that a length of $-1$ is default and will iterate
without bounds.


\subsection{Halting Using \texttt{stop\_ds}}
Similar to using \texttt{length}, but will stop when reaching a
certain dataset.
\begin{python}
for user, item in datasets.source.iterate_chain(
                       sliceno, ('user', 'item',),
                       stop_ds = 'foo-3'):
\end{python}
Stopping at a constant dataset has limited value.  Next section shows
how to stop iterating based on previous jobs.



\subsection{Halting Using Another Job's Input Parameters}
\begin{python}
for user, item in datasets.source.iterate_chain(
                       sliceno, ('user', 'item',),
                       stop_ds = {jobs.previous: 'source',}):
\end{python}
This will iterate until reaching the \texttt{source} dataset of
the \texttt{jobs.previous} job.



\section{Iterating Over a Data Range}
\label{sec:iterate_sloppy_range}
It is possible to iterate over rows having a specified column's value
within a certain range.  This works best on datasets that are sorted
on the specified column.
\begin{python}
for user, item in datasets.source.iterate_chain(
           sliceno, ('user', 'item',),
           range={timestamp, datetime(2016, 1, 1), datetime(2016, 3, 31),}):
\end{python}
This example will limit the iterator to exactly the range of lines
that fulfill the range condition.  It is relatively costly to filter
each line, and there is a speed advantage by instead specifying
\texttt{sloppy\_range}, which will iterate over all datasets that
contain part of the range:
\begin{python}
for user, item in datasets.source.iterate_chain(
           sliceno, ('user', 'item',),
           sloppy_range={timestamp,
                         datetime(2016, 1, 1),
                         datetime(2016, 3, 31),}):
\end{python}
Here, all datasets that \textsl{contain} any line containing values
within the range will be included in the iteration.  Still, if the
datasets are sorted, and there are many datasets, the side-effect
caused by reading too many lines will be limited.



\section{Iterating in the Reverse Direction}
By default, iterating over a chain of dataset starts at the oldest
dataset and ends at the latest dataset.  This behavior can be
reversed by specifying \mintinline{python}/reverse=True/.  But note
that row iteration is still in the forward direction within each
dataset!
\begin{python}
for user, item in datasets.source.iterate_chain(
                       sliceno, ('user', 'item',),
                       reverse=True):
\end{python}



\section{Hash Partitioned Datasets and on-the-fly Partitioning}
Hash partitioning a dataset on a particular column, see
section~\ref{sec:slicing_and_hashing}, may really simplify the
parallel programming of methods using the dataset.  However, the
parallel code will not work properly if it turns out that the input
data is in fact not hash partitioned in the expected way.  For that
reason, it is a good idea to \emph{assert} the hashlabel by entering
it into the iterator function, like this
\begin{python}
s = {user: item for user, item datasets.source.iterate_chain(
     sliceno, ('user', 'item',), hashlabel='user')}
\end{python}
so that execution will terminate if the \texttt{hashlabel} is not correct.

It is possible to hash partition the dataset on-the-fly.  This is done
by setting the \texttt{rehash} argument to the iterator to
\mintinline{python}/True/, like this
\begin{python}
for user, item in datasets.source.iterate_chain(
                       sliceno, ('user', 'item',),
                       rehash='item'):
    # only lines with items such that
    # has(item) % slices == sliceno here
\end{python}
While this works, the preferred way to rehash is to use the
\texttt{dataset\_rehash} method~\ref{sec:dataset_hash_partition},
since it will store the rehashed dataset for later use, which in most
scenarios will be more efficient.






\section{Callbacks}
\label{sec:callback}
The iterator may be assigned callback functions that are called before
starting iterating a new dataset, and/or after the current dataset is
exhausted.  Callbacks are useful for example to aggregate data by
dataset when iterating over a large dataset chain.

There are two independent callbacks for these two cases,
called \texttt{pre\_callback} and \texttt{post\_callback}.
If \texttt{sliceno} is set to \pyNone, i.e.\ iteration runs over all
slices of all datasets in one process, it is even possible to have
callback between slice changes.

The example below will print the dataset identifier for each dataset
prior to iterating over it.
\begin{python}
# pre_callback once per dataset
def prefun(dataset):
    print(dataset.name)

for user, item in datasets.source.iterate_chain(
                       sliceno, ('user', 'item',),
                       pre_callback=prefun):
    ...
\end{python}
The argument to the callback is the dataset instance corresponding to
the dataset to be iterated next.

Next is an example of an iterator running over all slices.  The
callback function is executed before each new slice is iterated.  The
callback takes two arguments in this scenario, first, the dataset
instance as per the example above, and second the number of the slice.
\begin{python}
# callback once per slice
def prefun(dataset, sliceno):
    print(dataset.name, sliceno)

for user, item in datasets.source.iterate(
                       None, ('user', 'item',),
                       pre_callback=prefun):
    ...
\end{python}
The \texttt{post\_callback} function is defined similarly.


\subsection{Skipping Datasets and Slices from Callbacks}
It is possible to skip dataset iterations by raising exceptions, as
follows.
\begin{itemize}
\item [--] To skip the next dataset do
\begin{python}
raise SkipJob
\end{python}

\item [--]  To have the iterator skip a slice, do a
\begin{python}
raise SkipSlice
\end{python}

\item [--] And to abort iterating completely
\begin{python}
raise StopIteration
\end{python}
In this case, a \texttt{post\_callback} will never be run.
\end{itemize}



%% \section{Translators}
%% \label{sec:translators}
%% Translators transform iterator output data values on-the-fly.  A
%% translator is either a callable or a \texttt{dict}.  Translators are
%% similar to \textsl{filters} (explained later), and always
%% executed \emph{before} filtering.  The idea behind translators and
%% filters is to provide a way to modify code behavior by supplying
%% functions as options to iterators.  Using translators and filters,
%% it is possible to write re-useable functions that can have different
%% behaviour depending on context.


%% \subsection*{Callable Translator, translating \texttt{tuple}s}

%%  kolla att det funkar

%% A translator function is a function from an input \texttt{tuple} (of
%% column values) to an output \texttt{tuple} of the same length.
%% Individual items may be passed through or modified, and it is possible
%% to mix different columns with each other before sending them to the
%% iterator output.  Here is an example
%% \begin{python}
%% def merger(user, item):
%%     return ("%s:%s" % (user, item), None)

%% for merge, _ in datasets.source.iterate_chain(
%%                      None, ('user', 'item',),
%%                      translator=merger):
%%     ...
%% \end{python}
%% The purpose of this translator is to convert each
%% \mintinline{python}/(user, item)/ tuple to a string ``\texttt{user:item}''.
%% This is the first output of the translator and iterator and is stored
%% in the \texttt{merge} variable.  The second output variable is not
%% used in this application, but a variable still has to be assigned, so
%% it is set arbitrarily to \pyNone.


%% \subsection{Translator \texttt{dict}, translating columns independently}

%%  kolla att det funkar

%%  ungefar har, kolla vilka sections som har star, kolla att exempel funkar.

%% One or more columns may be translated independently using a translator
%% dictionary.  Such a dictionary is specified
%% as \mintinline{python}/{name: translation}/.  A translation may be
%% either a \texttt{dict} or a callable.  Examples of both kinds are
%% shown below.  First an example illustrating the use of a
%% translation \texttt{dict}.  Here, integers are translated into more
%% comprehensible strings.
%% \begin{python}
%% mapper = {2: 'HUMANLIKE', 4: 'LABRADOR', 5: 'STARFISH',}
%% for animal in datasets.source.iterate(
%%                    None, 'NUM_LEGS',
%%                    translator={'NUM_LEGS': mapper,}):
%%     ...
%% \end{python}
%% This translator will substitute the integers 2, 4, and 5 into strings.
%% Items missing in a translation-\texttt{dict} yield \pyNone.  The next
%% example illustrates a callable inside a translator \texttt{dict}.  In
%% this example, each \texttt{user} string is output from the iterator
%% reading right-to-left.
%% \begin{python}
%% def reverse(x):
%%     return x[::-1]
%% for resu, item in dataset.source.iterate(
%%                        None, ('USER', 'ITEM',),
%%                        translator={'USER': reverse,}):
%%     ...
%% \end{python}
%% The \texttt{item} values will be passed through, but the \texttt{user}
%% strings will be reversed.



%% \section{Filters}
%% \label{sec:filters}
%% Filter are used to decide which output data rows that are allowed to
%% reach the iterator output.  Filters are run \emph{after} translators.
%% As for translators, filter are either callables or \texttt{dict}s.


%% \subsection{Callable Filter, filtering \texttt{tuple}s}
%% Callable filters receive the iterator \texttt{tuple} as input.  The
%% output of a filter must be \pyTrue for the \texttt{tuple} to be output
%% from the iterator, otherwise the iterator skips and continues to the
%% next row.  The following two examples will iterate over all animals
%% that have at least two legs and a trunk.  The first example is without
%% filter, using an \texttt{if}-statement, and the second example uses
%% iterator \texttt{filters}.
%% \begin{python}
%% # Ex. 1. Using if-statement
%% for animal, nlegs, wtrunk in datasets.source.iterate(
%%                     None,
%%                     ('animal', 'num_legs', 'has_trunk'),
%%                     filters=filter):
%%     if nlegs>=2 and wtrunk:
%%     ...

%% # Ex. 2.  Using iterator filters
%% filter = lambda line: line[1]>=2 and line[2]

%% for animal, nlegs, wtrunk in datasets.source.iterate(
%%                     None,
%%                     ('animal', 'num_legs', 'has_trunk'),
%%                     filters=filter):
%%     ...
%% \end{python}
%% Note the indexing in the \texttt{lambda} function.  Index zero
%% corresponds to the \texttt{animal} column, which is not includes in
%% the filtering expression.



%% \subsection*{Filter Dict, filtering independent columns}

%% It is possible to filter on one or more columns independently using a
%% \texttt{dict}.  If there is more than one filter, all filters must be
%% \pyTrue for a line to be output from the iterator.
%% Below are two examples of filter \texttt{dict}s.  The first example
%% will remove all rows except the ones with valid users.
%% \begin{python}
%% # keep valid users only
%% validusers={'user1', 'user2', 'user3'}
%% filters={'user': validusers.__contains__}
%% \end{python}
%% The second example will only keep rows with valid users and movie
%% items.
%% \begin{python}
%% # keep valid users with movie items
%% validusers={'user1', 'user2', 'user3'}
%% validitems={'movie': True, 'book': False}
%% filters={'user': validusers.__contains__, 'item': validitems.get}
%% \end{python}
%% The fact that book is \pyFalse is actually redundant, since
%% missing keys will never evaluate to \pyTrue and thus result in
%% discarded lines.




%% \subsection*{Filter by Column Values}

%% Filtering could also be by column value directly.  For example, assume
%% there is a column \texttt{has\_trunk} with values being Boolean
%% integers, i.e.\ \texttt{1} or \texttt{0}.  Animals with trunks may be
%% iterated using
%% \begin{python}
%% for animal, wtrunk in datasets.source.iterate_chain(None,
%%                       ('animal', 'has_trunk',),
%%                       filters={'has_trunk': None}):
%%     trunked.add(animal)
%% \end{python}
%% This may seem strange at first, but it works because the key for
%% the \texttt{has\_trunk} column exists, and the value is
%% \pyNone, which is neither a callable nor a \texttt{dict}.




